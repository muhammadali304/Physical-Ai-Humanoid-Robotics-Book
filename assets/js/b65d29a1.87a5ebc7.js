"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5197],{7074:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>r});var i=o(6540);const s={},t=i.createContext(s);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(t.Provider,{value:n},e.children)}},9907:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"vla/voice-commands","title":"Voice Commands - Natural Language Interaction with Robots","description":"Learning Objectives","source":"@site/docs/vla/voice-commands.md","sourceDirName":"vla","slug":"/vla/voice-commands","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/vla/voice-commands","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/voice-commands.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"LLM Planning - Cognitive Planning with Large Language Models","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/vla/llm-planning"},"next":{"title":"Capstone Project Integration Guide","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/capstone/capstone-project-integration-guide"}}');var s=o(4848),t=o(7074);const a={sidebar_position:1},r="Voice Commands - Natural Language Interaction with Robots",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Conceptual Overview",id:"conceptual-overview",level:2},{value:"Key Components of Voice Command Systems",id:"key-components-of-voice-command-systems",level:3},{value:"Voice Command Architecture",id:"voice-command-architecture",level:3},{value:"Benefits of Voice Commands",id:"benefits-of-voice-commands",level:3},{value:"Hands-On Implementation",id:"hands-on-implementation",level:2},{value:"Installing Speech Recognition Dependencies",id:"installing-speech-recognition-dependencies",level:3},{value:"Basic Speech Recognition Node",id:"basic-speech-recognition-node",level:3},{value:"Advanced Voice Command with Natural Language Processing",id:"advanced-voice-command-with-natural-language-processing",level:3},{value:"Offline Speech Recognition with Vosk",id:"offline-speech-recognition-with-vosk",level:3},{value:"Text-to-Speech Feedback Node",id:"text-to-speech-feedback-node",level:3},{value:"Creating a Voice Command Package",id:"creating-a-voice-command-package",level:3},{value:"Launch File for Voice Commands",id:"launch-file-for-voice-commands",level:3},{value:"Testing &amp; Verification",id:"testing--verification",level:2},{value:"Running Voice Command System",id:"running-voice-command-system",level:3},{value:"Useful Voice Command Commands",id:"useful-voice-command-commands",level:3},{value:"Performance Testing",id:"performance-testing",level:3},{value:"Common Issues",id:"common-issues",level:2},{value:"Issue: Microphone not detected or no audio input",id:"issue-microphone-not-detected-or-no-audio-input",level:3},{value:"Issue: High CPU usage with speech recognition",id:"issue-high-cpu-usage-with-speech-recognition",level:3},{value:"Issue: Poor recognition accuracy",id:"issue-poor-recognition-accuracy",level:3},{value:"Issue: Delay in voice feedback",id:"issue-delay-in-voice-feedback",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"voice-commands---natural-language-interaction-with-robots",children:"Voice Commands - Natural Language Interaction with Robots"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement speech recognition systems for robot command interpretation"}),"\n",(0,s.jsx)(n.li,{children:"Design natural language processing pipelines for voice commands"}),"\n",(0,s.jsx)(n.li,{children:"Integrate voice command systems with robot control architectures"}),"\n",(0,s.jsx)(n.li,{children:"Create custom voice command vocabularies and grammars"}),"\n",(0,s.jsx)(n.li,{children:"Handle speech-to-text conversion and command parsing"}),"\n",(0,s.jsx)(n.li,{children:"Implement voice feedback and confirmation systems"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before starting this chapter, you should:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Have ROS 2 Humble Hawksbill installed and configured"}),"\n",(0,s.jsx)(n.li,{children:"Understand ROS 2 nodes, topics, and message types"}),"\n",(0,s.jsx)(n.li,{children:"Completed the Isaac perception and navigation chapters"}),"\n",(0,s.jsx)(n.li,{children:"Basic knowledge of Python and speech processing concepts"}),"\n",(0,s.jsx)(n.li,{children:"Understanding of natural language processing fundamentals"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conceptual-overview",children:"Conceptual Overview"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Voice Command Systems"})," enable robots to understand and respond to spoken instructions from humans. This creates a more natural and intuitive interaction model compared to traditional button-based or app-based interfaces."]}),"\n",(0,s.jsx)(n.h3,{id:"key-components-of-voice-command-systems",children:"Key Components of Voice Command Systems"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Interpreting meaning from text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command Mapping"}),": Connecting understood commands to robot actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Feedback"}),": Providing audio confirmation to users"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise Filtering"}),": Processing audio in noisy environments"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"voice-command-architecture",children:"Voice Command Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The typical voice command architecture follows this flow:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Audio Input \u2192 Noise Reduction \u2192 Speech Recognition \u2192 NLU \u2192 Command Mapping \u2192 Robot Action\n"})}),"\n",(0,s.jsx)(n.h3,{id:"benefits-of-voice-commands",children:"Benefits of Voice Commands"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Natural Interaction"}),": More intuitive than manual controls"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accessibility"}),": Helps users with mobility limitations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hands-Free Operation"}),": Allows multitasking"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Remote Control"}),": Works at distances where physical controls don't"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multilingual Support"}),": Can support multiple languages"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-implementation",children:"Hands-On Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"installing-speech-recognition-dependencies",children:"Installing Speech Recognition Dependencies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install system dependencies\nsudo apt update\nsudo apt install python3-pyaudio python3-speechrecognition python3-pip portaudio19-dev\n\n# Install Python packages\npip3 install SpeechRecognition pyttsx3 vosk transformers torch numpy\npip3 install sounddevice webrtcvad pyaudio\n"})}),"\n",(0,s.jsx)(n.h3,{id:"basic-speech-recognition-node",children:"Basic Speech Recognition Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nBasic speech recognition node for voice commands.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport speech_recognition as sr\nimport threading\nimport queue\nimport time\n\n\nclass VoiceCommandNode(Node):\n    """\n    Node to recognize voice commands and convert them to robot actions.\n    """\n\n    def __init__(self):\n        super().__init__(\'voice_command_node\')\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Calibrate microphone for ambient noise\n        with self.microphone as source:\n            self.get_logger().info(\'Calibrating for ambient noise...\')\n            self.recognizer.adjust_for_ambient_noise(source)\n            self.get_logger().info(\'Calibration complete\')\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, \'voice_commands\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n\n        # Command mappings\n        self.command_mappings = {\n            \'move forward\': self.move_forward,\n            \'go forward\': self.move_forward,\n            \'move backward\': self.move_backward,\n            \'go backward\': self.move_backward,\n            \'turn left\': self.turn_left,\n            \'turn right\': self.turn_right,\n            \'stop\': self.stop_robot,\n            \'halt\': self.stop_robot,\n            \'go left\': self.move_left,\n            \'go right\': self.move_right,\n        }\n\n        # Start listening thread\n        self.listen_thread = threading.Thread(target=self.listen_continuously)\n        self.listen_thread.daemon = True\n        self.listen_thread.start()\n\n        # Queue for audio processing\n        self.audio_queue = queue.Queue()\n\n        self.get_logger().info(\'Voice command node initialized\')\n\n    def listen_continuously(self):\n        """Continuously listen for voice commands."""\n        with self.microphone as source:\n            self.get_logger().info(\'Listening for voice commands...\')\n\n        while rclpy.ok():\n            try:\n                # Listen for audio with timeout\n                with self.microphone as source:\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\n\n                # Add audio to queue for processing\n                self.audio_queue.put(audio)\n\n            except sr.WaitTimeoutError:\n                # Continue listening if no audio detected\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Error in audio capture: {e}\')\n                time.sleep(0.1)\n\n    def process_audio(self):\n        """Process audio from the queue."""\n        while not self.audio_queue.empty():\n            try:\n                audio = self.audio_queue.get_nowait()\n\n                # Recognize speech using Google\'s service\n                try:\n                    text = self.recognizer.recognize_google(audio)\n                    self.get_logger().info(f\'Recognized: {text}\')\n\n                    # Process the recognized text\n                    self.process_command(text.lower().strip())\n\n                except sr.UnknownValueError:\n                    self.get_logger().info(\'Could not understand audio\')\n                except sr.RequestError as e:\n                    self.get_logger().error(f\'Error with speech recognition service: {e}\')\n\n            except queue.Empty:\n                break\n\n    def process_command(self, text):\n        """Process recognized text and execute appropriate command."""\n        # Publish the recognized command\n        cmd_msg = String()\n        cmd_msg.data = text\n        self.command_pub.publish(cmd_msg)\n\n        # Check if command matches any mapping\n        for command_phrase, command_func in self.command_mappings.items():\n            if command_phrase in text:\n                self.get_logger().info(f\'Executing command: {command_phrase}\')\n                command_func()\n                return\n\n        # If no command matched\n        self.get_logger().info(f\'Unrecognized command: {text}\')\n\n    def move_forward(self):\n        """Move robot forward."""\n        twist = Twist()\n        twist.linear.x = 0.5  # m/s\n        self.cmd_vel_pub.publish(twist)\n\n    def move_backward(self):\n        """Move robot backward."""\n        twist = Twist()\n        twist.linear.x = -0.5  # m/s\n        self.cmd_vel_pub.publish(twist)\n\n    def turn_left(self):\n        """Turn robot left."""\n        twist = Twist()\n        twist.angular.z = 0.5  # rad/s\n        self.cmd_vel_pub.publish(twist)\n\n    def turn_right(self):\n        """Turn robot right."""\n        twist = Twist()\n        twist.angular.z = -0.5  # rad/s\n        self.cmd_vel_pub.publish(twist)\n\n    def move_left(self):\n        """Strafe robot left."""\n        twist = Twist()\n        twist.linear.y = 0.5  # m/s\n        self.cmd_vel_pub.publish(twist)\n\n    def move_right(self):\n        """Strafe robot right."""\n        twist = Twist()\n        twist.linear.y = -0.5  # m/s\n        self.cmd_vel_pub.publish(twist)\n\n    def stop_robot(self):\n        """Stop robot movement."""\n        twist = Twist()\n        # All velocities remain 0\n        self.cmd_vel_pub.publish(twist)\n\n    def spin_once(self):\n        """Process any queued audio."""\n        self.process_audio()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n\n    try:\n        while rclpy.ok():\n            node.spin_once()\n            time.sleep(0.01)  # Small delay to prevent busy waiting\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Voice command node stopped by user\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"advanced-voice-command-with-natural-language-processing",children:"Advanced Voice Command with Natural Language Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nAdvanced voice command node with natural language understanding.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import LaserScan\nimport speech_recognition as sr\nimport threading\nimport queue\nimport time\nimport re\nfrom enum import Enum\n\n\nclass CommandType(Enum):\n    MOVE = "move"\n    TURN = "turn"\n    STOP = "stop"\n    NAVIGATE = "navigate"\n    QUERY = "query"\n\n\nclass Command:\n    """Represents a parsed voice command."""\n    def __init__(self, cmd_type, direction=None, distance=None, speed=None, destination=None):\n        self.type = cmd_type\n        self.direction = direction\n        self.distance = distance\n        self.speed = speed\n        self.destination = destination\n\n\nclass AdvancedVoiceCommandNode(Node):\n    """\n    Advanced voice command node with natural language understanding.\n    """\n\n    def __init__(self):\n        super().__init__(\'advanced_voice_command_node\')\n\n        # Initialize speech recognizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Calibrate microphone\n        with self.microphone as source:\n            self.get_logger().info(\'Calibrating for ambient noise...\')\n            self.recognizer.adjust_for_ambient_noise(source)\n            self.get_logger().info(\'Calibration complete\')\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, \'parsed_commands\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.scan_callback,\n            10\n        )\n\n        # Initialize variables\n        self.laser_data = None\n        self.command_history = []\n\n        # Start listening thread\n        self.listen_thread = threading.Thread(target=self.listen_continuously)\n        self.listen_thread.daemon = True\n        self.listen_thread.start()\n\n        # Queue for audio processing\n        self.audio_queue = queue.Queue()\n\n        self.get_logger().info(\'Advanced voice command node initialized\')\n\n    def scan_callback(self, msg):\n        """Store laser scan data for obstacle detection."""\n        self.laser_data = msg\n\n    def listen_continuously(self):\n        """Continuously listen for voice commands."""\n        with self.microphone as source:\n            self.get_logger().info(\'Listening for voice commands...\')\n\n        while rclpy.ok():\n            try:\n                with self.microphone as source:\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\n                self.audio_queue.put(audio)\n            except sr.WaitTimeoutError:\n                continue\n            except Exception as e:\n                self.get_logger().error(f\'Error in audio capture: {e}\')\n                time.sleep(0.1)\n\n    def process_audio(self):\n        """Process audio from the queue."""\n        while not self.audio_queue.empty():\n            try:\n                audio = self.audio_queue.get_nowait()\n\n                try:\n                    text = self.recognizer.recognize_google(audio)\n                    self.get_logger().info(f\'Recognized: {text}\')\n\n                    # Parse and execute command\n                    self.parse_and_execute_command(text.lower().strip())\n\n                except sr.UnknownValueError:\n                    self.get_logger().info(\'Could not understand audio\')\n                except sr.RequestError as e:\n                    self.get_logger().error(f\'Error with speech recognition service: {e}\')\n\n            except queue.Empty:\n                break\n\n    def parse_and_execute_command(self, text):\n        """Parse text and execute appropriate command."""\n        # Publish original recognized text\n        original_msg = String()\n        original_msg.data = text\n        self.command_pub.publish(original_msg)\n\n        # Parse the command\n        command = self.parse_command(text)\n\n        if command:\n            self.get_logger().info(f\'Parsed command: {command.type.value}\')\n\n            # Execute based on command type\n            if command.type == CommandType.MOVE:\n                self.execute_move_command(command)\n            elif command.type == CommandType.TURN:\n                self.execute_turn_command(command)\n            elif command.type == CommandType.STOP:\n                self.execute_stop_command(command)\n            elif command.type == CommandType.NAVIGATE:\n                self.execute_navigate_command(command)\n            elif command.type == CommandType.QUERY:\n                self.execute_query_command(command)\n\n            # Store in history\n            self.command_history.append(command)\n        else:\n            self.get_logger().info(f\'Could not parse command: {text}\')\n\n    def parse_command(self, text):\n        """Parse text to extract command components."""\n        # Define patterns for different command types\n\n        # Move commands: "move forward 2 meters", "go left slowly"\n        move_patterns = [\n            r\'(?:move|go)\\s+(forward|backward|ahead|back|left|right|up|down)\',\n            r\'(?:move|go)\\s+(?:the\\s+)?robot\\s+(forward|backward|ahead|back|left|right)\'\n        ]\n\n        # Turn commands: "turn left", "rotate right"\n        turn_patterns = [\n            r\'(?:turn|rotate)\\s+(left|right)\',\n            r\'(?:spin|pivot)\\s+(left|right)\'\n        ]\n\n        # Stop commands: "stop", "halt", "freeze"\n        stop_patterns = [\n            r\'(?:stop|halt|freeze|pause|stand)\',\n            r\'(?:come\\s+to\\s+a\\s+stop|stop\\s+moving)\'\n        ]\n\n        # Navigate commands: "go to kitchen", "navigate to charging station"\n        navigate_patterns = [\n            r\'(?:go\\s+to|navigate\\s+to|move\\s+to)\\s+(.+?)(?:\\s+please|\\.|$)\',\n            r\'(?:take\\s+me\\s+to|bring\\s+me\\s+to)\\s+(.+?)(?:\\s+please|\\.|$)\'\n        ]\n\n        # Query commands: "where are you", "what is your status"\n        query_patterns = [\n            r\'(?:where\\s+are\\s+you|where\\s+is\\s+the\\s+robot|location|position)\',\n            r\'(?:status|what\\s+are\\s+you\\s+doing|what\\s+is\\s+your\\s+status)\',\n            r\'(?:battery|power|charge|energy)\'\n        ]\n\n        # Extract distance and speed modifiers\n        distance_match = re.search(r\'(\\d+(?:\\.\\d+)?)\\s*(?:meter|m|foot|ft)\', text)\n        speed_match = re.search(r\'(slowly|slow|fast|quickly|quick)\', text)\n\n        # Try to match patterns\n        for pattern in move_patterns:\n            match = re.search(pattern, text)\n            if match:\n                direction = match.group(1)\n                distance = float(distance_match.group(1)) if distance_match else None\n                speed = speed_match.group(1) if speed_match else None\n                return Command(CommandType.MOVE, direction=direction, distance=distance, speed=speed)\n\n        for pattern in turn_patterns:\n            match = re.search(pattern, text)\n            if match:\n                direction = match.group(1)\n                speed = speed_match.group(1) if speed_match else None\n                return Command(CommandType.TURN, direction=direction, speed=speed)\n\n        for pattern in stop_patterns:\n            if re.search(pattern, text):\n                return Command(CommandType.STOP)\n\n        for pattern in navigate_patterns:\n            match = re.search(pattern, text)\n            if match:\n                destination = match.group(1).strip()\n                return Command(CommandType.NAVIGATE, destination=destination)\n\n        for pattern in query_patterns:\n            if re.search(pattern, text):\n                return Command(CommandType.QUERY)\n\n        return None  # No command matched\n\n    def execute_move_command(self, command):\n        """Execute a move command."""\n        twist = Twist()\n\n        if command.direction in [\'forward\', \'ahead\']:\n            twist.linear.x = 0.5 if command.speed != \'slow\' else 0.2\n        elif command.direction in [\'backward\', \'back\']:\n            twist.linear.x = -0.5 if command.speed != \'slow\' else -0.2\n        elif command.direction == \'left\':\n            twist.linear.y = 0.3 if command.speed != \'slow\' else 0.15\n        elif command.direction == \'right\':\n            twist.linear.y = -0.3 if command.speed != \'slow\' else -0.15\n        elif command.direction == \'up\':\n            twist.linear.z = 0.2\n        elif command.direction == \'down\':\n            twist.linear.z = -0.2\n\n        self.cmd_vel_pub.publish(twist)\n\n    def execute_turn_command(self, command):\n        """Execute a turn command."""\n        twist = Twist()\n\n        if command.direction == \'left\':\n            twist.angular.z = 0.5 if command.speed != \'slow\' else 0.2\n        elif command.direction == \'right\':\n            twist.angular.z = -0.5 if command.speed != \'slow\' else -0.2\n\n        self.cmd_vel_pub.publish(twist)\n\n    def execute_stop_command(self, command):\n        """Execute a stop command."""\n        twist = Twist()\n        # All velocities remain 0\n        self.cmd_vel_pub.publish(twist)\n\n    def execute_navigate_command(self, command):\n        """Execute a navigate command."""\n        self.get_logger().info(f\'Navigating to: {command.destination}\')\n        # In a real implementation, this would interface with navigation stack\n        # For now, just log the command\n        pass\n\n    def execute_query_command(self, command):\n        """Execute a query command."""\n        # In a real implementation, this would respond with robot status\n        self.get_logger().info(f\'Query command received: {command.destination}\')\n        pass\n\n    def spin_once(self):\n        """Process any queued audio."""\n        self.process_audio()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AdvancedVoiceCommandNode()\n\n    try:\n        while rclpy.ok():\n            node.spin_once()\n            time.sleep(0.01)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Advanced voice command node stopped by user\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"offline-speech-recognition-with-vosk",children:"Offline Speech Recognition with Vosk"}),"\n",(0,s.jsx)(n.p,{children:"For privacy-sensitive applications, we can use offline speech recognition:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nOffline voice command node using Vosk for speech recognition.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom vosk import Model, KaldiRecognizer\nimport pyaudio\nimport json\nimport threading\nimport queue\n\n\nclass OfflineVoiceCommandNode(Node):\n    """\n    Voice command node using offline Vosk speech recognition.\n    """\n\n    def __init__(self):\n        super().__init__(\'offline_voice_command_node\')\n\n        # Initialize Vosk model (download from https://alphacephei.com/vosk/models)\n        try:\n            self.model = Model(lang="en-us")  # Change language as needed\n        except Exception as e:\n            self.get_logger().error(f\'Failed to load Vosk model: {e}\')\n            self.get_logger().error(\'Download model from https://alphacephei.com/vosk/models\')\n            raise\n\n        # Initialize audio\n        self.sample_rate = 16000\n        self.rec = KaldiRecognizer(self.model, self.sample_rate)\n\n        # Initialize PyAudio\n        self.p = pyaudio.PyAudio()\n        self.stream = self.p.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=8000\n        )\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, \'offline_commands\', 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n\n        # Command mappings\n        self.command_mappings = {\n            \'move forward\': self.move_forward,\n            \'go forward\': self.move_forward,\n            \'move backward\': self.move_backward,\n            \'go backward\': self.move_backward,\n            \'turn left\': self.turn_left,\n            \'turn right\': self.turn_right,\n            \'stop\': self.stop_robot,\n            \'halt\': self.stop_robot,\n        }\n\n        # Audio processing thread\n        self.audio_thread = threading.Thread(target=self.process_audio_stream)\n        self.audio_thread.daemon = True\n        self.audio_thread.start()\n\n        self.get_logger().info(\'Offline voice command node initialized\')\n\n    def process_audio_stream(self):\n        """Process audio stream continuously."""\n        while rclpy.ok():\n            try:\n                data = self.stream.read(4000, exception_on_overflow=False)\n\n                if len(data) == 0:\n                    continue\n\n                if self.rec.AcceptWaveform(data):\n                    result = self.rec.Result()\n                    result_dict = json.loads(result)\n\n                    if \'text\' in result_dict and result_dict[\'text\']:\n                        text = result_dict[\'text\'].lower().strip()\n                        if text:  # Only process non-empty text\n                            self.get_logger().info(f\'Recognized (offline): {text}\')\n                            self.process_command(text)\n\n            except Exception as e:\n                self.get_logger().error(f\'Error in audio processing: {e}\')\n\n    def process_command(self, text):\n        """Process recognized text and execute appropriate command."""\n        # Publish the recognized command\n        cmd_msg = String()\n        cmd_msg.data = text\n        self.command_pub.publish(cmd_msg)\n\n        # Check if command matches any mapping\n        for command_phrase, command_func in self.command_mappings.items():\n            if command_phrase in text:\n                self.get_logger().info(f\'Executing command: {command_phrase}\')\n                command_func()\n                return\n\n        # If no command matched\n        self.get_logger().info(f\'Unrecognized command: {text}\')\n\n    def move_forward(self):\n        """Move robot forward."""\n        twist = Twist()\n        twist.linear.x = 0.5\n        self.cmd_vel_pub.publish(twist)\n\n    def move_backward(self):\n        """Move robot backward."""\n        twist = Twist()\n        twist.linear.x = -0.5\n        self.cmd_vel_pub.publish(twist)\n\n    def turn_left(self):\n        """Turn robot left."""\n        twist = Twist()\n        twist.angular.z = 0.5\n        self.cmd_vel_pub.publish(twist)\n\n    def turn_right(self):\n        """Turn robot right."""\n        twist = Twist()\n        twist.angular.z = -0.5\n        self.cmd_vel_pub.publish(twist)\n\n    def stop_robot(self):\n        """Stop robot movement."""\n        twist = Twist()\n        self.cmd_vel_pub.publish(twist)\n\n    def destroy_node(self):\n        """Clean up resources."""\n        if hasattr(self, \'stream\'):\n            self.stream.stop_stream()\n            self.stream.close()\n        if hasattr(self, \'p\'):\n            self.p.terminate()\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = OfflineVoiceCommandNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Offline voice command node stopped by user\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"text-to-speech-feedback-node",children:"Text-to-Speech Feedback Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n\n"""\nText-to-speech feedback node for voice command responses.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport pyttsx3\nimport threading\n\n\nclass VoiceFeedbackNode(Node):\n    """\n    Node to provide voice feedback for robot actions.\n    """\n\n    def __init__(self):\n        super().__init__(\'voice_feedback_node\')\n\n        # Initialize text-to-speech engine\n        self.tts_engine = pyttsx3.init()\n\n        # Configure voice properties\n        voices = self.tts_engine.getProperty(\'voices\')\n        if voices:\n            self.tts_engine.setProperty(\'voice\', voices[0].id)  # Use first available voice\n        self.tts_engine.setProperty(\'rate\', 150)  # Speed of speech\n        self.tts_engine.setProperty(\'volume\', 0.9)  # Volume level\n\n        # Create subscriber for feedback requests\n        self.feedback_sub = self.create_subscription(\n            String,\n            \'voice_feedback\',\n            self.feedback_callback,\n            10\n        )\n\n        # Thread lock for TTS\n        self.tts_lock = threading.Lock()\n\n        self.get_logger().info(\'Voice feedback node initialized\')\n\n    def feedback_callback(self, msg):\n        """Handle feedback requests."""\n        text = msg.data\n        self.get_logger().info(f\'Playing voice feedback: {text}\')\n\n        # Speak the text in a separate thread to avoid blocking\n        speak_thread = threading.Thread(target=self.speak_text, args=(text,))\n        speak_thread.daemon = True\n        speak_thread.start()\n\n    def speak_text(self, text):\n        """Speak text using TTS engine."""\n        with self.tts_lock:\n            try:\n                self.tts_engine.say(text)\n                self.tts_engine.runAndWait()\n            except Exception as e:\n                self.get_logger().error(f\'Error in text-to-speech: {e}\')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceFeedbackNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Voice feedback node stopped by user\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"creating-a-voice-command-package",children:"Creating a Voice Command Package"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Create the package:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\nros2 pkg create --build-type ament_python voice_commands\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Update package.xml:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>voice_commands</name>\n  <version>0.0.1</version>\n  <description>Voice command processing for robotics</description>\n  <maintainer email="user@todo.todo">user</maintainer>\n  <license>Apache-2.0</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>sensor_msgs</depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Create setup.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\n\npackage_name = 'voice_commands'\n\nsetup(\n    name=package_name,\n    version='0.0.1',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='user',\n    maintainer_email='user@todo.todo',\n    description='Voice command processing for robotics',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'voice_command_node = voice_commands.voice_command_node:main',\n            'advanced_voice_node = voice_commands.advanced_voice_node:main',\n            'offline_voice_node = voice_commands.offline_voice_node:main',\n            'voice_feedback_node = voice_commands.voice_feedback_node:main',\n        ],\n    },\n)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"launch-file-for-voice-commands",children:"Launch File for Voice Commands"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Create launch/voice_commands_launch.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation time if true'\n    )\n\n    # Voice command node\n    voice_command_node = Node(\n        package='voice_commands',\n        executable='voice_command_node',\n        name='voice_command_node',\n        parameters=[\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\n        ],\n        output='screen'\n    )\n\n    # Voice feedback node\n    voice_feedback_node = Node(\n        package='voice_commands',\n        executable='voice_feedback_node',\n        name='voice_feedback_node',\n        parameters=[\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        use_sim_time,\n        voice_command_node,\n        voice_feedback_node\n    ])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"testing--verification",children:"Testing & Verification"}),"\n",(0,s.jsx)(n.h3,{id:"running-voice-command-system",children:"Running Voice Command System"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Install dependencies:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip3 install SpeechRecognition pyttsx3 vosk transformers torch numpy\nsudo apt install python3-pyaudio portaudio19-dev\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"2",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Build the package:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\ncolcon build --packages-select voice_commands\nsource install/setup.bash\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Run the voice command system:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Run the voice command node\nros2 run voice_commands voice_command_node\n\n# Terminal 2: Test with manual commands\nros2 topic pub /voice_feedback std_msgs/msg/String \"data: 'Hello, I am ready to receive commands'\"\n"})}),"\n",(0,s.jsxs)(n.ol,{start:"4",children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Test with different voice commands:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Speak commands like:\n# "Move forward"\n# "Turn left"\n# "Stop"\n# "Go backward"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"useful-voice-command-commands",children:"Useful Voice Command Commands"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Monitor voice commands:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /voice_commands\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Send feedback:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub /voice_feedback std_msgs/msg/String \"data: 'Command received'\"\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.strong,{children:"Check audio devices:"})}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Python script to list audio devices\npython3 -c \"import pyaudio; p = pyaudio.PyAudio(); print('Devices:', p.get_device_count()); [print(i, p.get_device_info_by_index(i)['name']) for i in range(p.get_device_count())]\"\n"})}),"\n",(0,s.jsx)(n.h3,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Test recognition accuracy in different environments\n# Test response time to commands\n# Test with different speakers and accents\n"})}),"\n",(0,s.jsx)(n.h2,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsx)(n.h3,{id:"issue-microphone-not-detected-or-no-audio-input",children:"Issue: Microphone not detected or no audio input"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check microphone permissions and connections"}),"\n",(0,s.jsxs)(n.li,{children:["Verify audio device using ",(0,s.jsx)(n.code,{children:"arecord -l"})]}),"\n",(0,s.jsxs)(n.li,{children:["Test microphone with ",(0,s.jsx)(n.code,{children:"arecord -D hw:0,0 -f cd test.wav"})]}),"\n",(0,s.jsx)(n.li,{children:"Ensure proper ALSA/PulseAudio configuration"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"issue-high-cpu-usage-with-speech-recognition",children:"Issue: High CPU usage with speech recognition"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use offline models like Vosk for continuous recognition"}),"\n",(0,s.jsx)(n.li,{children:"Implement wake word detection to activate recognition only when needed"}),"\n",(0,s.jsx)(n.li,{children:"Reduce sampling rate if quality permits"}),"\n",(0,s.jsx)(n.li,{children:"Use threading to prevent blocking"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"issue-poor-recognition-accuracy",children:"Issue: Poor recognition accuracy"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Calibrate microphone for ambient noise"}),"\n",(0,s.jsx)(n.li,{children:"Use directional microphones in noisy environments"}),"\n",(0,s.jsx)(n.li,{children:"Train custom acoustic models for specific environments"}),"\n",(0,s.jsx)(n.li,{children:"Implement confidence thresholds to filter uncertain recognitions"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"issue-delay-in-voice-feedback",children:"Issue: Delay in voice feedback"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use streaming TTS engines"}),"\n",(0,s.jsx)(n.li,{children:"Implement interruptible speech for urgent feedback"}),"\n",(0,s.jsx)(n.li,{children:"Optimize audio processing pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Use lightweight synthesis models"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Voice commands provide natural, intuitive robot interaction"}),"\n",(0,s.jsx)(n.li,{children:"Offline recognition offers privacy and reliability"}),"\n",(0,s.jsx)(n.li,{children:"Natural language processing enables complex command understanding"}),"\n",(0,s.jsx)(n.li,{children:"Audio feedback improves user experience and confirms actions"}),"\n",(0,s.jsx)(n.li,{children:"Wake word detection reduces power consumption"}),"\n",(0,s.jsx)(n.li,{children:"Noise filtering is crucial for real-world deployment"}),"\n",(0,s.jsx)(n.li,{children:"Integration with navigation and control systems enables complex tasks"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"In the next chapter, you'll learn about Large Language Model (LLM) integration for advanced planning and reasoning in robotics applications."})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}}}]);