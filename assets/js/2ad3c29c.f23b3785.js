"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9596],{1735:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"isaac-platform/vsland-pipeline-configuration","title":"VSLAM Pipeline Configuration for Isaac ROS","description":"Overview","source":"@site/docs/isaac-platform/vsland-pipeline-configuration.md","sourceDirName":"isaac-platform","slug":"/isaac-platform/vsland-pipeline-configuration","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/vsland-pipeline-configuration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-platform/vsland-pipeline-configuration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 Navigation Stack Setup Guide for ROS 2 Humble","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/nav2-setup-guide"},"next":{"title":"Robot Localization System Using AMCL","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/robot-localization-amcl"}}');var r=i(4848),o=i(7074);const s={},t="VSLAM Pipeline Configuration for Isaac ROS",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Understanding Isaac ROS Visual SLAM",id:"understanding-isaac-ros-visual-slam",level:2},{value:"Components",id:"components",level:3},{value:"Available Packages",id:"available-packages",level:3},{value:"VSLAM Pipeline Configuration",id:"vslam-pipeline-configuration",level:2},{value:"1. Basic Visual SLAM Node Configuration",id:"1-basic-visual-slam-node-configuration",level:3},{value:"2. Launch File Configuration",id:"2-launch-file-configuration",level:3},{value:"3. Monocular Camera Configuration",id:"3-monocular-camera-configuration",level:3},{value:"4. RGB-D Camera Configuration",id:"4-rgb-d-camera-configuration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. GPU Acceleration Configuration",id:"1-gpu-acceleration-configuration",level:3},{value:"2. Memory Management",id:"2-memory-management",level:3},{value:"Integration with Navigation Stack",id:"integration-with-navigation-stack",level:2},{value:"1. TF Tree Configuration",id:"1-tf-tree-configuration",level:3},{value:"2. Odometry Integration",id:"2-odometry-integration",level:3},{value:"Calibration Requirements",id:"calibration-requirements",level:2},{value:"1. Camera Calibration",id:"1-camera-calibration",level:3},{value:"2. Extrinsic Calibration",id:"2-extrinsic-calibration",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"Issue: Poor Tracking Performance",id:"issue-poor-tracking-performance",level:3},{value:"Issue: Map Drift",id:"issue-map-drift",level:3},{value:"Issue: High CPU/GPU Usage",id:"issue-high-cpugpu-usage",level:3},{value:"Issue: Scale Ambiguity (Monocular)",id:"issue-scale-ambiguity-monocular",level:3},{value:"Quality Assurance",id:"quality-assurance",level:2},{value:"1. Performance Metrics",id:"1-performance-metrics",level:3},{value:"2. Accuracy Validation",id:"2-accuracy-validation",level:3},{value:"Advanced Configuration",id:"advanced-configuration",level:2},{value:"1. Multi-Camera Setup",id:"1-multi-camera-setup",level:3},{value:"2. Dynamic Object Filtering",id:"2-dynamic-object-filtering",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Environment Considerations",id:"1-environment-considerations",level:3},{value:"2. Hardware Optimization",id:"2-hardware-optimization",level:3},{value:"3. Parameter Tuning",id:"3-parameter-tuning",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"1. Simple Test Environment",id:"1-simple-test-environment",level:3},{value:"2. Loop Closure Test",id:"2-loop-closure-test",level:3},{value:"Resources",id:"resources",level:2},{value:"Conclusion",id:"conclusion",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vslam-pipeline-configuration-for-isaac-ros",children:"VSLAM Pipeline Configuration for Isaac ROS"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This guide provides instructions for configuring a Visual Simultaneous Localization and Mapping (VSLAM) pipeline using Isaac ROS. VSLAM enables robots to build maps of their environment while simultaneously localizing themselves within those maps using visual sensors like cameras."}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Isaac ROS perception packages installed (completed in T063)"}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 Humble Hawksbill"}),"\n",(0,r.jsx)(n.li,{children:"Camera sensors (monocular, stereo, or RGB-D)"}),"\n",(0,r.jsx)(n.li,{children:"Properly calibrated camera(s)"}),"\n",(0,r.jsx)(n.li,{children:"Sufficient computational resources (GPU recommended)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"understanding-isaac-ros-visual-slam",children:"Understanding Isaac ROS Visual SLAM"}),"\n",(0,r.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo Image Processing"}),": Converts stereo images to depth maps"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Visual SLAM"}),": Performs mapping and localization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pose Graph Optimization"}),": Refines trajectory estimates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loop Closure Detection"}),": Corrects drift over time"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"available-packages",children:"Available Packages"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"isaac_ros_visual_slam"}),": Main VSLAM package"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"isaac_ros_stereo_image_proc"}),": Stereo processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"isaac_ros_image_proc"}),": Image processing utilities"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"vslam-pipeline-configuration",children:"VSLAM Pipeline Configuration"}),"\n",(0,r.jsx)(n.h3,{id:"1-basic-visual-slam-node-configuration",children:"1. Basic Visual SLAM Node Configuration"}),"\n",(0,r.jsxs)(n.p,{children:["Create a configuration file ",(0,r.jsx)(n.code,{children:"vsland_pipeline.yaml"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# Visual SLAM pipeline configuration\nvisual_slam_node:\n  ros__parameters:\n    # Input topics\n    camera_pose_with_covariance_output_topic: "/visual_slam/camera_pose_with_covariance"\n    corrected_pose_with_covariance_output_topic: "/visual_slam/corrected_pose_with_covariance"\n    feedback_frame_id: "base_link"\n    enable_slam_visualization: true\n    enable_landmarks_view: false\n    enable_observations_view: false\n    map_frame: "map"\n    odom_frame: "odom"\n    base_frame: "base_link"\n    input_bound: 10\n    min_num_images_to_start: 3\n    enable_localization_n_mapping: true\n    enable_localization: false\n    enable_occupancy_map: false\n    enable_point_cloud_output: true\n    enable_imu: false\n    enable_rectification: true\n    rectified_frame_id: "base_link"\n    use_odometry_input: false\n    use_sim_time: false\n\n    # Feature tracking parameters\n    tracker:\n      max_num_points: 1000\n      min_distance: 20.0\n      quality_level: 0.01\n      pyramid_level: 3\n      window_size: [21, 21]\n      termination_criteria: [30, 0.01]\n      use_gpu: true  # Enable GPU acceleration\n\n    # Keyframe selection\n    keyframe:\n      translation_threshold: 0.5\n      rotation_threshold: 0.5\n      minimum_translation: 0.1\n      minimum_rotation: 0.1\n      maximum_observations_lost: 10\n      minimum_observations_for_tracking: 10\n\n    # Mapping parameters\n    mapping:\n      max_num_landmarks: 10000\n      minimum_observations_for_landmark: 2\n      maximum_observations_lost: 5\n      minimum_observations_for_tracking: 3\n      reprojection_threshold: 2.0\n      use_gpu: true\n\n    # Loop closure detection\n    loop_closure:\n      enable_loop_detection: true\n      minimum_loop_closure_interval: 10.0\n      minimum_translation_for_loop: 1.0\n      minimum_keyframes_for_loop: 10\n      similarity_threshold: 0.7\n      geometric_verification_threshold: 10.0\n      inlier_threshold: 0.9\n      use_gpu: true\n\n# Stereo image processing node\nstereo_image_proc:\n  ros__parameters:\n    # Input topics\n    left/image_raw: "/camera/left/image_raw"\n    left/camera_info: "/camera/left/camera_info"\n    right/image_raw: "/camera/right/image_raw"\n    right/camera_info: "/camera/right/camera_info"\n\n    # Output topics\n    left/image_rect: "/stereo/left/image_rect_color"\n    right/image_rect: "/stereo/right/image_rect_color"\n    disparity: "/stereo/disparity"\n\n    # Processing parameters\n    queue_size: 5\n    alpha: -1.0  # Auto-calculated rectification\n    use_camera_info: true\n    alpha_adjuster: "none"\n    stereo_algorithm: 0  # Block matching\n    disparity_range: [0, 128]\n    correlation_window_size: 15\n    prefilter_cap: 31\n    prefilter_size: 9\n    speckle_size: 100\n    speckle_range: 32\n    disp12_max_diff: 1\n    min_disparity: 0\n    uniqueness_ratio: 15\n    texture_threshold: 10\n    uniqueness_ratio: 15\n    P1: 200\n    P2: 400\n    full_dp: false\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-launch-file-configuration",children:"2. Launch File Configuration"}),"\n",(0,r.jsxs)(n.p,{children:["Create a launch file ",(0,r.jsx)(n.code,{children:"vsland_pipeline_launch.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.conditions import IfCondition\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom launch.event_handlers import OnProcessExit\nfrom ament_index_python.packages import get_package_share_directory\n\n\ndef generate_launch_description():\n    # Launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    vsland_config_file = LaunchConfiguration('vsland_config_file')\n    enable_visualization = LaunchConfiguration('enable_visualization', default='true')\n\n    # Declare launch arguments\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation clock if true')\n\n    declare_vsland_config_file = DeclareLaunchArgument(\n        'vsland_config_file',\n        default_value=os.path.join(\n            get_package_share_directory('your_robot_navigation'),\n            'config', 'vsland_pipeline.yaml'),\n        description='Full path to the VSLAM configuration file')\n\n    declare_enable_visualization = DeclareLaunchArgument(\n        'enable_visualization',\n        default_value='true',\n        description='Enable VSLAM visualization')\n\n    # Visual SLAM node\n    visual_slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='visual_slam_node',\n        parameters=[vsland_config_file,\n                   {'use_sim_time': use_sim_time}],\n        remappings=[('stereo_camera/left/image', '/camera/left/image_rect_color'),\n                   ('stereo_camera/left/camera_info', '/camera/left/camera_info'),\n                   ('stereo_camera/right/image', '/camera/right/image_rect_color'),\n                   ('stereo_camera/right/camera_info', '/camera/right/camera_info'),\n                   ('visual_slam/imu', '/imu/data')],\n        output='screen'\n    )\n\n    # Stereo image processing node (if using stereo)\n    stereo_image_proc_node = Node(\n        package='isaac_ros_stereo_image_proc',\n        executable='stereo_image_proc',\n        parameters=[vsland_config_file,\n                   {'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Point cloud mapper node\n    point_cloud_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='pointcloud_mapper_node',\n        parameters=[vsland_config_file,\n                   {'use_sim_time': use_sim_time}],\n        remappings=[('visual_slam/landmarks', '/visual_slam/landmarks'),\n                   ('visual_slam/optimized_landmarks', '/visual_slam/optimized_landmarks')],\n        output='screen'\n    )\n\n    # RViz2 node for visualization (optional)\n    rviz_node = Node(\n        condition=IfCondition(enable_visualization),\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', os.path.join(get_package_share_directory('isaac_ros_visual_slam'),\n                                     'rviz', 'visual_slam.rviz')],\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    # Create launch description\n    ld = LaunchDescription()\n\n    # Add launch arguments\n    ld.add_action(declare_use_sim_time)\n    ld.add_action(declare_vsland_config_file)\n    ld.add_action(declare_enable_visualization)\n\n    # Add nodes\n    ld.add_action(stereo_image_proc_node)\n    ld.add_action(visual_slam_node)\n    ld.add_action(point_cloud_node)\n    ld.add_action(rviz_node)\n\n    return ld\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-monocular-camera-configuration",children:"3. Monocular Camera Configuration"}),"\n",(0,r.jsx)(n.p,{children:"For monocular VSLAM, modify the configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# Monocular VSLAM configuration\nvisual_slam_node:\n  ros__parameters:\n    # Use monocular input instead of stereo\n    image_raw_topic: "/camera/image_raw"\n    camera_info_topic: "/camera/camera_info"\n\n    # Disable stereo-specific features\n    enable_imu: false\n    use_stereo: false\n\n    # Monocular-specific parameters\n    tracker:\n      max_num_points: 800  # Slightly fewer features for monocular\n      min_distance: 15.0\n      quality_level: 0.015  # Higher quality features for monocular\n      pyramidal_level: 4    # More pyramid levels for better tracking\n\n    # Scale estimation (monocular is scale ambiguous)\n    scale_estimator:\n      enable_scale_estimation: true\n      initial_scale: 1.0\n      scale_window_size: 10\n\n    # Depth estimation for monocular\n    depth_estimator:\n      enable_depth_estimation: true\n      max_depth: 10.0\n      min_depth: 0.3\n'})}),"\n",(0,r.jsx)(n.h3,{id:"4-rgb-d-camera-configuration",children:"4. RGB-D Camera Configuration"}),"\n",(0,r.jsx)(n.p,{children:"For RGB-D VSLAM:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# RGB-D VSLAM configuration\nvisual_slam_node:\n  ros__parameters:\n    # RGB-D specific topics\n    rgb_image_topic: "/camera/color/image_raw"\n    depth_image_topic: "/camera/depth/image_rect_raw"\n    camera_info_topic: "/camera/color/camera_info"\n\n    # RGB-D specific parameters\n    use_depth_input: true\n    depth_scale_factor: 1000.0  # Convert depth to meters\n    max_depth: 5.0\n    min_depth: 0.1\n\n    # Enhanced tracking with depth\n    tracker:\n      max_num_points: 1200  # More features with depth guidance\n      use_depth_for_tracking: true\n      depth_threshold: 0.1  # Depth consistency threshold\n\n    # Mapping improvements with depth\n    mapping:\n      enable_3d_landmarks: true\n      depth_uncertainty_threshold: 0.05\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"1-gpu-acceleration-configuration",children:"1. GPU Acceleration Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Enable GPU acceleration in the configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"visual_slam_node:\n  ros__parameters:\n    # GPU acceleration settings\n    use_gpu: true\n    gpu_id: 0\n    cuda_device_id: 0\n\n    # Feature extraction GPU settings\n    feature_extraction:\n      use_gpu: true\n      gpu_memory_pool_size: 100  # MB\n      gpu_max_memory_usage: 500  # MB\n\n    # Tracking GPU settings\n    tracking:\n      use_gpu: true\n      gpu_thread_count: 4\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-memory-management",children:"2. Memory Management"}),"\n",(0,r.jsx)(n.p,{children:"Optimize memory usage:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'visual_slam_node:\n  ros__parameters:\n    # Memory optimization\n    max_map_size: 50000  # Maximum landmarks in map\n    landmark_forgetting_policy: "oldest"  # Remove oldest landmarks\n    keyframe_forgetting_policy: "aggressive"  # Remove redundant keyframes\n\n    # Processing optimization\n    processing_queue_size: 3  # Limit processing backlog\n    max_processing_threads: 8  # CPU threads for processing\n'})}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-navigation-stack",children:"Integration with Navigation Stack"}),"\n",(0,r.jsx)(n.h3,{id:"1-tf-tree-configuration",children:"1. TF Tree Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Ensure proper TF relationships:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# TF configuration for VSLAM integration\nvisual_slam_node:\n  ros__parameters:\n    # TF frames\n    map_frame: "map"\n    odom_frame: "odom"\n    base_frame: "base_link"\n    camera_frame: "camera_link"\n\n    # TF publishing\n    publish_tf: true\n    tf_publish_rate: 50.0  # Hz\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-odometry-integration",children:"2. Odometry Integration"}),"\n",(0,r.jsx)(n.p,{children:"Combine VSLAM with other odometry sources:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"# Robot localization node configuration\nrobot_localization:\n  ros__parameters:\n    # VSLAM as odometry source\n    odom0: /visual_slam/odometry\n    odom0_config: [true, true, true,  # x, y, z\n                   true, true, true,  # roll, pitch, yaw\n                   false, false, false]  # No linear velocity\n    odom0_differential: false\n    odom0_relative: false\n\n    # Other odometry sources\n    odom1: /wheel/odometry  # Wheel encoders\n    odom1_config: [true, true, false,  # x, y, no z\n                   false, false, true,  # Only yaw from IMU\n                   false, false, false]\n\n    # Fusion parameters\n    frequency: 50.0\n    sensor_timeout: 0.1\n    two_d_mode: true\n    transform_time_offset: 0.0\n"})}),"\n",(0,r.jsx)(n.h2,{id:"calibration-requirements",children:"Calibration Requirements"}),"\n",(0,r.jsx)(n.h3,{id:"1-camera-calibration",children:"1. Camera Calibration"}),"\n",(0,r.jsx)(n.p,{children:"Ensure cameras are properly calibrated:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Check camera calibration\nros2 run camera_calibration_parsers read_calibration /path/to/calibration.yaml\n\n# Verify calibration parameters\nros2 topic echo /camera/camera_info\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-extrinsic-calibration",children:"2. Extrinsic Calibration"}),"\n",(0,r.jsx)(n.p,{children:"Calibrate camera-to-robot transforms:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# Extrinsic calibration in URDF\n<link name="camera_link">\n  <visual>\n    <origin xyz="0.1 0 0.2" rpy="0 0 0"/>\n    \x3c!-- Camera offset from base --\x3e\n  </visual>\n  <collision>\n    <origin xyz="0.1 0 0.2" rpy="0 0 0"/>\n  </collision>\n</link>\n'})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,r.jsx)(n.h3,{id:"issue-poor-tracking-performance",children:"Issue: Poor Tracking Performance"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Frequent tracking failures, drifting pose estimates\n",(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Increase feature count:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"tracker:\n  max_num_points: 1500\n  min_distance: 10.0\n"})}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Improve lighting conditions"}),"\n",(0,r.jsx)(n.li,{children:"Check camera calibration"}),"\n",(0,r.jsx)(n.li,{children:"Reduce motion speed during initialization"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"issue-map-drift",children:"Issue: Map Drift"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Map deforms over time, loop closure fails\n",(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Enable loop closure:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"loop_closure:\n  enable_loop_detection: true\n  minimum_translation_for_loop: 0.5\n"})}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Increase keyframe frequency"}),"\n",(0,r.jsx)(n.li,{children:"Improve feature tracking parameters"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"issue-high-cpugpu-usage",children:"Issue: High CPU/GPU Usage"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": System becomes unresponsive, frame drops\n",(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Reduce feature count:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"tracker:\n  max_num_points: 500\n"})}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Lower processing frequency"}),"\n",(0,r.jsx)(n.li,{children:"Use simpler stereo algorithms"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"issue-scale-ambiguity-monocular",children:"Issue: Scale Ambiguity (Monocular)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Map scale changes during operation\n",(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Use known object sizes for scale reference"}),"\n",(0,r.jsx)(n.li,{children:"Integrate with IMU for scale estimation"}),"\n",(0,r.jsx)(n.li,{children:"Use stereo or RGB-D instead of monocular"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"quality-assurance",children:"Quality Assurance"}),"\n",(0,r.jsx)(n.h3,{id:"1-performance-metrics",children:"1. Performance Metrics"}),"\n",(0,r.jsx)(n.p,{children:"Monitor VSLAM performance:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Track frame rates\nros2 topic hz /camera/image_raw\nros2 topic hz /visual_slam/odometry\n\n# Monitor processing times\nros2 run topic_tools relay /visual_slam/timing_stats\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-accuracy-validation",children:"2. Accuracy Validation"}),"\n",(0,r.jsx)(n.p,{children:"Validate mapping accuracy:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Compare with ground truth (if available)\nros2 run tf2_tools view_frames\nros2 topic echo /visual_slam/odometry --field pose.pose.position\n"})}),"\n",(0,r.jsx)(n.h2,{id:"advanced-configuration",children:"Advanced Configuration"}),"\n",(0,r.jsx)(n.h3,{id:"1-multi-camera-setup",children:"1. Multi-Camera Setup"}),"\n",(0,r.jsx)(n.p,{children:"For multiple cameras:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'visual_slam_node:\n  ros__parameters:\n    # Multi-camera parameters\n    enable_multi_camera: true\n    camera_ids: [0, 1, 2]\n\n    # Camera 0 (front)\n    camera_0:\n      topic: "/camera0/image_raw"\n      info_topic: "/camera0/camera_info"\n      transform: [0.2, 0, 0.3, 0, 0, 0]  # x, y, z, roll, pitch, yaw\n\n    # Camera 1 (left)\n    camera_1:\n      topic: "/camera1/image_raw"\n      info_topic: "/camera1/camera_info"\n      transform: [0.1, 0.1, 0.3, 0, 0, 1.57]  # 90 degree offset\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-dynamic-object-filtering",children:"2. Dynamic Object Filtering"}),"\n",(0,r.jsx)(n.p,{children:"Filter dynamic objects:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"visual_slam_node:\n  ros__parameters:\n    # Dynamic object handling\n    enable_dynamic_object_filtering: true\n    dynamic_object_threshold: 5.0  # Pixels of motion threshold\n    temporal_consistency_window: 10  # Frames for consistency check\n"})}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"1-environment-considerations",children:"1. Environment Considerations"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Ensure sufficient lighting"}),"\n",(0,r.jsx)(n.li,{children:"Avoid repetitive patterns"}),"\n",(0,r.jsx)(n.li,{children:"Provide distinctive visual features"}),"\n",(0,r.jsx)(n.li,{children:"Minimize motion blur"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-hardware-optimization",children:"2. Hardware Optimization"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use cameras with global shutters for fast motion"}),"\n",(0,r.jsx)(n.li,{children:"Ensure sufficient GPU memory"}),"\n",(0,r.jsx)(n.li,{children:"Use fast storage for map saving/loading"}),"\n",(0,r.jsx)(n.li,{children:"Maintain stable power supply"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-parameter-tuning",children:"3. Parameter Tuning"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Start with default parameters"}),"\n",(0,r.jsx)(n.li,{children:"Adjust one parameter at a time"}),"\n",(0,r.jsx)(n.li,{children:"Test in controlled environments first"}),"\n",(0,r.jsx)(n.li,{children:"Document working configurations"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,r.jsx)(n.h3,{id:"1-simple-test-environment",children:"1. Simple Test Environment"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Start with a simple, well-textured environment"}),"\n",(0,r.jsx)(n.li,{children:"Move robot slowly in straight lines"}),"\n",(0,r.jsx)(n.li,{children:"Verify pose estimates are reasonable"}),"\n",(0,r.jsx)(n.li,{children:"Check map quality and consistency"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-loop-closure-test",children:"2. Loop Closure Test"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Drive in a loop pattern"}),"\n",(0,r.jsx)(n.li,{children:"Verify loop closure detection"}),"\n",(0,r.jsx)(n.li,{children:"Check map consistency after closure"}),"\n",(0,r.jsx)(n.li,{children:"Measure drift correction"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_visual_slam/index.html",children:"Isaac ROS Visual SLAM Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://navigation.ros.org/tutorials/docs/navigation2_with_vslam.html",children:"ROS 2 Navigation with VSLAM Tutorial"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://wiki.ros.org/camera_calibration",children:"Camera Calibration Guide"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,r.jsx)(n.p,{children:"This guide provides a comprehensive configuration for Isaac ROS Visual SLAM pipeline. Proper configuration is essential for achieving accurate mapping and localization. Start with basic configurations and gradually optimize parameters based on your specific application requirements and hardware capabilities. The combination of visual SLAM with navigation systems enables powerful autonomous capabilities for robots operating in unknown environments."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},7074:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>t});var a=i(6540);const r={},o=a.createContext(r);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);