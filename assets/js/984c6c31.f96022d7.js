"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[2426],{3339:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"isaac-platform/perception-navigation-integration","title":"Perception Pipeline Integration with Navigation System","description":"Overview","source":"@site/docs/isaac-platform/perception-navigation-integration.md","sourceDirName":"isaac-platform","slug":"/isaac-platform/perception-navigation-integration","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/perception-navigation-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-platform/perception-navigation-integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Path Execution Controller for Robot Movement","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/path-execution-controller"},"next":{"title":"Isaac Navigation - Autonomous Navigation and Path Planning","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/navigation"}}');var i=t(4848),s=t(7074);const o={},r="Perception Pipeline Integration with Navigation System",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Understanding Perception-Navigation Integration",id:"understanding-perception-navigation-integration",level:2},{value:"Key Integration Points",id:"key-integration-points",level:3},{value:"Benefits of Integration",id:"benefits-of-integration",level:3},{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"1. High-Level Integration Architecture",id:"1-high-level-integration-architecture",level:3},{value:"Integration Implementation",id:"integration-implementation",level:2},{value:"1. Perception-Enhanced Costmap Configuration",id:"1-perception-enhanced-costmap-configuration",level:3},{value:"2. Perception-Enhanced Localization Configuration",id:"2-perception-enhanced-localization-configuration",level:3},{value:"3. Perception-Enhanced Controller Configuration",id:"3-perception-enhanced-controller-configuration",level:3},{value:"Custom Integration Nodes",id:"custom-integration-nodes",level:2},{value:"1. Perception-Navigation Bridge Node",id:"1-perception-navigation-bridge-node",level:3},{value:"2. Semantic Navigation Node",id:"2-semantic-navigation-node",level:3},{value:"Launch Files for Integration",id:"launch-files-for-integration",level:2},{value:"1. Complete Integration Launch File",id:"1-complete-integration-launch-file",level:3},{value:"Performance Optimization for Integration",id:"performance-optimization-for-integration",level:2},{value:"1. Optimized Integration Configuration",id:"1-optimized-integration-configuration",level:3},{value:"2. Memory and Computation Optimization",id:"2-memory-and-computation-optimization",level:3},{value:"Quality Assurance and Testing",id:"quality-assurance-and-testing",level:2},{value:"1. Integration Testing Framework",id:"1-integration-testing-framework",level:3},{value:"Troubleshooting Integration Issues",id:"troubleshooting-integration-issues",level:2},{value:"1. Common Integration Problems and Solutions",id:"1-common-integration-problems-and-solutions",level:3},{value:"Issue: Perception data not affecting navigation",id:"issue-perception-data-not-affecting-navigation",level:4},{value:"Issue: High CPU usage with integration",id:"issue-high-cpu-usage-with-integration",level:4},{value:"Issue: Navigation conflicts with perception",id:"issue-navigation-conflicts-with-perception",level:4},{value:"2. Performance Monitoring",id:"2-performance-monitoring",level:3},{value:"Best Practices for Integration",id:"best-practices-for-integration",level:2},{value:"1. Design Guidelines",id:"1-design-guidelines",level:3},{value:"2. Testing Strategies",id:"2-testing-strategies",level:3},{value:"3. Performance Optimization",id:"3-performance-optimization",level:3},{value:"Resources",id:"resources",level:2},{value:"Conclusion",id:"conclusion",level:2}];function _(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"perception-pipeline-integration-with-navigation-system",children:"Perception Pipeline Integration with Navigation System"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This guide provides comprehensive instructions for integrating Isaac ROS perception pipelines with Navigation2. The integration enables robots to use visual and sensor data for enhanced navigation, including dynamic obstacle detection, semantic mapping, and visual-inertial odometry for improved localization."}),"\n",(0,i.jsx)(n.h2,{id:"understanding-perception-navigation-integration",children:"Understanding Perception-Navigation Integration"}),"\n",(0,i.jsx)(n.h3,{id:"key-integration-points",children:"Key Integration Points"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Data Integration"}),": Feeding perception outputs to costmaps"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Localization Enhancement"}),": Using visual data to improve pose estimation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dynamic Obstacle Detection"}),": Real-time detection and avoidance of moving objects"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic Navigation"}),": Using object recognition for intelligent navigation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Visual-Inertial Odometry"}),": Enhanced motion estimation using visual data"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"benefits-of-integration",children:"Benefits of Integration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improved obstacle detection and classification"}),"\n",(0,i.jsx)(n.li,{children:"Enhanced navigation in dynamic environments"}),"\n",(0,i.jsx)(n.li,{children:"Better localization accuracy"}),"\n",(0,i.jsx)(n.li,{children:"Semantic-aware path planning"}),"\n",(0,i.jsx)(n.li,{children:"Robust operation in challenging conditions"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,i.jsx)(n.h3,{id:"1-high-level-integration-architecture",children:"1. High-Level Integration Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Isaac ROS Perception Pipeline\n\u251c\u2500\u2500 Visual SLAM \u2192 Pose/Odometry \u2192 AMCL/Localization\n\u251c\u2500\u2500 Stereo Processing \u2192 Depth \u2192 Costmap Obstacle Layer\n\u251c\u2500\u2500 Semantic Segmentation \u2192 Object Classification \u2192 Semantic Costmap\n\u251c\u2500\u2500 Object Detection \u2192 Dynamic Objects \u2192 Dynamic Obstacle Layer\n\u2514\u2500\u2500 IMU Integration \u2192 Visual-Inertial Odometry \u2192 Controller\n\nNavigation2 Stack\n\u251c\u2500\u2500 Global Planner (uses semantic map)\n\u251c\u2500\u2500 Local Planner (uses perception-enhanced costmaps)\n\u251c\u2500\u2500 Controller (uses enhanced localization)\n\u2514\u2500\u2500 Behavior Trees (uses semantic information)\n"})}),"\n",(0,i.jsx)(n.h2,{id:"integration-implementation",children:"Integration Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"1-perception-enhanced-costmap-configuration",children:"1. Perception-Enhanced Costmap Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["Create a configuration file ",(0,i.jsx)(n.code,{children:"perception_costmap_config.yaml"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Perception-enhanced costmap configuration\nglobal_costmap:\n  global_costmap:\n    ros__parameters:\n      update_frequency: 0.5\n      publish_frequency: 0.5\n      transform_tolerance: 1.0\n      use_sim_time: false\n\n      global_frame: map\n      robot_base_frame: base_link\n      robot_radius: 0.22\n      resolution: 0.05\n\n      plugins: [\n        "static_layer",\n        "obstacle_layer",\n        "isaac_perception_layer",\n        "semantic_layer",\n        "inflation_layer"\n      ]\n\n      static_layer:\n        plugin: "nav2_costmap_2d::StaticLayer"\n        map_subscribe_transient_local: True\n        transform_tolerance: 1.0\n\n      obstacle_layer:\n        plugin: "nav2_costmap_2d::ObstacleLayer"\n        enabled: True\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 5.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 4.0\n          obstacle_min_range: 0.0\n\n      # Isaac ROS perception layer\n      isaac_perception_layer:\n        plugin: "nav2_isaac_perception_layer/IsaacPerceptionLayer"\n        enabled: True\n        observation_sources: stereo_depth segmentation\n        stereo_depth:\n          topic: /stereo/depth/disparity\n          sensor_frame: stereo_camera_link\n          data_type: "Disparity"\n          clearing: True\n          marking: True\n          obstacle_range: 3.0\n          raytrace_range: 4.0\n        segmentation:\n          topic: /segmentation/segmentation_map\n          sensor_frame: camera_link\n          data_type: "Image"\n          clearing: False\n          marking: True\n          obstacle_value: 254\n          free_space_value: 0\n\n      # Semantic layer for object classification\n      semantic_layer:\n        plugin: "nav2_semantic_costmap_layer/SemanticLayer"\n        enabled: True\n        observation_sources: semantic_input\n        semantic_input:\n          topic: /semantic_segmentation/output\n          sensor_frame: camera_frame\n          data_type: "Image"\n          clearing: False\n          marking: True\n          class_mappings:\n            - {class_id: 0, class_name: "free_space", cost: 0, is_obstacle: false}\n            - {class_id: 1, class_name: "wall", cost: 254, is_obstacle: true}\n            - {class_id: 2, class_name: "person", cost: 200, is_obstacle: true}\n            - {class_id: 3, class_name: "furniture", cost: 254, is_obstacle: true}\n            - {class_id: 4, class_name: "plant", cost: 100, is_obstacle: true}\n            - {class_id: 5, class_name: "clutter", cost: 150, is_obstacle: true}\n\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 5.0\n        inflation_radius: 0.8\n        inflate_unknown: False\n\n  global_costmap_client:\n    ros__parameters:\n      use_sim_time: false\n  global_costmap_rclcpp_node:\n    ros__parameters:\n      use_sim_time: false\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 10.0\n      publish_frequency: 5.0\n      transform_tolerance: 0.2\n      use_sim_time: false\n\n      global_frame: odom\n      robot_base_frame: base_link\n      robot_radius: 0.22\n      resolution: 0.025\n\n      rolling_window: true\n      width: 6\n      height: 6\n      origin_x: -3.0\n      origin_y: -3.0\n\n      plugins: [\n        "voxel_layer",\n        "isaac_perception_layer",\n        "dynamic_object_layer",\n        "inflation_layer"\n      ]\n\n      voxel_layer:\n        plugin: "nav2_costmap_2d::VoxelLayer"\n        enabled: True\n        publish_voxel_map: True\n        origin_z: 0.0\n        z_resolution: 0.05\n        z_voxels: 16\n        max_obstacle_height: 2.0\n        mark_threshold: 0\n        observation_sources: scan\n        scan:\n          topic: /scan\n          max_obstacle_height: 2.0\n          clearing: True\n          marking: True\n          data_type: "LaserScan"\n          raytrace_max_range: 5.0\n          raytrace_min_range: 0.0\n          obstacle_max_range: 4.0\n          obstacle_min_range: 0.0\n\n      # Isaac ROS perception layer for local costmap\n      isaac_perception_layer:\n        plugin: "nav2_isaac_perception_layer/IsaacPerceptionLayer"\n        enabled: True\n        observation_sources: stereo_depth segmentation\n        stereo_depth:\n          topic: /stereo/depth/disparity\n          sensor_frame: stereo_camera_link\n          data_type: "Disparity"\n          clearing: True\n          marking: True\n        segmentation:\n          topic: /segmentation/segmentation_map\n          sensor_frame: camera_link\n          data_type: "Image"\n          clearing: False\n          marking: True\n\n      # Dynamic object layer for moving obstacles\n      dynamic_object_layer:\n        plugin: "nav2_dynamic_obstacles_layer/DynamicObstaclesLayer"\n        enabled: True\n        observation_sources: dynamic_objects\n        dynamic_objects:\n          topic: /dynamic_objects\n          sensor_frame: base_link\n          data_type: "PointCloud2"\n          clearing: True\n          marking: True\n          obstacle_value: 254\n          free_space_value: 0\n\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 8.0\n        inflation_radius: 0.6\n        inflate_unknown: False\n\n  local_costmap_client:\n    ros__parameters:\n      use_sim_time: false\n  local_costmap_rclcpp_node:\n    ros__parameters:\n      use_sim_time: false\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-perception-enhanced-localization-configuration",children:"2. Perception-Enhanced Localization Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["Create a localization configuration file ",(0,i.jsx)(n.code,{children:"perception_localization.yaml"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Perception-enhanced localization configuration\namcl:\n  ros__parameters:\n    use_sim_time: false\n    alpha1: 0.2\n    alpha2: 0.2\n    alpha3: 0.2\n    alpha4: 0.2\n    alpha5: 0.2\n    base_frame_id: "base_link"\n    beam_skip_distance: 0.5\n    beam_skip_error_threshold: 0.9\n    beam_skip_threshold: 0.3\n    do_beamskip: false\n    global_frame_id: "map"\n    lambda_short: 0.1\n    laser_likelihood_max_dist: 2.0\n    laser_max_range: 100.0\n    laser_min_range: -1.0\n    laser_model_type: "likelihood_field"\n    max_beams: 60\n    max_particles: 2000\n    min_particles: 500\n    odom_frame_id: "odom"\n    pf_err: 0.05\n    pf_z: 0.99\n    recovery_alpha_fast: 0.0\n    recovery_alpha_slow: 0.0\n    resample_interval: 1\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\n    save_pose_rate: 0.5\n    sigma_hit: 0.2\n    tf_broadcast: true\n    transform_tolerance: 1.0\n    update_min_a: 0.2\n    update_min_d: 0.25\n    z_hit: 0.5\n    z_max: 0.05\n    z_rand: 0.5\n    z_short: 0.05\n    scan_topic: scan\n\n# Robot localization for visual-inertial fusion\nrobot_localization:\n  ros__parameters:\n    # Visual SLAM as primary pose source\n    pose0: /visual_slam/odometry\n    pose0_config: [true, true, false,    # x, y, no z\n                   false, false, true,   # no roll, pitch, yes yaw\n                   false, false, false]  # no linear/angular velocity\n    pose0_differential: false\n    pose0_relative: false\n\n    # Wheel odometry for local motion\n    twist0: /wheel/odometry\n    twist0_config: [false, false, false,  # no position\n                    false, false, false,  # no orientation\n                    true, true, true]     # linear velocity x,y,z\n    twist0_differential: false\n    twist0_relative: false\n\n    # IMU for orientation\n    imu0: /imu/data\n    imu0_config: [false, false, false,    # no position\n                  true, true, true,       # orientation x,y,z\n                  false, false, false]    # no angular velocity/linear acceleration\n    imu0_differential: false\n    imu0_relative: false\n    imu0_remove_gravitational_acceleration: true\n\n    # Fusion parameters\n    frequency: 50.0\n    sensor_timeout: 0.1\n    two_d_mode: true\n    transform_time_offset: 0.0\n    transform_timeout: 0.0\n    print_diagnostics: false\n\n    # Process noise\n    process_noise_covariance: [0.05, 0.0,    0.0,    0.0,    0.0,    0.0,\n                              0.0,    0.05, 0.0,    0.0,    0.0,    0.0,\n                              0.0,    0.0,    0.06, 0.0,    0.0,    0.0,\n                              0.0,    0.0,    0.0,    0.03, 0.0,    0.0,\n                              0.0,    0.0,    0.0,    0.0,    0.03, 0.0,\n                              0.0,    0.0,    0.0,    0.0,    0.0,    0.06]\n\n    # Initial estimate covariance\n    initial_estimate_covariance: [0.1, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                 0.0, 0.1, 0.0, 0.0, 0.0, 0.0,\n                                 0.0, 0.0, 0.1, 0.0, 0.0, 0.0,\n                                 0.0, 0.0, 0.0, 0.05, 0.0, 0.0,\n                                 0.0, 0.0, 0.0, 0.0, 0.05, 0.0,\n                                 0.0, 0.0, 0.0, 0.0, 0.0, 0.05]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-perception-enhanced-controller-configuration",children:"3. Perception-Enhanced Controller Configuration"}),"\n",(0,i.jsx)(n.p,{children:"Create a controller configuration with perception feedback:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'controller_server:\n  ros__parameters:\n    use_sim_time: false\n    controller_frequency: 30.0  # Higher for perception integration\n    min_x_velocity_threshold: 0.001\n    min_y_velocity_threshold: 0.001\n    min_theta_velocity_threshold: 0.001\n    failure_tolerance: 0.3\n    progress_checker_plugin: "perception_progress_checker"\n    goal_checker_plugins: ["perception_goal_checker"]\n    controller_plugins: ["PerceptionAwareController", "BackupController"]\n\n    # Perception-aware progress checker\n    perception_progress_checker:\n      plugin: "nav2_controller::PerceptionProgressChecker"\n      required_movement_radius: 0.3\n      movement_time_allowance: 5.0\n      visual_confidence_threshold: 0.6\n\n    # Perception-aware goal checker\n    perception_goal_checker:\n      plugin: "nav2_controller::PerceptionGoalChecker"\n      xy_goal_tolerance: 0.2\n      yaw_goal_tolerance: 0.2\n      stateful: True\n      visual_alignment_tolerance: 0.3\n\n    # Main perception-aware controller\n    PerceptionAwareController:\n      plugin: "dwb_core::DWBLocalPlanner"\n      debug_trajectory_details: False\n      min_vel_x: 0.05\n      min_vel_y: 0.0\n      max_vel_x: 0.6\n      max_vel_y: 0.0\n      max_vel_theta: 1.5\n      min_speed_xy: 0.05\n      max_speed_xy: 0.6\n      min_speed_theta: 0.1\n      acc_lim_x: 2.0\n      acc_lim_y: 0.0\n      acc_lim_theta: 3.0\n      decel_lim_x: -2.0\n      decel_lim_y: 0.0\n      decel_lim_theta: -3.0\n      vx_samples: 25\n      vy_samples: 5\n      vtheta_samples: 25\n      sim_time: 1.8\n      linear_granularity: 0.04\n      angular_granularity: 0.02\n      transform_tolerance: 0.1\n      xy_goal_tolerance: 0.2\n      yaw_goal_tolerance: 0.2\n      stateful: True\n      restore_defaults: False\n      publish_cost_grid_pc: False\n      use_dwb: True\n      max_vel_obstacle: 1.8\n      # Perception-specific parameters\n      perception_weight: 0.7\n      visual_inertial_fusion: true\n      dynamic_object_aware: true\n\n    # Backup controller for emergency situations\n    BackupController:\n      plugin: "nav2_controller::BackUpController"\n      min_vel_x: -0.2\n      max_vel_x: -0.1\n      acc_lim_x: 1.0\n      decel_lim_x: -1.0\n      sim_time: 1.0\n      vx_samples: 10\n      tolerance: 0.1\n      threshold_to_rotate: 0.2\n'})}),"\n",(0,i.jsx)(n.h2,{id:"custom-integration-nodes",children:"Custom Integration Nodes"}),"\n",(0,i.jsx)(n.h3,{id:"1-perception-navigation-bridge-node",children:"1. Perception-Navigation Bridge Node"}),"\n",(0,i.jsx)(n.p,{children:"Create a bridge node that connects perception outputs to navigation inputs:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nPerception-Navigation Integration Bridge\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, PointCloud2, LaserScan\nfrom geometry_msgs.msg import Twist, PoseWithCovarianceStamped\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import Header\nfrom builtin_interfaces.msg import Duration\nimport numpy as np\nfrom tf2_ros import TransformException\nfrom tf2_ros.buffer import Buffer\nfrom tf2_ros.transform_listener import TransformListener\n\n\nclass PerceptionNavigationBridge(Node):\n    def __init__(self):\n        super().__init__(\'perception_navigation_bridge\')\n\n        # Parameters\n        self.declare_parameter(\'perception_timeout\', 1.0)\n        self.declare_parameter(\'dynamic_object_threshold\', 0.5)\n        self.declare_parameter(\'semantic_confidence_threshold\', 0.7)\n\n        self.perception_timeout = self.get_parameter(\'perception_timeout\').value\n        self.dynamic_threshold = self.get_parameter(\'dynamic_object_threshold\').value\n        self.confidence_threshold = self.get_parameter(\'semantic_confidence_threshold\').value\n\n        # TF buffer for coordinate transformations\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Perception input topics\n        self.semantic_sub = self.create_subscription(\n            Image, \'/segmentation/segmentation_map\', self.semantic_callback, 10)\n        self.dynamic_obj_sub = self.create_subscription(\n            MarkerArray, \'/dynamic_objects\', self.dynamic_objects_callback, 10)\n        self.stereo_depth_sub = self.create_subscription(\n            Image, \'/stereo/depth/disparity\', self.stereo_depth_callback, 10)\n\n        # Navigation output topics\n        self.dynamic_costmap_pub = self.create_publisher(\n            PointCloud2, \'/local_costmap/dynamic_layer/clearing_endpoints\', 10)\n        self.semantic_costmap_pub = self.create_publisher(\n            Image, \'/semantic_costmap_input\', 10)\n        self.perception_status_pub = self.create_publisher(\n            Header, \'/perception_navigation_status\', 10)\n\n        # Internal state\n        self.last_perception_time = self.get_clock().now()\n        self.dynamic_objects = []\n        self.semantic_map = None\n        self.stereo_depth = None\n\n        # Timer for periodic processing\n        self.process_timer = self.create_timer(0.1, self.process_perception_data)\n\n        self.get_logger().info(\'Perception-Navigation Bridge initialized\')\n\n    def semantic_callback(self, msg):\n        """Process semantic segmentation data"""\n        self.semantic_map = msg\n        self.last_perception_time = self.get_clock().now()\n        self.get_logger().debug(\'Received semantic segmentation data\')\n\n    def dynamic_objects_callback(self, msg):\n        """Process dynamic object detections"""\n        self.dynamic_objects = msg.markers\n        self.last_perception_time = self.get_clock().now()\n        self.get_logger().debug(f\'Received {len(msg.markers)} dynamic objects\')\n\n    def stereo_depth_callback(self, msg):\n        """Process stereo depth data"""\n        self.stereo_depth = msg\n        self.last_perception_time = self.get_clock().now()\n        self.get_logger().debug(\'Received stereo depth data\')\n\n    def process_perception_data(self):\n        """Process perception data and publish to navigation system"""\n        current_time = self.get_clock().now()\n\n        # Check if perception data is still valid\n        if (current_time - self.last_perception_time).nanoseconds / 1e9 > self.perception_timeout:\n            self.get_logger().warn(\'Perception data timeout, navigation may be degraded\')\n            return\n\n        # Process dynamic objects for costmap\n        if self.dynamic_objects:\n            self.publish_dynamic_objects_to_costmap()\n\n        # Process semantic data for semantic costmap\n        if self.semantic_map:\n            self.publish_semantic_to_costmap()\n\n        # Process stereo depth for obstacle layer\n        if self.stereo_depth:\n            self.publish_depth_to_costmap()\n\n        # Publish status\n        status_msg = Header()\n        status_msg.stamp = current_time.to_msg()\n        status_msg.frame_id = "perception_active"\n        self.perception_status_pub.publish(status_msg)\n\n    def publish_dynamic_objects_to_costmap(self):\n        """Convert dynamic objects to costmap-compatible format"""\n        # Create point cloud from dynamic objects\n        points = []\n        for obj in self.dynamic_objects:\n            if obj.type == 1:  # Sphere type\n                # Check if object is moving (dynamic)\n                if self.is_moving_object(obj):\n                    points.append([obj.pose.position.x, obj.pose.position.y, obj.pose.position.z])\n\n        if points:\n            # Convert to PointCloud2 message\n            pc2_msg = self.create_pointcloud2(points, "odom")\n            self.dynamic_costmap_pub.publish(pc2_msg)\n\n    def is_moving_object(self, marker):\n        """Determine if a detected object is moving"""\n        # In a real implementation, this would check object velocity\n        # For now, we\'ll assume all detected objects are potentially dynamic\n        return True\n\n    def publish_semantic_to_costmap(self):\n        """Publish semantic data to semantic costmap layer"""\n        # In a real implementation, this would convert semantic segmentation\n        # to a format compatible with semantic costmap layer\n        # For now, we just republish the semantic map\n        self.semantic_costmap_pub.publish(self.semantic_map)\n\n    def publish_depth_to_costmap(self):\n        """Convert depth data to costmap-compatible format"""\n        # Process depth image and convert to obstacle points\n        # This would typically involve:\n        # 1. Converting disparity to depth\n        # 2. Transforming to costmap frame\n        # 3. Creating obstacle points\n        pass\n\n    def create_pointcloud2(self, points, frame_id):\n        """Create a PointCloud2 message from a list of points"""\n        from sensor_msgs.msg import PointCloud2, PointField\n        import struct\n\n        # Create PointCloud2 message\n        pc2_msg = PointCloud2()\n        pc2_msg.header = Header()\n        pc2_msg.header.stamp = self.get_clock().now().to_msg()\n        pc2_msg.header.frame_id = frame_id\n        pc2_msg.height = 1\n        pc2_msg.width = len(points)\n        pc2_msg.is_dense = False\n        pc2_msg.is_bigendian = False\n\n        # Define point fields\n        pc2_msg.fields = [\n            PointField(name=\'x\', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'y\', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name=\'z\', offset=8, datatype=PointField.FLOAT32, count=1)\n        ]\n\n        pc2_msg.point_step = 12  # 3 floats * 4 bytes\n        pc2_msg.row_step = pc2_msg.point_step * pc2_msg.width\n\n        # Pack the data\n        data = []\n        for point in points:\n            data.append(struct.pack(\'fff\', point[0], point[1], point[2]))\n\n        pc2_msg.data = b\'\'.join(data)\n        return pc2_msg\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    bridge = PerceptionNavigationBridge()\n\n    try:\n        rclpy.spin(bridge)\n    except KeyboardInterrupt:\n        bridge.get_logger().info(\'Perception-Navigation Bridge stopped by user\')\n    finally:\n        bridge.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-semantic-navigation-node",children:"2. Semantic Navigation Node"}),"\n",(0,i.jsx)(n.p,{children:"Create a node that uses semantic information for intelligent navigation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSemantic Navigation Node\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom nav_msgs.msg import Path\nfrom sensor_msgs.msg import Image\nfrom visualization_msgs.msg import MarkerArray\nfrom std_msgs.msg import String, Int32\nfrom action_msgs.msg import GoalStatus\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nimport numpy as np\n\n\nclass SemanticNavigationNode(Node):\n    def __init__(self):\n        super().__init__(\'semantic_navigation_node\')\n\n        # Parameters\n        self.declare_parameter(\'semantic_class_weights\', [0.1, 10.0, 8.0, 10.0, 5.0, 6.0])\n        self.declare_parameter(\'navigation_priority_threshold\', 0.8)\n\n        self.class_weights = self.get_parameter(\'semantic_class_weights\').value\n        self.priority_threshold = self.get_parameter(\'navigation_priority_threshold\').value\n\n        # Semantic class mapping\n        self.semantic_classes = {\n            0: "free_space",\n            1: "wall",\n            2: "person",\n            3: "furniture",\n            4: "plant",\n            5: "clutter"\n        }\n\n        # Navigation action client\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Subscriptions\n        self.semantic_sub = self.create_subscription(\n            Image, \'/segmentation/segmentation_map\', self.semantic_callback, 10)\n        self.dynamic_objects_sub = self.create_subscription(\n            MarkerArray, \'/dynamic_objects\', self.dynamic_objects_callback, 10)\n\n        # Publishers\n        self.semantic_path_pub = self.create_publisher(\n            Path, \'/semantic_plan\', 10)\n        self.navigation_intent_pub = self.create_publisher(\n            String, \'/navigation_intent\', 10)\n\n        # Internal state\n        self.current_semantic_map = None\n        self.dynamic_objects = []\n        self.navigation_goals = []\n\n        self.get_logger().info(\'Semantic Navigation Node initialized\')\n\n    def semantic_callback(self, msg):\n        """Process semantic segmentation data"""\n        self.current_semantic_map = msg\n        self.get_logger().debug(\'Processed semantic segmentation data\')\n\n    def dynamic_objects_callback(self, msg):\n        """Process dynamic object detections"""\n        self.dynamic_objects = msg.markers\n        self.get_logger().debug(f\'Processed {len(msg.markers)} dynamic objects\')\n\n    def navigate_with_semantic_awareness(self, goal_pose, target_object_class=None):\n        """Navigate with semantic awareness"""\n        if target_object_class:\n            # If navigating to a specific object, use semantic guidance\n            goal_pose = self.adjust_goal_for_target_object(goal_pose, target_object_class)\n\n        # Send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = goal_pose\n\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal_msg)\n\n        # Publish navigation intent\n        intent_msg = String()\n        intent_msg.data = f"navigating_to_{target_object_class if target_object_class else \'position\'}"\n        self.navigation_intent_pub.publish(intent_msg)\n\n        return future\n\n    def adjust_goal_for_target_object(self, goal_pose, target_class):\n        """Adjust navigation goal based on target object class"""\n        if not self.current_semantic_map:\n            return goal_pose\n\n        # In a real implementation, this would:\n        # 1. Analyze semantic map to find instances of target_class\n        # 2. Adjust goal to approach the target object appropriately\n        # 3. Consider object orientation and approach angle\n\n        # For now, just return the original goal\n        return goal_pose\n\n    def get_semantic_path_cost(self, path):\n        """Calculate cost of a path based on semantic information"""\n        if not self.current_semantic_map:\n            return float(\'inf\')\n\n        total_cost = 0\n        for pose in path.poses:\n            # Get semantic class at this position\n            semantic_class = self.get_semantic_class_at_position(\n                pose.pose.position.x, pose.pose.position.y)\n\n            # Apply class-specific cost\n            if semantic_class < len(self.class_weights):\n                cost = self.class_weights[semantic_class]\n                total_cost += cost\n\n        return total_cost\n\n    def get_semantic_class_at_position(self, x, y):\n        """Get semantic class at a specific world position"""\n        # This would involve transforming world coordinates to image coordinates\n        # and sampling the semantic segmentation map\n        # For now, return a default value\n        return 0  # free space\n\n    def create_semantic_aware_path(self, start, goal, preferred_classes=None):\n        """Create a path that considers semantic information"""\n        # This would implement a semantic-aware path planning algorithm\n        # that prefers certain semantic classes over others\n        path = Path()\n        path.header.frame_id = "map"\n\n        # For now, create a simple straight-line path\n        # In reality, this would use semantic information to guide path planning\n        current = start\n        steps = 10\n        for i in range(steps + 1):\n            t = i / steps\n            x = start.x + t * (goal.x - start.x)\n            y = start.y + t * (goal.y - start.y)\n            z = start.z + t * (goal.z - start.z)\n\n            pose_stamped = PoseStamped()\n            pose_stamped.pose.position.x = x\n            pose_stamped.pose.position.y = y\n            pose_stamped.pose.position.z = z\n            path.poses.append(pose_stamped)\n\n        return path\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    semantic_nav = SemanticNavigationNode()\n\n    try:\n        rclpy.spin(semantic_nav)\n    except KeyboardInterrupt:\n        semantic_nav.get_logger().info(\'Semantic Navigation Node stopped by user\')\n    finally:\n        semantic_nav.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"launch-files-for-integration",children:"Launch Files for Integration"}),"\n",(0,i.jsx)(n.h3,{id:"1-complete-integration-launch-file",children:"1. Complete Integration Launch File"}),"\n",(0,i.jsxs)(n.p,{children:["Create a launch file ",(0,i.jsx)(n.code,{children:"perception_navigation_integration_launch.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, RegisterEventHandler\nfrom launch.conditions import IfCondition\nfrom launch.event_handlers import OnProcessExit\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\n\ndef generate_launch_description():\n    # Launch configuration variables\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n    params_file = LaunchConfiguration('params_file')\n    run_rviz = LaunchConfiguration('run_rviz', default='true')\n    autostart = LaunchConfiguration('autostart', default='true')\n\n    # Declare launch arguments\n    declare_use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation clock if true')\n\n    declare_params_file = DeclareLaunchArgument(\n        'params_file',\n        default_value=os.path.join(\n            get_package_share_directory('your_robot_navigation'),\n            'config', 'perception_navigation.yaml'),\n        description='Full path to the ROS2 parameters file to use for all launched nodes')\n\n    declare_autostart = DeclareLaunchArgument(\n        'autostart',\n        default_value='true',\n        description='Automatically startup the nav2 stack')\n\n    declare_run_rviz = DeclareLaunchArgument(\n        'run_rviz',\n        default_value='true',\n        description='Whether to start RViz')\n\n    # Isaac ROS Visual SLAM node\n    visual_slam_node = Node(\n        package='isaac_ros_visual_slam',\n        executable='visual_slam_node',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'enable_occupancy_map': True,\n            'enable_point_cloud_output': True\n        }],\n        remappings=[\n            ('/stereo_camera/left/image', '/camera/left/image_rect_color'),\n            ('/stereo_camera/left/camera_info', '/camera/left/camera_info'),\n            ('/stereo_camera/right/image', '/camera/right/image_rect_color'),\n            ('/stereo_camera/right/camera_info', '/camera/right/camera_info'),\n        ],\n        output='screen'\n    )\n\n    # Isaac ROS stereo image processing\n    stereo_image_proc_node = Node(\n        package='isaac_ros_stereo_image_proc',\n        executable='stereo_image_proc',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n\n    # Isaac ROS segmentation\n    segmentation_node = Node(\n        package='isaac_ros_dnn_segmentation',\n        executable='dnn_segmentation_node',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'model_name': 'unet',\n            'input_topic': '/camera/image_raw',\n            'output_topic': '/segmentation/segmentation_map'\n        }],\n        output='screen'\n    )\n\n    # Perception-Navigation bridge\n    perception_bridge_node = Node(\n        package='your_robot_navigation',\n        executable='perception_navigation_bridge',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'perception_timeout': 1.0,\n            'dynamic_object_threshold': 0.5\n        }],\n        output='screen'\n    )\n\n    # Semantic navigation node\n    semantic_navigation_node = Node(\n        package='your_robot_navigation',\n        executable='semantic_navigation_node',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'semantic_class_weights': [0.1, 10.0, 8.0, 10.0, 5.0, 6.0]\n        }],\n        output='screen'\n    )\n\n    # Navigation2 stack\n    navigation2_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource(\n            os.path.join(\n                get_package_share_directory('nav2_bringup'),\n                'launch', 'navigation_launch.py')),\n        launch_arguments={\n            'use_sim_time': use_sim_time,\n            'params_file': params_file,\n            'autostart': autostart\n        }.items()\n    )\n\n    # RViz2\n    rviz_node = Node(\n        condition=IfCondition(run_rviz),\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', os.path.join(\n            get_package_share_directory('nav2_bringup'),\n            'rviz', 'nav2_default_view.rviz')],\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n\n    # Create launch description\n    ld = LaunchDescription()\n\n    # Add launch arguments\n    ld.add_action(declare_use_sim_time)\n    ld.add_action(declare_params_file)\n    ld.add_action(declare_autostart)\n    ld.add_action(declare_run_rviz)\n\n    # Add Isaac ROS nodes\n    ld.add_action(stereo_image_proc_node)\n    ld.add_action(visual_slam_node)\n    ld.add_action(segmentation_node)\n\n    # Add perception-navigation bridge\n    ld.add_action(perception_bridge_node)\n    ld.add_action(semantic_navigation_node)\n\n    # Add Navigation2 stack\n    ld.add_action(navigation2_launch)\n\n    # Add RViz2\n    ld.add_action(rviz_node)\n\n    return ld\n"})}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization-for-integration",children:"Performance Optimization for Integration"}),"\n",(0,i.jsx)(n.h3,{id:"1-optimized-integration-configuration",children:"1. Optimized Integration Configuration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Optimized perception-navigation integration\nglobal_costmap:\n  global_costmap:\n    ros__parameters:\n      update_frequency: 0.5  # Lower for global map\n      publish_frequency: 0.5\n      transform_tolerance: 1.0\n      use_sim_time: false\n\n      global_frame: map\n      robot_base_frame: base_link\n      robot_radius: 0.22\n      resolution: 0.05\n\n      plugins: ["static_layer", "isaac_perception_layer", "inflation_layer"]\n\n      static_layer:\n        plugin: "nav2_costmap_2d::StaticLayer"\n        map_subscribe_transient_local: True\n\n      # Optimized Isaac perception layer\n      isaac_perception_layer:\n        plugin: "nav2_isaac_perception_layer/IsaacPerceptionLayer"\n        enabled: True\n        observation_sources: segmentation\n        segmentation:\n          topic: /segmentation/segmentation_map\n          sensor_frame: camera_link\n          data_type: "Image"\n          clearing: False\n          marking: True\n          obstacle_value: 254\n          free_space_value: 0\n          observation_persistence: 0.5  # Keep observations for 0.5s\n\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 3.0\n        inflation_radius: 0.5\n\nlocal_costmap:\n  local_costmap:\n    ros__parameters:\n      update_frequency: 15.0  # Higher for local planning\n      publish_frequency: 10.0\n      transform_tolerance: 0.2\n      use_sim_time: false\n\n      global_frame: odom\n      robot_base_frame: base_link\n      robot_radius: 0.22\n      resolution: 0.025\n\n      rolling_window: true\n      width: 4\n      height: 4\n      origin_x: -2.0\n      origin_y: -2.0\n\n      plugins: ["voxel_layer", "isaac_perception_layer", "inflation_layer"]\n\n      voxel_layer:\n        plugin: "nav2_costmap_2d::VoxelLayer"\n        enabled: True\n        publish_voxel_map: False  # Disable for performance\n        origin_z: 0.0\n        z_resolution: 0.05\n        z_voxels: 8  # Reduce for performance\n        max_obstacle_height: 2.0\n        mark_threshold: 0\n        observation_sources: scan\n        scan:\n          topic: /scan\n\n      # Optimized perception layer for local costmap\n      isaac_perception_layer:\n        plugin: "nav2_isaac_perception_layer/IsaacPerceptionLayer"\n        enabled: True\n        observation_sources: stereo_depth\n        stereo_depth:\n          topic: /stereo/depth/disparity\n          sensor_frame: stereo_camera_link\n          data_type: "Disparity"\n          clearing: True\n          marking: True\n          obstacle_range: 2.0  # Shorter range for local planning\n          raytrace_range: 3.0\n\n      inflation_layer:\n        plugin: "nav2_costmap_2d::InflationLayer"\n        cost_scaling_factor: 5.0\n        inflation_radius: 0.4\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-memory-and-computation-optimization",children:"2. Memory and Computation Optimization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:'# Memory-optimized integration configuration\ncontroller_server:\n  ros__parameters:\n    use_sim_time: false\n    controller_frequency: 20.0  # Balance performance and control quality\n    min_x_velocity_threshold: 0.001\n    min_theta_velocity_threshold: 0.001\n    failure_tolerance: 0.5\n    progress_checker_plugin: "simple_progress_checker"\n    goal_checker_plugins: ["simple_goal_checker"]\n    controller_plugins: ["SimplePerceptionController"]\n\n    simple_progress_checker:\n      plugin: "nav2_controller::SimpleProgressChecker"\n      required_movement_radius: 0.5\n      movement_time_allowance: 10.0\n\n    simple_goal_checker:\n      plugin: "nav2_controller::SimpleGoalChecker"\n      xy_goal_tolerance: 0.3\n      yaw_goal_tolerance: 0.3\n      stateful: False  # Disable stateful behavior for performance\n\n    SimplePerceptionController:\n      plugin: "dwb_core::DWBLocalPlanner"\n      debug_trajectory_details: False\n      min_vel_x: 0.1\n      max_vel_x: 0.4\n      max_vel_theta: 1.0\n      vx_samples: 15  # Reduce samples for performance\n      vtheta_samples: 15\n      sim_time: 1.0  # Shorter simulation time\n      linear_granularity: 0.1\n      angular_granularity: 0.05\n      transform_tolerance: 0.2\n      xy_goal_tolerance: 0.3\n      stateful: False\n      publish_cost_grid_pc: False  # Disable for performance\n'})}),"\n",(0,i.jsx)(n.h2,{id:"quality-assurance-and-testing",children:"Quality Assurance and Testing"}),"\n",(0,i.jsx)(n.h3,{id:"1-integration-testing-framework",children:"1. Integration Testing Framework"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nPerception-Navigation Integration Testing\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Bool, Float32\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nimport time\n\n\nclass IntegrationTester(Node):\n    def __init__(self):\n        super().__init__(\'integration_tester\')\n\n        # Parameters\n        self.declare_parameter(\'test_duration\', 30.0)\n        self.declare_parameter(\'success_threshold\', 0.9)\n\n        self.test_duration = self.get_parameter(\'test_duration\').value\n        self.success_threshold = self.get_parameter(\'success_threshold\').value\n\n        # Publishers and subscribers\n        self.test_status_pub = self.create_publisher(Bool, \'/integration_test_status\', 10)\n        self.test_result_pub = self.create_publisher(Float32, \'/integration_test_result\', 10)\n\n        # Navigation action client\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Test parameters\n        self.test_start_time = None\n        self.test_active = False\n        self.success_count = 0\n        self.total_attempts = 0\n\n        # Timer for periodic testing\n        self.test_timer = self.create_timer(5.0, self.run_test_cycle)\n\n        self.get_logger().info(\'Integration Tester initialized\')\n\n    def run_test_cycle(self):\n        """Run a navigation test cycle"""\n        if not self.test_active:\n            self.start_test()\n            return\n\n        # Check if test duration has elapsed\n        if (self.get_clock().now().nanoseconds / 1e9 - self.test_start_time) > self.test_duration:\n            self.end_test()\n            return\n\n        # Run navigation test\n        self.run_navigation_test()\n\n    def start_test(self):\n        """Start integration test"""\n        self.test_active = True\n        self.test_start_time = self.get_clock().now().nanoseconds / 1e9\n        self.get_logger().info(f\'Starting integration test for {self.test_duration} seconds\')\n\n    def run_navigation_test(self):\n        """Run a single navigation test"""\n        # Define test goal (this would be more sophisticated in practice)\n        goal_pose = PoseStamped()\n        goal_pose.header.frame_id = "map"\n        goal_pose.pose.position.x = 2.0\n        goal_pose.pose.position.y = 2.0\n        goal_pose.pose.position.z = 0.0\n        goal_pose.pose.orientation.w = 1.0\n\n        # Send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = goal_pose\n\n        if self.nav_client.wait_for_server(timeout_sec=1.0):\n            self.total_attempts += 1\n\n            # Send goal and wait for result\n            future = self.nav_client.send_goal_async(goal_msg)\n            # In a real test, we\'d wait for the result and check success\n            # For this example, we\'ll simulate success/failure\n            success = True  # Simulated result\n            if success:\n                self.success_count += 1\n\n    def end_test(self):\n        """End integration test and report results"""\n        success_rate = self.success_count / self.total_attempts if self.total_attempts > 0 else 0\n        self.get_logger().info(f\'Test completed - Success rate: {success_rate:.2f} ({self.success_count}/{self.total_attempts})\')\n\n        # Publish results\n        result_msg = Float32()\n        result_msg.data = success_rate\n        self.test_result_pub.publish(result_msg)\n\n        status_msg = Bool()\n        status_msg.data = success_rate >= self.success_threshold\n        self.test_status_pub.publish(status_msg)\n\n        self.test_active = False\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    tester = IntegrationTester()\n\n    try:\n        rclpy.spin(tester)\n    except KeyboardInterrupt:\n        tester.get_logger().info(\'Integration tester stopped by user\')\n    finally:\n        tester.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting-integration-issues",children:"Troubleshooting Integration Issues"}),"\n",(0,i.jsx)(n.h3,{id:"1-common-integration-problems-and-solutions",children:"1. Common Integration Problems and Solutions"}),"\n",(0,i.jsx)(n.h4,{id:"issue-perception-data-not-affecting-navigation",children:"Issue: Perception data not affecting navigation"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": Robot doesn't respond to dynamic objects or semantic information\n",(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Check topic connections:","\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /segmentation/segmentation_map\nros2 topic list | grep costmap\n"})}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Verify costmap configuration includes perception layers"}),"\n",(0,i.jsx)(n.li,{children:"Check TF tree for proper frame relationships"}),"\n",(0,i.jsx)(n.li,{children:"Confirm perception nodes are publishing data"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"issue-high-cpu-usage-with-integration",children:"Issue: High CPU usage with integration"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": System becomes unresponsive when perception is active\n",(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Reduce perception processing frequency"}),"\n",(0,i.jsx)(n.li,{children:"Lower costmap resolution"}),"\n",(0,i.jsx)(n.li,{children:"Simplify perception algorithms"}),"\n",(0,i.jsx)(n.li,{children:"Use hardware acceleration for perception"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"issue-navigation-conflicts-with-perception",children:"Issue: Navigation conflicts with perception"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": Robot stops frequently due to false obstacle detections\n",(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Adjust perception confidence thresholds"}),"\n",(0,i.jsx)(n.li,{children:"Increase obstacle filtering"}),"\n",(0,i.jsx)(n.li,{children:"Tune costmap inflation parameters"}),"\n",(0,i.jsx)(n.li,{children:"Implement perception validation"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-performance-monitoring",children:"2. Performance Monitoring"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Monitor integration performance\nros2 run topic_tools relay /perception_processing_time\nros2 run topic_tools relay /navigation_compute_time\nros2 run topic_tools relay /perception_navigation_status\n\n# Check TF tree\nros2 run tf2_tools view_frames\n\n# Monitor node performance\nros2 run top top\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices-for-integration",children:"Best Practices for Integration"}),"\n",(0,i.jsx)(n.h3,{id:"1-design-guidelines",children:"1. Design Guidelines"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use appropriate data fusion techniques"}),"\n",(0,i.jsx)(n.li,{children:"Implement proper error handling and fallbacks"}),"\n",(0,i.jsx)(n.li,{children:"Design for graceful degradation when perception fails"}),"\n",(0,i.jsx)(n.li,{children:"Maintain real-time performance requirements"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-testing-strategies",children:"2. Testing Strategies"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Test in simulation before real-world deployment"}),"\n",(0,i.jsx)(n.li,{children:"Validate individual components before integration"}),"\n",(0,i.jsx)(n.li,{children:"Test with various environmental conditions"}),"\n",(0,i.jsx)(n.li,{children:"Verify safety and reliability requirements"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-performance-optimization",children:"3. Performance Optimization"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use appropriate data structures for real-time processing"}),"\n",(0,i.jsx)(n.li,{children:"Implement efficient algorithms for perception tasks"}),"\n",(0,i.jsx)(n.li,{children:"Optimize communication between nodes"}),"\n",(0,i.jsx)(n.li,{children:"Consider hardware acceleration where possible"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/concepts/navigation_integration/index.html",children:"Isaac ROS Navigation Integration Guide"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://navigation.ros.org/tutorials/docs/navigation2_with_vslam.html",children:"Navigation2 Perception Tutorials"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://navigation.ros.org/plugins/costmap_plugins/index.html",children:"Costmap Layer Development"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"This guide provides comprehensive instructions for integrating Isaac ROS perception with Navigation2. The integration enables robots to leverage visual and sensor data for enhanced navigation capabilities, including better obstacle detection, semantic awareness, and improved localization. Proper integration requires careful attention to data flow, timing, and performance considerations. The key to successful integration is starting with basic configurations and gradually adding complexity while maintaining system stability and performance."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(_,{...e})}):_(e)}},7074:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var a=t(6540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);