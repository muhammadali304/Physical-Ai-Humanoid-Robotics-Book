"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[315],{7074:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var a=t(6540);const s={},r=a.createContext(s);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}},7884:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"isaac-platform/llm-integration-guide","title":"LLM Integration Guide for Robotics Applications","description":"This document provides instructions for integrating Large Language Models (LLMs) with robotics applications using API-based approaches.","source":"@site/docs/isaac-platform/llm-integration-guide.md","sourceDirName":"isaac-platform","slug":"/isaac-platform/llm-integration-guide","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/llm-integration-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-platform/llm-integration-guide.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Basic RL Training Example for Robot Manipulation","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/robot-manipulation-rl-example"},"next":{"title":"Voice Command Processing System","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/voice-command-processing-system"}}');var s=t(4848),r=t(7074);const i={},o="LLM Integration Guide for Robotics Applications",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Architecture Patterns",id:"architecture-patterns",level:2},{value:"1. Cloud-Based API Integration",id:"1-cloud-based-api-integration",level:3},{value:"2. Hybrid Approach",id:"2-hybrid-approach",level:3},{value:"3. Edge Deployment (Advanced)",id:"3-edge-deployment-advanced",level:3},{value:"API-Based Integration Approaches",id:"api-based-integration-approaches",level:2},{value:"1. OpenAI GPT Integration",id:"1-openai-gpt-integration",level:3},{value:"Setup and Configuration",id:"setup-and-configuration",level:4},{value:"Example Usage",id:"example-usage",level:4},{value:"2. Anthropic Claude Integration",id:"2-anthropic-claude-integration",level:3},{value:"Setup and Configuration",id:"setup-and-configuration-1",level:4},{value:"3. Google Gemini Integration",id:"3-google-gemini-integration",level:3},{value:"Setup and Configuration",id:"setup-and-configuration-2",level:4},{value:"Implementation Examples",id:"implementation-examples",level:2},{value:"1. Natural Language Command Processing",id:"1-natural-language-command-processing",level:3},{value:"2. Task Planning and Execution",id:"2-task-planning-and-execution",level:3},{value:"Safety and Security Considerations",id:"safety-and-security-considerations",level:2},{value:"1. Input Validation",id:"1-input-validation",level:3},{value:"2. Access Control",id:"2-access-control",level:3},{value:"3. Fail-Safe Mechanisms",id:"3-fail-safe-mechanisms",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Caching Strategies",id:"1-caching-strategies",level:3},{value:"2. Asynchronous Processing",id:"2-asynchronous-processing",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Error Handling",id:"1-error-handling",level:3},{value:"2. Cost Management",id:"2-cost-management",level:3},{value:"3. Testing and Validation",id:"3-testing-and-validation",level:3},{value:"Resources and Further Reading",id:"resources-and-further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"llm-integration-guide-for-robotics-applications",children:"LLM Integration Guide for Robotics Applications"})}),"\n",(0,s.jsx)(n.p,{children:"This document provides instructions for integrating Large Language Models (LLMs) with robotics applications using API-based approaches."}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Large Language Models can enhance robotics applications by providing natural language understanding, high-level task planning, and human-robot interaction capabilities. This guide covers API-based integration approaches that are suitable for robotics applications."}),"\n",(0,s.jsx)(n.h2,{id:"architecture-patterns",children:"Architecture Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"1-cloud-based-api-integration",children:"1. Cloud-Based API Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LLMs accessed via cloud APIs (OpenAI, Anthropic, Google, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Suitable for applications with reliable internet connectivity"}),"\n",(0,s.jsx)(n.li,{children:"Lower computational requirements on robot side"}),"\n",(0,s.jsx)(n.li,{children:"Real-time processing with potential latency considerations"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-hybrid-approach",children:"2. Hybrid Approach"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Combination of local lightweight models and cloud LLMs"}),"\n",(0,s.jsx)(n.li,{children:"Critical functions handled locally, complex reasoning in cloud"}),"\n",(0,s.jsx)(n.li,{children:"Balances performance and capability requirements"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-edge-deployment-advanced",children:"3. Edge Deployment (Advanced)"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LLMs deployed on edge devices for offline capability"}),"\n",(0,s.jsx)(n.li,{children:"Requires powerful edge hardware (NVIDIA Jetson, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Lower latency but higher resource requirements"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"api-based-integration-approaches",children:"API-Based Integration Approaches"}),"\n",(0,s.jsx)(n.h3,{id:"1-openai-gpt-integration",children:"1. OpenAI GPT Integration"}),"\n",(0,s.jsx)(n.h4,{id:"setup-and-configuration",children:"Setup and Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nimport asyncio\nfrom typing import Dict, List, Optional\n\nclass OpenAIIntegrator:\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\n        self.api_key = api_key\n        self.model = model\n        openai.api_key = api_key\n\n    async def generate_robot_command(self, user_request: str, robot_state: Dict) -> Dict:\n        """\n        Generate robot commands based on natural language request\n        """\n        system_prompt = f"""\n        You are a robotics command interpreter. Convert natural language requests into robot commands.\n        Current robot state: {robot_state}\n        Available actions: move_to, pick_up, place, speak, navigate, grasp, release\n        Return commands in JSON format with action, parameters, and confidence score.\n        """\n\n        response = await openai.ChatCompletion.acreate(\n            model=self.model,\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": user_request}\n            ],\n            temperature=0.3,\n            max_tokens=200\n        )\n\n        return self._parse_response(response.choices[0].message.content)\n\n    def _parse_response(self, response: str) -> Dict:\n        """\n        Parse the LLM response and extract structured robot commands\n        """\n        # Implementation to parse JSON response\n        import json\n        try:\n            return json.loads(response)\n        except json.JSONDecodeError:\n            # Fallback parsing\n            return {"action": "unknown", "confidence": 0.0}\n'})}),"\n",(0,s.jsx)(n.h4,{id:"example-usage",children:"Example Usage"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Initialize the integrator\nllm_integrator = OpenAIIntegrator(api_key="your-api-key")\n\n# Example robot state\nrobot_state = {\n    "location": "kitchen",\n    "battery_level": 85,\n    "gripper_status": "open",\n    "current_task": "idle",\n    "objects_detected": ["mug", "table", "refrigerator"]\n}\n\n# Process natural language command\nuser_command = "Please bring me a coffee from the kitchen counter"\ncommand = await llm_integrator.generate_robot_command(user_command, robot_state)\nprint(f"Generated command: {command}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-anthropic-claude-integration",children:"2. Anthropic Claude Integration"}),"\n",(0,s.jsx)(n.h4,{id:"setup-and-configuration-1",children:"Setup and Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import anthropic\nfrom typing import Dict, List\n\nclass ClaudeIntegrator:\n    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229"):\n        self.client = anthropic.Anthropic(api_key=api_key)\n        self.model = model\n\n    def generate_robot_behavior(self, user_request: str, environment_context: Dict) -> Dict:\n        """\n        Generate detailed robot behavior based on natural language request\n        """\n        prompt = f"""\n        Human: {user_request}\n\n        Robot Environment Context:\n        - Location: {environment_context.get(\'location\', \'unknown\')}\n        - Available objects: {environment_context.get(\'objects\', [])}\n        - Robot capabilities: {environment_context.get(\'capabilities\', [])}\n        - Current state: {environment_context.get(\'state\', {})}\n\n        Please provide:\n        1. High-level plan\n        2. Specific actions to execute\n        3. Expected outcomes\n        4. Potential challenges and solutions\n\n        Respond in structured JSON format.\n        """\n\n        response = self.client.messages.create(\n            model=self.model,\n            max_tokens=1000,\n            temperature=0.3,\n            system="You are a robotics planning assistant. Generate detailed plans for robot behavior based on natural language requests.",\n            messages=[\n                {"role": "user", "content": prompt}\n            ]\n        )\n\n        return self._extract_structured_response(response.content)\n\n    def _extract_structured_response(self, content) -> Dict:\n        """\n        Extract structured response from Claude\'s content\n        """\n        # Implementation to extract structured data\n        text_content = ""\n        for block in content:\n            if block.type == "text":\n                text_content += block.text\n\n        # Parse the text content for JSON structure\n        import json\n        import re\n\n        # Extract JSON from the response\n        json_match = re.search(r\'\\{.*\\}\', text_content, re.DOTALL)\n        if json_match:\n            try:\n                return json.loads(json_match.group())\n            except json.JSONDecodeError:\n                pass\n\n        return {"plan": text_content, "actions": [], "confidence": 0.5}\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-google-gemini-integration",children:"3. Google Gemini Integration"}),"\n",(0,s.jsx)(n.h4,{id:"setup-and-configuration-2",children:"Setup and Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import google.generativeai as genai\nfrom typing import Dict, List\n\nclass GeminiIntegrator:\n    def __init__(self, api_key: str, model: str = "gemini-pro"):\n        genai.configure(api_key=api_key)\n        self.model = genai.GenerativeModel(model)\n\n    async def generate_task_plan(self, goal: str, constraints: List[str]) -> Dict:\n        """\n        Generate task plan for robot based on goal and constraints\n        """\n        prompt = f"""\n        Goal: {goal}\n        Constraints: {constraints}\n\n        Generate a step-by-step task plan for a robot to achieve the goal.\n        Consider:\n        1. Environmental constraints\n        2. Robot capabilities\n        3. Safety requirements\n        4. Efficiency optimization\n\n        Return in JSON format with steps, estimated time, and confidence scores.\n        """\n\n        response = await self.model.generate_content_async(prompt)\n        return self._parse_task_plan(response.text)\n\n    def _parse_task_plan(self, response: str) -> Dict:\n        """\n        Parse the task plan response\n        """\n        import json\n        import re\n\n        # Extract JSON from response\n        json_pattern = r\'\\{[^{}]*\\}\'  # Simple pattern, can be enhanced\n        matches = re.findall(json_pattern, response)\n\n        if matches:\n            try:\n                return json.loads(matches[-1])  # Use the last JSON block\n            except json.JSONDecodeError:\n                pass\n\n        return {"steps": [response], "estimated_time": "unknown", "confidence": 0.5}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"implementation-examples",children:"Implementation Examples"}),"\n",(0,s.jsx)(n.h3,{id:"1-natural-language-command-processing",children:"1. Natural Language Command Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# robot_llm_interface.py\nimport asyncio\nfrom typing import Dict, List, Any\nimport logging\n\nclass RobotLLMInterface:\n    """\n    Main interface for LLM integration with robot systems\n    """\n    def __init__(self, llm_integrator):\n        self.llm_integrator = llm_integrator\n        self.logger = logging.getLogger(__name__)\n\n    async def process_command(self, user_input: str, robot_context: Dict) -> Dict:\n        """\n        Process natural language command and generate robot actions\n        """\n        try:\n            # Generate command from LLM\n            llm_response = await self.llm_integrator.generate_robot_command(\n                user_input, robot_context\n            )\n\n            # Validate and sanitize response\n            validated_command = self._validate_command(llm_response, robot_context)\n\n            # Log the interaction\n            self.logger.info(f"Processed command: {user_input} -> {validated_command}")\n\n            return validated_command\n\n        except Exception as e:\n            self.logger.error(f"Error processing command: {e}")\n            return {\n                "action": "error",\n                "message": f"Failed to process command: {str(e)}",\n                "confidence": 0.0\n            }\n\n    def _validate_command(self, command: Dict, context: Dict) -> Dict:\n        """\n        Validate and sanitize LLM-generated commands\n        """\n        # Check if action is in allowed actions\n        allowed_actions = ["move_to", "pick_up", "place", "speak", "navigate", "grasp", "release"]\n\n        if command.get("action") not in allowed_actions:\n            return {\n                "action": "error",\n                "message": f"Invalid action: {command.get(\'action\')}",\n                "confidence": 0.0\n            }\n\n        # Validate parameters based on action\n        if command["action"] in ["move_to", "navigate"]:\n            if "location" not in command.get("parameters", {}):\n                return {\n                    "action": "error",\n                    "message": "Missing location parameter for navigation",\n                    "confidence": 0.0\n                }\n\n        # Add safety checks\n        command["safety_verified"] = True\n\n        return command\n\n    async def generate_conversation_response(self, user_input: str, conversation_history: List[Dict]) -> str:\n        """\n        Generate natural language response for human-robot interaction\n        """\n        context = f"""\n        Conversation history: {conversation_history}\n        User input: {user_input}\n\n        Respond as a helpful robot assistant. Keep responses concise and relevant to the robot\'s capabilities.\n        """\n\n        try:\n            response = await self.llm_integrator.generate_response(context)\n            return response\n        except Exception as e:\n            self.logger.error(f"Error generating conversation response: {e}")\n            return "I\'m sorry, I couldn\'t process that request."\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-task-planning-and-execution",children:"2. Task Planning and Execution"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# task_planner.py\nfrom typing import Dict, List, Optional\nimport asyncio\nimport time\n\nclass RobotTaskPlanner:\n    """\n    Task planning system using LLM for high-level reasoning\n    """\n    def __init__(self, llm_integrator):\n        self.llm_integrator = llm_integrator\n        self.current_task = None\n        self.task_history = []\n\n    async def plan_task(self, goal: str, environment_state: Dict) -> List[Dict]:\n        """\n        Generate a task plan based on goal and environment state\n        """\n        plan_prompt = f"""\n        Goal: {goal}\n        Environment state: {environment_state}\n\n        Generate a detailed task plan with:\n        1. Sequential steps\n        2. Pre-conditions for each step\n        3. Expected outcomes\n        4. Error handling procedures\n        5. Alternative strategies\n\n        Return as ordered list of steps in JSON format.\n        """\n\n        plan = await self.llm_integrator.generate_task_plan(plan_prompt)\n        return plan.get("steps", [])\n\n    async def execute_task_with_llm_guidance(self, goal: str, robot_interface) -> Dict:\n        """\n        Execute a task with LLM providing guidance and adaptation\n        """\n        # Generate initial plan\n        environment_state = await robot_interface.get_environment_state()\n        task_plan = await self.plan_task(goal, environment_state)\n\n        results = {\n            "goal": goal,\n            "plan": task_plan,\n            "executed_steps": [],\n            "success": False,\n            "reasoning_log": []\n        }\n\n        for i, step in enumerate(task_plan):\n            self.current_task = step\n\n            # Execute step\n            step_result = await self._execute_step(step, robot_interface)\n            results["executed_steps"].append(step_result)\n\n            # Log reasoning\n            reasoning = f"Step {i+1}: {step[\'description\']}, Result: {step_result[\'status\']}"\n            results["reasoning_log"].append(reasoning)\n\n            if not step_result["success"]:\n                # Ask LLM for alternative approach\n                adaptation_prompt = f"""\n                Task step failed: {step}\n                Failure reason: {step_result.get(\'error\', \'Unknown\')}\n                Current environment: {await robot_interface.get_environment_state()}\n\n                Suggest alternative approach or recovery strategy.\n                """\n\n                alternative = await self.llm_integrator.generate_response(adaptation_prompt)\n                results["reasoning_log"].append(f"Adaptation: {alternative}")\n\n                # Try alternative approach\n                if alternative:\n                    alt_result = await self._execute_step(alternative, robot_interface)\n                    results["executed_steps"].append(alt_result)\n\n        # Determine overall success\n        results["success"] = all(step["success"] for step in results["executed_steps"])\n\n        return results\n\n    async def _execute_step(self, step: Dict, robot_interface) -> Dict:\n        """\n        Execute a single task step\n        """\n        try:\n            # Execute the robot action\n            result = await robot_interface.execute_action(step["action"], step.get("parameters", {}))\n\n            return {\n                "step": step,\n                "success": result.get("success", False),\n                "result": result,\n                "timestamp": time.time()\n            }\n        except Exception as e:\n            return {\n                "step": step,\n                "success": False,\n                "error": str(e),\n                "timestamp": time.time()\n            }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"safety-and-security-considerations",children:"Safety and Security Considerations"}),"\n",(0,s.jsx)(n.h3,{id:"1-input-validation",children:"1. Input Validation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Sanitize all user inputs before sending to LLM"}),"\n",(0,s.jsx)(n.li,{children:"Implement content filtering to prevent harmful instructions"}),"\n",(0,s.jsx)(n.li,{children:"Validate LLM outputs before execution"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-access-control",children:"2. Access Control"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use secure API keys with limited permissions"}),"\n",(0,s.jsx)(n.li,{children:"Implement rate limiting to prevent abuse"}),"\n",(0,s.jsx)(n.li,{children:"Log all API interactions for monitoring"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-fail-safe-mechanisms",children:"3. Fail-Safe Mechanisms"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement timeout mechanisms for LLM calls"}),"\n",(0,s.jsx)(n.li,{children:"Provide fallback behaviors when LLM is unavailable"}),"\n",(0,s.jsx)(n.li,{children:"Ensure robot safety during LLM processing delays"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-caching-strategies",children:"1. Caching Strategies"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import functools\nimport time\nfrom typing import Any\n\nclass CachedLLMIntegrator:\n    def __init__(self, llm_integrator, cache_ttl: int = 300):  # 5 minutes\n        self.llm_integrator = llm_integrator\n        self.cache_ttl = cache_ttl\n        self.cache = {}\n\n    def _get_cache_key(self, *args, **kwargs):\n        import hashlib\n        import json\n        cache_input = json.dumps((args, kwargs), sort_keys=True)\n        return hashlib.md5(cache_input.encode()).hexdigest()\n\n    async def generate_with_cache(self, prompt: str, context: Dict[str, Any] = None) -> Any:\n        cache_key = self._get_cache_key(prompt, context)\n\n        # Check cache\n        if cache_key in self.cache:\n            cached_result, timestamp = self.cache[cache_key]\n            if time.time() - timestamp < self.cache_ttl:\n                return cached_result\n\n        # Generate new result\n        result = await self.llm_integrator.generate_response(prompt)\n\n        # Store in cache\n        self.cache[cache_key] = (result, time.time())\n\n        return result\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-asynchronous-processing",children:"2. Asynchronous Processing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use async/await for non-blocking LLM calls"}),"\n",(0,s.jsx)(n.li,{children:"Implement request queuing for high-volume scenarios"}),"\n",(0,s.jsx)(n.li,{children:"Batch similar requests when possible"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# llm_ros_integration.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nimport asyncio\nimport threading\n\nclass LLMROSInterface(Node):\n    """\n    ROS 2 interface for LLM integration\n    """\n    def __init__(self, llm_integrator):\n        super().__init__(\'llm_interface\')\n        self.llm_integrator = llm_integrator\n\n        # Publishers and subscribers\n        self.command_publisher = self.create_publisher(String, \'robot_commands\', 10)\n        self.nlp_subscriber = self.create_subscription(\n            String, \'natural_language_commands\', self.nlp_callback, 10\n        )\n        self.state_subscriber = self.create_subscription(\n            String, \'robot_state\', self.state_callback, 10\n        )\n\n        self.current_state = {}\n        self.command_queue = asyncio.Queue()\n\n        # Start async processing\n        self.processing_thread = threading.Thread(target=self._start_async_processing)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n    def nlp_callback(self, msg):\n        """\n        Handle natural language commands\n        """\n        command_future = asyncio.run_coroutine_threadsafe(\n            self.process_natural_language_command(msg.data),\n            self.loop\n        )\n        command_future.add_done_callback(self._publish_robot_command)\n\n    def state_callback(self, msg):\n        """\n        Update robot state from ROS messages\n        """\n        import json\n        try:\n            self.current_state = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error(f"Invalid state message: {msg.data}")\n\n    async def process_natural_language_command(self, command: str):\n        """\n        Process natural language command using LLM\n        """\n        llm_response = await self.llm_integrator.generate_robot_command(\n            command, self.current_state\n        )\n        return llm_response\n\n    def _publish_robot_command(self, future):\n        """\n        Publish processed robot command\n        """\n        try:\n            command = future.result()\n            msg = String()\n            msg.data = str(command)\n            self.command_publisher.publish(msg)\n        except Exception as e:\n            self.get_logger().error(f"Error processing command: {e}")\n\n    def _start_async_processing(self):\n        """\n        Start the asyncio event loop in a separate thread\n        """\n        self.loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.loop)\n        self.loop.run_forever()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"1-error-handling",children:"1. Error Handling"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement retry mechanisms for API failures"}),"\n",(0,s.jsx)(n.li,{children:"Provide graceful degradation when LLM is unavailable"}),"\n",(0,s.jsx)(n.li,{children:"Log all interactions for debugging and monitoring"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-cost-management",children:"2. Cost Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Monitor API usage and costs"}),"\n",(0,s.jsx)(n.li,{children:"Implement caching for repeated requests"}),"\n",(0,s.jsx)(n.li,{children:"Use appropriate model sizes for the task complexity"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-testing-and-validation",children:"3. Testing and Validation"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Test with various natural language inputs"}),"\n",(0,s.jsx)(n.li,{children:"Validate LLM outputs before robot execution"}),"\n",(0,s.jsx)(n.li,{children:"Implement safety checks and bounds verification"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"resources-and-further-reading",children:"Resources and Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.anthropic.com/",children:"Anthropic API Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://ai.google.dev/",children:"Google AI Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.ros.org/",children:"ROS 2 Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://research.nvidia.com/publication/llm-robotics-applications",children:"Robot Operating System 2 (ROS 2) and LLM Integration Patterns"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);