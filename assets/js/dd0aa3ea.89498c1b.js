"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6840],{7074:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var i=o(6540);const t={},s=i.createContext(t);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(s.Provider,{value:n},e.children)}},8220:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"isaac-platform/voice-command-processing-system","title":"Voice Command Processing System","description":"This document provides a comprehensive guide for implementing a voice command processing system for robotics applications.","source":"@site/docs/isaac-platform/voice-command-processing-system.md","sourceDirName":"isaac-platform","slug":"/isaac-platform/voice-command-processing-system","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/voice-command-processing-system","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-platform/voice-command-processing-system.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LLM Integration Guide for Robotics Applications","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/llm-integration-guide"},"next":{"title":"Sim-to-Real Transfer Guide for Jetson Deployment","permalink":"/Physical-Ai-Humanoid-Robotics-Book/docs/isaac-platform/sim-to-real-transfer-jetson-guide"}}');var t=o(4848),s=o(7074);const r={},a="Voice Command Processing System",c={},l=[{value:"Overview",id:"overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"1. High-Level Architecture",id:"1-high-level-architecture",level:3},{value:"2. Component Breakdown",id:"2-component-breakdown",level:3},{value:"Implementation Components",id:"implementation-components",level:2},{value:"1. Audio Input and Preprocessing",id:"1-audio-input-and-preprocessing",level:3},{value:"2. Speech Recognition Module",id:"2-speech-recognition-module",level:3},{value:"3. Natural Language Processing and Intent Recognition",id:"3-natural-language-processing-and-intent-recognition",level:3},{value:"4. Voice Command Processing System",id:"4-voice-command-processing-system",level:3},{value:"5. ROS 2 Integration",id:"5-ros-2-integration",level:3},{value:"6. Example Usage and Integration",id:"6-example-usage-and-integration",level:3},{value:"Installation Requirements",id:"installation-requirements",level:2},{value:"Python Dependencies",id:"python-dependencies",level:3},{value:"For Local Whisper Support (optional)",id:"for-local-whisper-support-optional",level:3},{value:"For ROS 2 Integration",id:"for-ros-2-integration",level:3},{value:"Configuration and Tuning",id:"configuration-and-tuning",level:2},{value:"1. Audio Configuration",id:"1-audio-configuration",level:3},{value:"2. Recognition Accuracy",id:"2-recognition-accuracy",level:3},{value:"3. Intent Recognition",id:"3-intent-recognition",level:3},{value:"Security and Privacy Considerations",id:"security-and-privacy-considerations",level:2},{value:"1. Data Privacy",id:"1-data-privacy",level:3},{value:"2. Command Validation",id:"2-command-validation",level:3},{value:"3. Access Control",id:"3-access-control",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Resource Management",id:"1-resource-management",level:3},{value:"2. Accuracy Improvements",id:"2-accuracy-improvements",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues:",id:"common-issues",level:3},{value:"Verification Steps:",id:"verification-steps",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"voice-command-processing-system",children:"Voice Command Processing System"})}),"\n",(0,t.jsx)(n.p,{children:"This document provides a comprehensive guide for implementing a voice command processing system for robotics applications."}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"The voice command processing system enables natural human-robot interaction through spoken commands. This system handles speech recognition, natural language processing, and command execution for robotics applications."}),"\n",(0,t.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"1-high-level-architecture",children:"1. High-Level Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Voice Input \u2192 Speech Recognition \u2192 Natural Language Processing \u2192 Command Execution \u2192 Robot Actions\n"})}),"\n",(0,t.jsx)(n.h3,{id:"2-component-breakdown",children:"2. Component Breakdown"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Input Module"}),": Captures and preprocesses audio from microphones"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech Recognition"}),": Converts speech to text using local or cloud-based ASR"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intent Recognition"}),": Parses text to identify user intent and parameters"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Mapping"}),": Maps intents to specific robot actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution Module"}),": Executes commands safely on the robot"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-components",children:"Implementation Components"}),"\n",(0,t.jsx)(n.h3,{id:"1-audio-input-and-preprocessing",children:"1. Audio Input and Preprocessing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# audio_input.py\nimport pyaudio\nimport numpy as np\nimport webrtcvad\nimport collections\nimport queue\nfrom typing import Generator, Tuple\nimport threading\nimport time\n\nclass AudioInput:\n    """\n    Audio input module with voice activity detection\n    """\n    def __init__(self, sample_rate: int = 16000, chunk_size: int = 320):\n        self.sample_rate = sample_rate\n        self.chunk_size = chunk_size\n        self.audio = pyaudio.PyAudio()\n\n        # Voice activity detection\n        self.vad = webrtcvad.Vad(2)  # Aggressiveness level 2\n        self.ring_buffer = collections.deque(maxlen=300)  # 300 frames\n        self.triggered = False\n        self.vad_frames = []\n\n        # Audio stream\n        self.stream = self.audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=sample_rate,\n            input=True,\n            frames_per_buffer=chunk_size\n        )\n\n        # Audio queue for processing\n        self.audio_queue = queue.Queue()\n        self.is_listening = False\n\n    def start_listening(self):\n        """Start audio capture in a separate thread"""\n        self.is_listening = True\n        self.capture_thread = threading.Thread(target=self._capture_audio)\n        self.capture_thread.daemon = True\n        self.capture_thread.start()\n\n    def stop_listening(self):\n        """Stop audio capture"""\n        self.is_listening = False\n        if hasattr(self, \'capture_thread\'):\n            self.capture_thread.join()\n\n    def _capture_audio(self):\n        """Capture audio in a loop"""\n        while self.is_listening:\n            audio_data = self.stream.read(self.chunk_size, exception_on_overflow=False)\n            self.audio_queue.put(audio_data)\n\n    def get_audio_stream(self) -> Generator[bytes, None, None]:\n        """Generator that yields audio chunks when voice activity is detected"""\n        while self.is_listening:\n            try:\n                audio_chunk = self.audio_queue.get(timeout=0.1)\n                yield audio_chunk\n            except queue.Empty:\n                continue\n\n    def detect_voice_activity(self, audio_chunk: bytes) -> bool:\n        """Detect if voice activity is present in audio chunk"""\n        # Convert to appropriate format for VAD\n        # VAD requires 16kHz, 16-bit, mono audio in 10, 20, or 30ms chunks\n        chunk_duration = len(audio_chunk) * 1000 // (self.sample_rate * 2)  # Duration in ms\n\n        if chunk_duration not in [10, 20, 30]:\n            # Adjust chunk size or skip\n            return False\n\n        return self.vad.is_speech(audio_chunk, self.sample_rate)\n\n    def close(self):\n        """Close audio resources"""\n        self.is_listening = False\n        self.stream.stop_stream()\n        self.stream.close()\n        self.audio.terminate()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-speech-recognition-module",children:"2. Speech Recognition Module"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# speech_recognition.py\nimport speech_recognition as sr\nimport asyncio\nimport logging\nfrom typing import Optional\nimport tempfile\nimport os\nimport wave\n\nclass SpeechRecognizer:\n    """\n    Speech recognition module with multiple backend support\n    """\n    def __init__(self, backend: str = "google"):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.backend = backend\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source, duration=1.0)\n\n        # Set energy threshold\n        self.recognizer.energy_threshold = 4000\n\n        # Supported backends\n        self.supported_backends = {\n            "google": self._recognize_google,\n            "wit": self._recognize_wit,\n            "houndify": self._recognize_houndify,\n            "ibm": self._recognize_ibm\n        }\n\n        self.logger = logging.getLogger(__name__)\n\n    def recognize_speech(self, audio_data: sr.AudioData) -> Optional[str]:\n        """\n        Recognize speech from audio data using configured backend\n        """\n        if self.backend not in self.supported_backends:\n            raise ValueError(f"Unsupported backend: {self.backend}")\n\n        try:\n            result = self.supported_backends[self.backend](audio_data)\n            self.logger.info(f"Recognized speech: {result}")\n            return result\n        except Exception as e:\n            self.logger.error(f"Speech recognition error: {e}")\n            return None\n\n    def _recognize_google(self, audio_data: sr.AudioData) -> str:\n        """Google Web Speech API recognition"""\n        return self.recognizer.recognize_google(audio_data)\n\n    def _recognize_wit(self, audio_data: sr.AudioData) -> str:\n        """Wit.ai recognition (requires API key)"""\n        # This would require wit.ai API key configuration\n        # return self.recognizer.recognize_wit(audio_data, key="YOUR_WIT_AI_KEY")\n        raise NotImplementedError("Wit.ai integration requires API key setup")\n\n    def _recognize_houndify(self, audio_data: sr.AudioData) -> str:\n        """Houndify recognition (requires client ID and key)"""\n        # This would require Houndify credentials\n        # return self.recognizer.recognize_houndify(audio_data, client_id="YOUR_CLIENT_ID", client_key="YOUR_CLIENT_KEY")\n        raise NotImplementedError("Houndify integration requires credentials")\n\n    def _recognize_ibm(self, audio_data: sr.AudioData) -> str:\n        """IBM Watson recognition (requires credentials)"""\n        # This would require IBM Watson credentials\n        # return self.recognizer.recognize_ibm(audio_data, username="YOUR_USERNAME", password="YOUR_PASSWORD")\n        raise NotImplementedError("IBM Watson integration requires credentials")\n\n    def listen_for_command(self, timeout: int = 5, phrase_time_limit: int = 10) -> Optional[str]:\n        """\n        Listen for a voice command using the microphone\n        """\n        try:\n            with self.microphone as source:\n                self.logger.info("Listening for voice command...")\n                audio = self.recognizer.listen(source, timeout=timeout, phrase_time_limit=phrase_time_limit)\n\n            return self.recognize_speech(audio)\n        except sr.WaitTimeoutError:\n            self.logger.warning("Timeout waiting for speech")\n            return None\n        except sr.UnknownValueError:\n            self.logger.warning("Could not understand audio")\n            return None\n        except sr.RequestError as e:\n            self.logger.error(f"Speech recognition service error: {e}")\n            return None\n\n# Example of using Whisper for local speech recognition\nclass LocalSpeechRecognizer:\n    """\n    Local speech recognition using OpenAI Whisper (requires installation)\n    """\n    def __init__(self, model_size: str = "base"):\n        try:\n            import whisper\n            self.model = whisper.load_model(model_size)\n        except ImportError:\n            raise ImportError("Please install whisper: pip install openai-whisper")\n\n    def recognize_from_audio_data(self, audio_data: sr.AudioData) -> Optional[str]:\n        """\n        Recognize speech using local Whisper model\n        """\n        import io\n        import numpy as np\n\n        # Convert audio data to WAV format\n        wav_buffer = io.BytesIO()\n        with wave.open(wav_buffer, \'wb\') as wav_file:\n            wav_file.setnchannels(1)  # Mono\n            wav_file.setsampwidth(2)  # 16-bit\n            wav_file.setframerate(16000)  # 16kHz\n            wav_file.writeframes(audio_data.frame_data)\n\n        # Convert to numpy array for Whisper\n        wav_buffer.seek(0)\n        audio_array = np.frombuffer(wav_buffer.read(), dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Transcribe\n        result = self.model.transcribe(audio_array)\n        return result["text"]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-natural-language-processing-and-intent-recognition",children:"3. Natural Language Processing and Intent Recognition"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# nlp_processor.py\nimport re\nimport spacy\nimport logging\nfrom typing import Dict, List, Optional, Tuple\nfrom dataclasses import dataclass\n\n@dataclass\nclass CommandIntent:\n    """Structured representation of a recognized command"""\n    intent: str\n    parameters: Dict[str, str]\n    confidence: float\n    raw_text: str\n\nclass NLPProcessor:\n    """\n    Natural language processing module for intent recognition\n    """\n    def __init__(self):\n        # Load spaCy model (install with: python -m spacy download en_core_web_sm)\n        try:\n            self.nlp = spacy.load("en_core_web_sm")\n        except OSError:\n            raise OSError("Please install spaCy English model: python -m spacy download en_core_web_sm")\n\n        self.logger = logging.getLogger(__name__)\n\n        # Define intent patterns\n        self.intent_patterns = {\n            "move_to": [\n                r"move to (.+)",\n                r"go to (.+)",\n                r"navigate to (.+)",\n                r"move towards (.+)",\n                r"head to (.+)"\n            ],\n            "pick_up": [\n                r"pick up (.+)",\n                r"grasp (.+)",\n                r"take (.+)",\n                r"grab (.+)"\n            ],\n            "place": [\n                r"place (.+) (?:at|on|in) (.+)",\n                r"put (.+) (?:at|on|in) (.+)",\n                r"drop (.+) (?:at|on|in) (.+)"\n            ],\n            "stop": [\n                r"stop",\n                r"halt",\n                r"freeze",\n                r"pause"\n            ],\n            "follow": [\n                r"follow (.+)",\n                r"follow me",\n                r"come with me"\n            ],\n            "greet": [\n                r"hello",\n                r"hi",\n                r"hey",\n                r"greetings"\n            ]\n        }\n\n        # Location keywords\n        self.location_keywords = {\n            "kitchen", "living room", "bedroom", "bathroom", "office",\n            "dining room", "hallway", "garage", "garden", "entrance"\n        }\n\n    def process_command(self, text: str) -> Optional[CommandIntent]:\n        """\n        Process text command and extract intent and parameters\n        """\n        text = text.lower().strip()\n        doc = self.nlp(text)\n\n        # Try pattern matching first\n        intent, params, confidence = self._match_patterns(text)\n\n        if intent:\n            # Enhance with NLP processing\n            enhanced_params = self._enhance_parameters(params, doc)\n            return CommandIntent(\n                intent=intent,\n                parameters=enhanced_params,\n                confidence=confidence,\n                raw_text=text\n            )\n\n        # If no pattern matched, try semantic analysis\n        semantic_intent = self._analyze_semantic_intent(doc)\n        if semantic_intent:\n            return semantic_intent\n\n        self.logger.warning(f"Could not identify intent for: {text}")\n        return None\n\n    def _match_patterns(self, text: str) -> Tuple[Optional[str], Dict, float]:\n        """\n        Match text against predefined patterns to identify intent\n        """\n        for intent, patterns in self.intent_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text)\n                if match:\n                    groups = match.groups()\n\n                    if intent == "move_to":\n                        return intent, {"location": groups[0]}, 0.9\n                    elif intent == "pick_up":\n                        return intent, {"object": groups[0]}, 0.9\n                    elif intent == "place":\n                        return intent, {"object": groups[0], "location": groups[1]}, 0.9\n                    elif intent == "follow":\n                        return intent, {"target": groups[0] if len(groups) > 0 else "user"}, 0.8\n                    elif intent in ["stop", "greet"]:\n                        return intent, {}, 0.95\n\n        return None, {}, 0.0\n\n    def _enhance_parameters(self, params: Dict, doc) -> Dict:\n        """\n        Enhance extracted parameters using NLP analysis\n        """\n        enhanced = params.copy()\n\n        # Extract named entities\n        for ent in doc.ents:\n            if ent.label_ in ["PERSON", "GPE", "LOC", "FAC"]:\n                enhanced[f"named_entity_{ent.label_.lower()}"] = ent.text\n\n        # Extract adjectives and adverbs for context\n        for token in doc:\n            if token.pos_ == "ADJ":\n                if "adjectives" not in enhanced:\n                    enhanced["adjectives"] = []\n                enhanced["adjectives"].append(token.text)\n            elif token.pos_ == "ADV":\n                if "adverbs" not in enhanced:\n                    enhanced["adverbs"] = []\n                enhanced["adverbs"].append(token.text)\n\n        return enhanced\n\n    def _analyze_semantic_intent(self, doc) -> Optional[CommandIntent]:\n        """\n        Analyze semantic intent when pattern matching fails\n        """\n        # Look for action verbs\n        action_verbs = [token.lemma_ for token in doc if token.pos_ == "VERB"]\n\n        # Look for objects\n        objects = [token.text for token in doc if token.pos_ in ["NOUN", "PROPN"]]\n\n        # Look for locations\n        locations = [ent.text for ent in doc.ents if ent.label_ in ["GPE", "LOC", "FAC"]]\n\n        # Determine intent based on semantic analysis\n        if any(verb in ["go", "move", "navigate", "walk", "run", "head"] for verb in action_verbs):\n            location = locations[0] if locations else None\n            return CommandIntent(\n                intent="move_to",\n                parameters={"location": location or "unknown"},\n                confidence=0.7,\n                raw_text=doc.text\n            )\n\n        if any(verb in ["pick", "take", "grasp", "grab", "hold"] for verb in action_verbs):\n            obj = objects[0] if objects else None\n            return CommandIntent(\n                intent="pick_up",\n                parameters={"object": obj or "unknown"},\n                confidence=0.7,\n                raw_text=doc.text\n            )\n\n        return None\n'})}),"\n",(0,t.jsx)(n.h3,{id:"4-voice-command-processing-system",children:"4. Voice Command Processing System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# voice_command_system.py\nimport asyncio\nimport logging\nfrom typing import Dict, Optional, Callable\nimport threading\nimport time\nfrom dataclasses import dataclass\n\n@dataclass\nclass VoiceCommandResult:\n    """Result of voice command processing"""\n    success: bool\n    message: str\n    command: Optional[CommandIntent] = None\n    execution_result: Optional[Dict] = None\n\nclass VoiceCommandProcessor:\n    """\n    Main voice command processing system\n    """\n    def __init__(self, speech_recognizer, nlp_processor):\n        self.speech_recognizer = speech_recognizer\n        self.nlp_processor = nlp_processor\n        self.logger = logging.getLogger(__name__)\n\n        # Command execution callbacks\n        self.command_handlers = {}\n\n        # System state\n        self.is_active = False\n        self.command_queue = asyncio.Queue()\n        self.response_callbacks = []\n\n        # Wake word detection (simple implementation)\n        self.wake_words = ["robot", "hey robot", "hello robot", "attention"]\n        self.wake_word_threshold = 0.8\n\n    def register_command_handler(self, intent: str, handler: Callable):\n        """\n        Register a handler for a specific command intent\n        """\n        self.command_handlers[intent] = handler\n\n    async def process_voice_command(self, timeout: int = 10) -> VoiceCommandResult:\n        """\n        Process a single voice command from start to finish\n        """\n        try:\n            # Listen for command\n            self.logger.info("Listening for voice command...")\n            text = await asyncio.get_event_loop().run_in_executor(\n                None,\n                self.speech_recognizer.listen_for_command,\n                timeout\n            )\n\n            if not text:\n                return VoiceCommandResult(\n                    success=False,\n                    message="No speech detected or recognition failed"\n                )\n\n            self.logger.info(f"Recognized: {text}")\n\n            # Process with NLP\n            command_intent = self.nlp_processor.process_command(text)\n            if not command_intent:\n                return VoiceCommandResult(\n                    success=False,\n                    message=f"Could not understand command: {text}"\n                )\n\n            # Execute command if handler exists\n            if command_intent.intent in self.command_handlers:\n                handler = self.command_handlers[command_intent.intent]\n                execution_result = await asyncio.get_event_loop().run_in_executor(\n                    None,\n                    handler,\n                    command_intent\n                )\n\n                return VoiceCommandResult(\n                    success=True,\n                    message=f"Command \'{command_intent.intent}\' executed successfully",\n                    command=command_intent,\n                    execution_result=execution_result\n                )\n            else:\n                return VoiceCommandResult(\n                    success=False,\n                    message=f"No handler for command intent: {command_intent.intent}",\n                    command=command_intent\n                )\n\n        except Exception as e:\n            self.logger.error(f"Error processing voice command: {e}")\n            return VoiceCommandResult(\n                success=False,\n                message=f"Error processing voice command: {str(e)}"\n            )\n\n    def add_response_callback(self, callback: Callable):\n        """\n        Add a callback for voice command responses\n        """\n        self.response_callbacks.append(callback)\n\n    def speak_response(self, text: str):\n        """\n        Generate speech response (placeholder - implement with TTS)\n        """\n        import pyttsx3\n        try:\n            engine = pyttsx3.init()\n            # Configure speech properties\n            rate = engine.getProperty(\'rate\')\n            engine.setProperty(\'rate\', rate - 50)  # Slower speech\n\n            volume = engine.getProperty(\'volume\')\n            engine.setProperty(\'volume\', volume + 0.25)\n\n            engine.say(text)\n            engine.runAndWait()\n        except Exception as e:\n            self.logger.error(f"Text-to-speech error: {e}")\n\n    async def continuous_listening(self):\n        """\n        Continuously listen for voice commands\n        """\n        self.is_active = True\n        self.logger.info("Starting continuous voice command listening...")\n\n        while self.is_active:\n            try:\n                result = await self.process_voice_command(timeout=5)\n\n                if result.success:\n                    self.logger.info(f"Command executed: {result.message}")\n\n                    # Execute response callbacks\n                    for callback in self.response_callbacks:\n                        callback(result)\n\n                    # Provide audio feedback\n                    self.speak_response("Command executed successfully")\n                else:\n                    self.logger.warning(f"Command failed: {result.message}")\n                    self.speak_response("Sorry, I couldn\'t understand that command")\n\n                # Small delay to prevent excessive processing\n                await asyncio.sleep(0.5)\n\n            except KeyboardInterrupt:\n                self.logger.info("Voice command processing interrupted")\n                break\n            except Exception as e:\n                self.logger.error(f"Error in continuous listening: {e}")\n                await asyncio.sleep(1)  # Wait before retrying\n\n    def stop_listening(self):\n        """\n        Stop continuous listening\n        """\n        self.is_active = False\n        self.logger.info("Stopped voice command listening")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"5-ros-2-integration",children:"5. ROS 2 Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# ros_voice_integration.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import AudioData\nimport asyncio\nimport threading\n\nclass ROSVoiceInterface(Node):\n    """\n    ROS 2 interface for voice command processing\n    """\n    def __init__(self, voice_processor):\n        super().__init__(\'voice_command_interface\')\n        self.voice_processor = voice_processor\n\n        # Publishers\n        self.command_publisher = self.create_publisher(String, \'robot_commands\', 10)\n        self.speech_publisher = self.create_publisher(String, \'speech_output\', 10)\n\n        # Subscribers\n        self.voice_command_sub = self.create_subscription(\n            String, \'voice_commands\', self.voice_command_callback, 10\n        )\n\n        # Timer for continuous processing\n        self.processing_timer = self.create_timer(0.1, self.process_voice_queue)\n\n        # Voice command queue\n        self.voice_command_queue = asyncio.Queue()\n\n        # Start async processing in separate thread\n        self.voice_thread = threading.Thread(target=self._run_voice_processing)\n        self.voice_thread.daemon = True\n        self.voice_thread.start()\n\n    def voice_command_callback(self, msg):\n        """\n        Handle voice command messages\n        """\n        asyncio.run_coroutine_threadsafe(\n            self.voice_command_queue.put(msg.data),\n            self.voice_loop\n        )\n\n    async def process_voice_queue(self):\n        """\n        Process voice commands from the queue\n        """\n        try:\n            # Non-blocking check for voice commands\n            command = self.voice_command_queue.get_nowait()\n            result = await self.voice_processor.process_voice_command_from_text(command)\n\n            if result.success:\n                # Publish robot command\n                cmd_msg = String()\n                cmd_msg.data = str(result.command)\n                self.command_publisher.publish(cmd_msg)\n\n                # Publish success feedback\n                feedback_msg = String()\n                feedback_msg.data = "Command executed successfully"\n                self.speech_publisher.publish(feedback_msg)\n        except asyncio.QueueEmpty:\n            pass  # No commands to process\n        except Exception as e:\n            self.get_logger().error(f"Error processing voice command: {e}")\n\n    def _run_voice_processing(self):\n        """\n        Run the voice processing loop in a separate thread\n        """\n        self.voice_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.voice_loop)\n        self.voice_loop.run_forever()\n\n    def publish_robot_command(self, command: str):\n        """\n        Publish a command to the robot\n        """\n        msg = String()\n        msg.data = command\n        self.command_publisher.publish(msg)\n\n    def publish_speech_output(self, text: str):\n        """\n        Publish speech output for TTS\n        """\n        msg = String()\n        msg.data = text\n        self.speech_publisher.publish(msg)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"6-example-usage-and-integration",children:"6. Example Usage and Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# example_voice_robot.py\nimport asyncio\nimport logging\nfrom voice_command_system import VoiceCommandProcessor, VoiceCommandResult\nfrom speech_recognition import SpeechRecognizer\nfrom nlp_processor import NLPProcessor\n\n# Example robot command handlers\nclass RobotCommandHandlers:\n    def __init__(self, robot_interface):\n        self.robot_interface = robot_interface\n\n    def handle_move_to(self, command_intent):\n        """Handle move_to commands"""\n        location = command_intent.parameters.get("location", "unknown")\n        self.robot_interface.move_to_location(location)\n        return {"status": "success", "location": location}\n\n    def handle_pick_up(self, command_intent):\n        """Handle pick_up commands"""\n        obj = command_intent.parameters.get("object", "unknown")\n        self.robot_interface.pick_up_object(obj)\n        return {"status": "success", "object": obj}\n\n    def handle_place(self, command_intent):\n        """Handle place commands"""\n        obj = command_intent.parameters.get("object", "unknown")\n        location = command_intent.parameters.get("location", "unknown")\n        self.robot_interface.place_object(obj, location)\n        return {"status": "success", "object": obj, "location": location}\n\n    def handle_stop(self, command_intent):\n        """Handle stop commands"""\n        self.robot_interface.stop_movement()\n        return {"status": "stopped"}\n\ndef main():\n    # Setup logging\n    logging.basicConfig(level=logging.INFO)\n\n    # Initialize components\n    speech_recognizer = SpeechRecognizer(backend="google")  # or "local" for Whisper\n    nlp_processor = NLPProcessor()\n\n    # Create voice command processor\n    voice_processor = VoiceCommandProcessor(speech_recognizer, nlp_processor)\n\n    # Example robot interface (replace with actual robot interface)\n    class MockRobotInterface:\n        def move_to_location(self, location):\n            print(f"Moving to {location}")\n\n        def pick_up_object(self, obj):\n            print(f"Picking up {obj}")\n\n        def place_object(self, obj, location):\n            print(f"Placing {obj} at {location}")\n\n        def stop_movement(self):\n            print("Stopping movement")\n\n    # Create command handlers\n    robot_interface = MockRobotInterface()\n    command_handlers = RobotCommandHandlers(robot_interface)\n\n    # Register command handlers\n    voice_processor.register_command_handler("move_to", command_handlers.handle_move_to)\n    voice_processor.register_command_handler("pick_up", command_handlers.handle_pick_up)\n    voice_processor.register_command_handler("place", command_handlers.handle_place)\n    voice_processor.register_command_handler("stop", command_handlers.handle_stop)\n\n    # Add response callback\n    def command_response_callback(result: VoiceCommandResult):\n        print(f"Command result: {result.message}")\n        if result.execution_result:\n            print(f"Execution result: {result.execution_result}")\n\n    voice_processor.add_response_callback(command_response_callback)\n\n    # Run continuous listening\n    print("Starting voice command system...")\n    print("Say \'robot\' followed by a command, or speak a direct command")\n    print("Examples: \'move to kitchen\', \'pick up the cup\', \'stop\'")\n\n    try:\n        # Run the continuous listening loop\n        asyncio.run(voice_processor.continuous_listening())\n    except KeyboardInterrupt:\n        print("\\nStopping voice command system...")\n        voice_processor.stop_listening()\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"installation-requirements",children:"Installation Requirements"}),"\n",(0,t.jsx)(n.h3,{id:"python-dependencies",children:"Python Dependencies"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install SpeechRecognition pyaudio webrtcvad spacy pyttsx3 numpy\npython -m spacy download en_core_web_sm\n"})}),"\n",(0,t.jsx)(n.h3,{id:"for-local-whisper-support-optional",children:"For Local Whisper Support (optional)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,t.jsx)(n.h3,{id:"for-ros-2-integration",children:"For ROS 2 Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install rclpy\n"})}),"\n",(0,t.jsx)(n.h2,{id:"configuration-and-tuning",children:"Configuration and Tuning"}),"\n",(0,t.jsx)(n.h3,{id:"1-audio-configuration",children:"1. Audio Configuration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Adjust ",(0,t.jsx)(n.code,{children:"energy_threshold"})," based on ambient noise levels"]}),"\n",(0,t.jsxs)(n.li,{children:["Modify ",(0,t.jsx)(n.code,{children:"phrase_time_limit"})," for longer/shorter commands"]}),"\n",(0,t.jsx)(n.li,{children:"Tune VAD aggressiveness level (0-3) for voice detection sensitivity"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-recognition-accuracy",children:"2. Recognition Accuracy"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Use appropriate speech recognition backend based on requirements:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Google: Good accuracy, requires internet, free tier"}),"\n",(0,t.jsx)(n.li,{children:"Local Whisper: Privacy, offline capability, resource intensive"}),"\n",(0,t.jsx)(n.li,{children:"Wit.ai/IBM: Customizable, requires API keys"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-intent-recognition",children:"3. Intent Recognition"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Extend intent patterns for domain-specific commands"}),"\n",(0,t.jsx)(n.li,{children:"Train spaCy models on robotics-specific language"}),"\n",(0,t.jsx)(n.li,{children:"Implement confidence thresholds for command execution"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"security-and-privacy-considerations",children:"Security and Privacy Considerations"}),"\n",(0,t.jsx)(n.h3,{id:"1-data-privacy",children:"1. Data Privacy"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use local speech recognition for privacy-sensitive applications"}),"\n",(0,t.jsx)(n.li,{children:"Encrypt audio data in transit if using cloud services"}),"\n",(0,t.jsx)(n.li,{children:"Implement data retention policies for audio recordings"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-command-validation",children:"2. Command Validation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Validate all voice commands before execution"}),"\n",(0,t.jsx)(n.li,{children:"Implement safety checks and bounds verification"}),"\n",(0,t.jsx)(n.li,{children:"Use authentication for sensitive commands"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-access-control",children:"3. Access Control"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Limit voice command vocabulary to safe operations"}),"\n",(0,t.jsx)(n.li,{children:"Implement user identification and authorization"}),"\n",(0,t.jsx)(n.li,{children:"Log all voice commands for audit purposes"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"1-resource-management",children:"1. Resource Management"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use appropriate model sizes for target hardware"}),"\n",(0,t.jsx)(n.li,{children:"Implement audio buffering and streaming"}),"\n",(0,t.jsx)(n.li,{children:"Optimize recognition timeout values"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-accuracy-improvements",children:"2. Accuracy Improvements"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Train domain-specific NLP models"}),"\n",(0,t.jsx)(n.li,{children:"Implement acoustic adaptation for environment"}),"\n",(0,t.jsx)(n.li,{children:"Use wake word detection to reduce processing load"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"common-issues",children:"Common Issues:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Input Problems"}),": Check microphone permissions and audio drivers"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Recognition Accuracy"}),": Adjust energy threshold and ambient noise settings"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Network Issues"}),": Implement fallback mechanisms for offline capability"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Processing Delays"}),": Optimize model sizes and implement async processing"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"verification-steps",children:"Verification Steps:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Test audio input and voice activity detection"}),"\n",(0,t.jsx)(n.li,{children:"Verify speech recognition accuracy in your environment"}),"\n",(0,t.jsx)(n.li,{children:"Confirm intent recognition works with expected commands"}),"\n",(0,t.jsx)(n.li,{children:"Test command execution safety and validation"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);