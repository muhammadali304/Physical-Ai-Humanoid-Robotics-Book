"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6324],{2776:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"vla/integration","title":"VLA Integration - Vision-Language-Action Systems","description":"Learning Objectives","source":"@site/docs/vla/integration.md","sourceDirName":"vla","slug":"/vla/integration","permalink":"/./docs/vla/integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/integration.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Edge Device Optimization Techniques for Robotics","permalink":"/./docs/isaac-platform/edge-device-optimization-techniques"},"next":{"title":"LLM Planning - Cognitive Planning with Large Language Models","permalink":"/./docs/vla/llm-planning"}}');var s=r(4848),a=r(7074);const i={sidebar_position:3},o="VLA Integration - Vision-Language-Action Systems",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Conceptual Overview",id:"conceptual-overview",level:2},{value:"VLA System Architecture",id:"vla-system-architecture",level:3},{value:"Key Integration Challenges",id:"key-integration-challenges",level:3},{value:"Benefits of VLA Integration",id:"benefits-of-vla-integration",level:3},{value:"Hands-On Implementation",id:"hands-on-implementation",level:2},{value:"Creating an Integrated VLA Node",id:"creating-an-integrated-vla-node",level:3},{value:"Creating a Multimodal Fusion Node",id:"creating-a-multimodal-fusion-node",level:3},{value:"Creating a VLA Coordinator Node",id:"creating-a-vla-coordinator-node",level:3},{value:"Creating a Complete VLA Launch File",id:"creating-a-complete-vla-launch-file",level:3},{value:"Creating a System Monitor Node",id:"creating-a-system-monitor-node",level:3},{value:"Testing &amp; Verification",id:"testing--verification",level:2},{value:"Running the Complete VLA System",id:"running-the-complete-vla-system",level:3},{value:"Useful VLA Integration Commands",id:"useful-vla-integration-commands",level:3},{value:"Performance Testing",id:"performance-testing",level:3},{value:"Common Issues",id:"common-issues",level:2},{value:"Issue: Timing mismatches between modalities",id:"issue-timing-mismatches-between-modalities",level:3},{value:"Issue: Coordination conflicts between modalities",id:"issue-coordination-conflicts-between-modalities",level:3},{value:"Issue: Computational overload with multimodal processing",id:"issue-computational-overload-with-multimodal-processing",level:3},{value:"Issue: Semantic gap between modalities",id:"issue-semantic-gap-between-modalities",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"vla-integration---vision-language-action-systems",children:"VLA Integration - Vision-Language-Action Systems"})}),"\n",(0,s.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Integrate vision, language, and action systems into a cohesive robot architecture"}),"\n",(0,s.jsx)(e.li,{children:"Implement multimodal perception for enhanced robot awareness"}),"\n",(0,s.jsx)(e.li,{children:"Create feedback loops between vision, language, and action components"}),"\n",(0,s.jsx)(e.li,{children:"Design coordination mechanisms for VLA system components"}),"\n",(0,s.jsx)(e.li,{children:"Implement error handling and recovery in VLA systems"}),"\n",(0,s.jsx)(e.li,{children:"Validate and test integrated VLA system performance"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(e.p,{children:"Before starting this chapter, you should:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Have ROS 2 Humble Hawksbill installed and configured"}),"\n",(0,s.jsx)(e.li,{children:"Completed the Isaac perception and navigation chapters"}),"\n",(0,s.jsx)(e.li,{children:"Completed the voice commands and LLM planning chapters"}),"\n",(0,s.jsx)(e.li,{children:"Understanding of sensor fusion and multimodal systems"}),"\n",(0,s.jsx)(e.li,{children:"Experience with ROS 2 action servers and clients"}),"\n",(0,s.jsx)(e.li,{children:"Knowledge of system integration patterns"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"conceptual-overview",children:"Conceptual Overview"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Vision-Language-Action (VLA)"})," systems represent the integration of three key modalities in robotics:"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision"}),": Sensory perception and environmental understanding"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language"}),": Natural communication and high-level reasoning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action"}),": Physical interaction and task execution"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"vla-system-architecture",children:"VLA System Architecture"}),"\n",(0,s.jsx)(e.p,{children:"A complete VLA system follows this architecture:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"[VISION] \u2190\u2192 [LANGUAGE] \u2190\u2192 [ACTION]\r\n    \u2191           \u2191           \u2191\r\nSensors    Natural      Actuators\r\n          Language\r\n         Processing\n"})}),"\n",(0,s.jsx)(e.p,{children:"With feedback loops for:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Perception \u2192 Action \u2192 Correction"}),"\n",(0,s.jsx)(e.li,{children:"Language \u2192 Plan \u2192 Execution \u2192 Update"}),"\n",(0,s.jsx)(e.li,{children:"Multi-modal fusion for enhanced decision making"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"key-integration-challenges",children:"Key Integration Challenges"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Temporal Coordination"}),": Aligning processing across different time scales"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Spatial Coordination"}),": Maintaining consistent coordinate frames"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Semantic Coordination"}),": Connecting visual concepts with linguistic concepts"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Behavioral Coordination"}),": Sequencing actions based on perception and language"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"benefits-of-vla-integration",children:"Benefits of VLA Integration"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Enhanced Situational Awareness"}),": Combined perception and reasoning"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Natural Human-Robot Interaction"}),": Intuitive communication and control"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Adaptive Behavior"}),": Respond to changing environments and goals"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robust Performance"}),": Multiple modalities provide redundancy"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Cognitive Capabilities"}),": Higher-level reasoning and planning"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"hands-on-implementation",children:"Hands-On Implementation"}),"\n",(0,s.jsx)(e.h3,{id:"creating-an-integrated-vla-node",children:"Creating an Integrated VLA Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\n\"\"\"\r\nIntegrated Vision-Language-Action node for multimodal robotics.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image, LaserScan\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom nav_msgs.msg import Odometry\r\nfrom rclpy.qos import QoSProfile, ReliabilityPolicy\r\nfrom typing import Dict, List, Optional, Any\r\nimport json\r\nimport threading\r\nimport time\r\nfrom dataclasses import dataclass\r\n\r\n\r\n@dataclass\r\nclass VLAState:\r\n    \"\"\"Current state of the VLA system.\"\"\"\r\n    vision_data: Optional[Any] = None\r\n    language_input: Optional[str] = None\r\n    action_plan: Optional[List[Dict]] = None\r\n    current_action: Optional[Dict] = None\r\n    execution_status: str = \"idle\"\r\n    timestamp: float = 0.0\r\n\r\n\r\nclass VLAIntegrationNode(Node):\r\n    \"\"\"\r\n    Node to integrate Vision, Language, and Action systems.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('vla_integration_node')\r\n\r\n        # Initialize VLA state\r\n        self.state = VLAState()\r\n        self.state_lock = threading.Lock()\r\n\r\n        # Create subscribers for all modalities\r\n        # Vision inputs\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.detection_sub = self.create_subscription(\r\n            Detection2DArray,\r\n            '/detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            '/odom',\r\n            self.odom_callback,\r\n            10\r\n        )\r\n\r\n        # Language inputs\r\n        self.voice_command_sub = self.create_subscription(\r\n            String,\r\n            '/voice_commands',\r\n            self.voice_command_callback,\r\n            10\r\n        )\r\n\r\n        self.text_command_sub = self.create_subscription(\r\n            String,\r\n            '/text_commands',\r\n            self.text_command_callback,\r\n            10\r\n        )\r\n\r\n        # Action inputs\r\n        self.action_feedback_sub = self.create_subscription(\r\n            String,\r\n            '/action_feedback',\r\n            self.action_feedback_callback,\r\n            10\r\n        )\r\n\r\n        # Create publishers\r\n        self.command_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.navigation_goal_pub = self.create_publisher(PoseStamped, '/goal_pose', 10)\r\n        self.vla_status_pub = self.create_publisher(String, '/vla_status', 10)\r\n        self.system_response_pub = self.create_publisher(String, '/system_response', 10)\r\n\r\n        # Timer for coordination loop\r\n        self.coordination_timer = self.create_timer(0.1, self.coordination_loop)\r\n\r\n        # Initialize perception and planning components\r\n        self.perception_processor = PerceptionProcessor(self)\r\n        self.language_processor = LanguageProcessor(self)\r\n        self.action_executor = ActionExecutor(self)\r\n\r\n        self.get_logger().info('VLA integration node initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Handle image input.\"\"\"\r\n        with self.state_lock:\r\n            self.state.vision_data = {\r\n                'image': msg,\r\n                'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\r\n            }\r\n            self.state.timestamp = time.time()\r\n\r\n    def detection_callback(self, msg):\r\n        \"\"\"Handle object detection input.\"\"\"\r\n        with self.state_lock:\r\n            self.state.vision_data = {\r\n                'detections': msg,\r\n                'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\r\n            }\r\n\r\n    def scan_callback(self, msg):\r\n        \"\"\"Handle laser scan input.\"\"\"\r\n        with self.state_lock:\r\n            self.state.vision_data = {\r\n                'scan': msg,\r\n                'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\r\n            }\r\n\r\n    def odom_callback(self, msg):\r\n        \"\"\"Handle odometry input.\"\"\"\r\n        with self.state_lock:\r\n            self.state.vision_data = {\r\n                'odometry': msg,\r\n                'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\r\n            }\r\n\r\n    def voice_command_callback(self, msg):\r\n        \"\"\"Handle voice command input.\"\"\"\r\n        with self.state_lock:\r\n            self.state.language_input = msg.data\r\n            self.state.timestamp = time.time()\r\n\r\n    def text_command_callback(self, msg):\r\n        \"\"\"Handle text command input.\"\"\"\r\n        with self.state_lock:\r\n            self.state.language_input = msg.data\r\n            self.state.timestamp = time.time()\r\n\r\n    def action_feedback_callback(self, msg):\r\n        \"\"\"Handle action execution feedback.\"\"\"\r\n        with self.state_lock:\r\n            try:\r\n                feedback = json.loads(msg.data)\r\n                self.state.execution_status = feedback.get('status', 'unknown')\r\n                self.state.current_action = feedback.get('current_action', None)\r\n            except json.JSONDecodeError:\r\n                self.state.execution_status = 'error'\r\n                self.get_logger().error('Error parsing action feedback')\r\n\r\n    def coordination_loop(self):\r\n        \"\"\"Main coordination loop for VLA integration.\"\"\"\r\n        with self.state_lock:\r\n            current_state = self.state\r\n\r\n        # Process based on available inputs\r\n        if current_state.language_input:\r\n            # Process language input to generate plan\r\n            plan = self.language_processor.process_command(current_state.language_input)\r\n\r\n            if plan:\r\n                # Update state with new plan\r\n                with self.state_lock:\r\n                    self.state.action_plan = plan\r\n                    self.state.language_input = None  # Clear processed input\r\n\r\n                # Execute the plan\r\n                self.execute_plan(plan)\r\n\r\n        # Process vision data for situational awareness\r\n        if current_state.vision_data:\r\n            # Update perception based on vision data\r\n            self.perception_processor.update_perception(current_state.vision_data)\r\n\r\n        # Publish system status\r\n        self.publish_vla_status()\r\n\r\n    def execute_plan(self, plan: List[Dict]):\r\n        \"\"\"Execute a plan with multimodal feedback.\"\"\"\r\n        for step in plan:\r\n            # Check for interruptions during execution\r\n            with self.state_lock:\r\n                if self.state.language_input:  # New command arrived\r\n                    self.get_logger().info('Plan interrupted by new command')\r\n                    return\r\n\r\n            # Execute the action step\r\n            success = self.action_executor.execute_action(step)\r\n\r\n            if not success:\r\n                self.get_logger().error(f'Action failed: {step}')\r\n                break\r\n\r\n            # Wait for action completion\r\n            time.sleep(0.5)\r\n\r\n    def publish_vla_status(self):\r\n        \"\"\"Publish current VLA system status.\"\"\"\r\n        status_msg = String()\r\n        with self.state_lock:\r\n            status_data = {\r\n                'execution_status': self.state.execution_status,\r\n                'vision_available': self.state.vision_data is not None,\r\n                'language_input_pending': self.state.language_input is not None,\r\n                'plan_steps_remaining': len(self.state.action_plan) if self.state.action_plan else 0,\r\n                'timestamp': time.time()\r\n            }\r\n            status_msg.data = json.dumps(status_data)\r\n\r\n        self.vla_status_pub.publish(status_msg)\r\n\r\n\r\nclass PerceptionProcessor:\r\n    \"\"\"Handles vision processing and multimodal perception.\"\"\"\r\n\r\n    def __init__(self, parent_node):\r\n        self.parent_node = parent_node\r\n        self.perception_model = None  # Would be a real perception model in practice\r\n\r\n    def update_perception(self, vision_data: Dict):\r\n        \"\"\"Update perception based on vision data.\"\"\"\r\n        # Process different types of vision data\r\n        if 'image' in vision_data:\r\n            self.process_image(vision_data['image'])\r\n        elif 'detections' in vision_data:\r\n            self.process_detections(vision_data['detections'])\r\n        elif 'scan' in vision_data:\r\n            self.process_scan(vision_data['scan'])\r\n        elif 'odometry' in vision_data:\r\n            self.process_odometry(vision_data['odometry'])\r\n\r\n    def process_image(self, image_msg):\r\n        \"\"\"Process camera image.\"\"\"\r\n        self.parent_node.get_logger().debug('Processing camera image')\r\n\r\n    def process_detections(self, detection_array):\r\n        \"\"\"Process object detections.\"\"\"\r\n        for detection in detection_array.detections:\r\n            self.parent_node.get_logger().debug(f'Detected: {detection.results}')\r\n\r\n    def process_scan(self, scan_msg):\r\n        \"\"\"Process laser scan for obstacle detection.\"\"\"\r\n        min_distance = min(scan_msg.ranges) if scan_msg.ranges else float('inf')\r\n        if min_distance < 0.5:\r\n            self.parent_node.get_logger().warn(f'Obstacle detected at {min_distance:.2f}m')\r\n\r\n    def process_odometry(self, odom_msg):\r\n        \"\"\"Process odometry for localization.\"\"\"\r\n        pos = odom_msg.pose.pose.position\r\n        self.parent_node.get_logger().debug(f'Position: ({pos.x:.2f}, {pos.y:.2f})')\r\n\r\n\r\nclass LanguageProcessor:\r\n    \"\"\"Handles language processing and command interpretation.\"\"\"\r\n\r\n    def __init__(self, parent_node):\r\n        self.parent_node = parent_node\r\n        self.command_interpreter = None  # Would be a real NLP model in practice\r\n\r\n    def process_command(self, command: str) -> Optional[List[Dict]]:\r\n        \"\"\"Process natural language command and return action plan.\"\"\"\r\n        self.parent_node.get_logger().info(f'Processing command: {command}')\r\n\r\n        # Simple rule-based command interpreter for demonstration\r\n        # In practice, this would use LLMs or other NLP models\r\n        return self.interpret_command(command)\r\n\r\n    def interpret_command(self, command: str) -> Optional[List[Dict]]:\r\n        \"\"\"Interpret command and generate action plan.\"\"\"\r\n        command_lower = command.lower()\r\n\r\n        # Example interpretations\r\n        if 'kitchen' in command_lower:\r\n            return [\r\n                {\r\n                    'action': 'navigate_to',\r\n                    'parameters': {'location': 'kitchen'},\r\n                    'description': 'Navigate to kitchen'\r\n                }\r\n            ]\r\n        elif 'bedroom' in command_lower:\r\n            return [\r\n                {\r\n                    'action': 'navigate_to',\r\n                    'parameters': {'location': 'bedroom'},\r\n                    'description': 'Navigate to bedroom'\r\n                }\r\n            ]\r\n        elif 'find' in command_lower or 'look for' in command_lower:\r\n            return [\r\n                {\r\n                    'action': 'localize',\r\n                    'parameters': {},\r\n                    'description': 'Localize to current position'\r\n                },\r\n                {\r\n                    'action': 'perceive',\r\n                    'parameters': {},\r\n                    'description': 'Perceive surroundings'\r\n                }\r\n            ]\r\n        else:\r\n            # Default action for unrecognized commands\r\n            return [\r\n                {\r\n                    'action': 'report_status',\r\n                    'parameters': {},\r\n                    'description': f'Received command: {command}'\r\n                }\r\n            ]\r\n\r\n\r\nclass ActionExecutor:\r\n    \"\"\"Handles action execution and coordination.\"\"\"\r\n\r\n    def __init__(self, parent_node):\r\n        self.parent_node = parent_node\r\n        self.command_publisher = parent_node.command_pub\r\n        self.navigation_publisher = parent_node.navigation_goal_pub\r\n\r\n    def execute_action(self, action: Dict) -> bool:\r\n        \"\"\"Execute a single action.\"\"\"\r\n        action_type = action.get('action', '')\r\n        parameters = action.get('parameters', {})\r\n\r\n        self.parent_node.get_logger().info(f'Executing action: {action_type}')\r\n\r\n        try:\r\n            if action_type == 'navigate_to':\r\n                return self.execute_navigate_to(parameters)\r\n            elif action_type == 'move_forward':\r\n                return self.execute_move_forward(parameters)\r\n            elif action_type == 'turn':\r\n                return self.execute_turn(parameters)\r\n            elif action_type == 'report_status':\r\n                return self.execute_report_status(parameters)\r\n            elif action_type == 'perceive':\r\n                return self.execute_perceive(parameters)\r\n            else:\r\n                self.parent_node.get_logger().warn(f'Unknown action type: {action_type}')\r\n                return False\r\n\r\n        except Exception as e:\r\n            self.parent_node.get_logger().error(f'Error executing action {action_type}: {e}')\r\n            return False\r\n\r\n    def execute_navigate_to(self, params: Dict) -> bool:\r\n        \"\"\"Execute navigation action.\"\"\"\r\n        location = params.get('location', 'unknown')\r\n\r\n        # In a real system, this would send navigation goals\r\n        # For this example, we'll simulate the action\r\n        self.parent_node.get_logger().info(f'Navigating to {location}')\r\n        time.sleep(2)  # Simulate navigation time\r\n\r\n        return True\r\n\r\n    def execute_move_forward(self, params: Dict) -> bool:\r\n        \"\"\"Execute forward movement.\"\"\"\r\n        distance = float(params.get('distance', 1.0))\r\n        speed = float(params.get('speed', 0.5))\r\n\r\n        # Create and publish velocity command\r\n        twist = Twist()\r\n        twist.linear.x = speed\r\n        self.command_publisher.publish(twist)\r\n\r\n        # Simulate movement\r\n        time.sleep(distance / speed)\r\n\r\n        # Stop the robot\r\n        stop_twist = Twist()\r\n        self.command_publisher.publish(stop_twist)\r\n\r\n        return True\r\n\r\n    def execute_turn(self, params: Dict) -> bool:\r\n        \"\"\"Execute turning action.\"\"\"\r\n        angle = float(params.get('angle', 90.0))\r\n        direction = params.get('direction', 'left')\r\n\r\n        # Create and publish turning command\r\n        twist = Twist()\r\n        twist.angular.z = 0.5 if direction == 'left' else -0.5\r\n        self.command_publisher.publish(twist)\r\n\r\n        # Simulate turn\r\n        time.sleep(abs(angle) / 90.0 * 2)  # Rough timing\r\n\r\n        # Stop turning\r\n        stop_twist = Twist()\r\n        self.command_publisher.publish(stop_twist)\r\n\r\n        return True\r\n\r\n    def execute_report_status(self, params: Dict) -> bool:\r\n        \"\"\"Execute status reporting.\"\"\"\r\n        self.parent_node.get_logger().info('Reporting system status')\r\n        return True\r\n\r\n    def execute_perceive(self, params: Dict) -> bool:\r\n        \"\"\"Execute perception action.\"\"\"\r\n        self.parent_node.get_logger().info('Performing environmental perception')\r\n        time.sleep(1)  # Simulate perception time\r\n        return True\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VLAIntegrationNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('VLA integration node stopped by user')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"creating-a-multimodal-fusion-node",children:"Creating a Multimodal Fusion Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\n\"\"\"\r\nMultimodal fusion node for combining vision, language, and action data.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom sensor_msgs.msg import Image, LaserScan\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom typing import Dict, List, Optional, Any\r\nimport json\r\nimport numpy as np\r\nfrom collections import deque\r\n\r\n\r\nclass MultimodalFusionNode(Node):\r\n    \"\"\"\r\n    Node to fuse information from vision, language, and action modalities.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('multimodal_fusion_node')\r\n\r\n        # Create subscribers for all modalities\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/image_raw',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        self.detection_sub = self.create_subscription(\r\n            Detection2DArray,\r\n            '/detections',\r\n            self.detection_callback,\r\n            10\r\n        )\r\n\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n\r\n        self.language_sub = self.create_subscription(\r\n            String,\r\n            '/parsed_commands',\r\n            self.language_callback,\r\n            10\r\n        )\r\n\r\n        self.action_sub = self.create_subscription(\r\n            String,\r\n            '/execution_status',\r\n            self.action_callback,\r\n            10\r\n        )\r\n\r\n        # Create publishers\r\n        self.fused_state_pub = self.create_publisher(String, '/fused_state', 10)\r\n        self.decision_pub = self.create_publisher(String, '/fused_decision', 10)\r\n\r\n        # Initialize fusion components\r\n        self.vision_buffer = deque(maxlen=10)\r\n        self.language_buffer = deque(maxlen=5)\r\n        self.action_buffer = deque(maxlen=10)\r\n\r\n        # Initialize fusion model (simplified for this example)\r\n        self.fusion_model = MultimodalFusionModel()\r\n\r\n        # Timer for fusion processing\r\n        self.fusion_timer = self.create_timer(0.2, self.fusion_loop)\r\n\r\n        self.get_logger().info('Multimodal fusion node initialized')\r\n\r\n    def image_callback(self, msg):\r\n        \"\"\"Handle image input.\"\"\"\r\n        # Convert image to features (simplified)\r\n        features = self.extract_image_features(msg)\r\n        self.vision_buffer.append({\r\n            'type': 'image',\r\n            'features': features,\r\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\r\n        })\r\n\r\n    def detection_callback(self, msg):\r\n        \"\"\"Handle detection input.\"\"\"\r\n        detections = []\r\n        for detection in msg.detections:\r\n            for result in detection.results:\r\n                detections.append({\r\n                    'class': result.hypothesis.class_id,\r\n                    'confidence': result.hypothesis.score,\r\n                    'bbox': [detection.bbox.center.x, detection.bbox.center.y,\r\n                             detection.bbox.size_x, detection.bbox.size_y]\r\n                })\r\n\r\n        self.vision_buffer.append({\r\n            'type': 'detection',\r\n            'data': detections,\r\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\r\n        })\r\n\r\n    def scan_callback(self, msg):\r\n        \"\"\"Handle laser scan input.\"\"\"\r\n        # Process laser scan for obstacle information\r\n        obstacle_distances = [r for r in msg.ranges if 0.1 < r < 10.0]  # Filter valid ranges\r\n        min_distance = min(obstacle_distances) if obstacle_distances else float('inf')\r\n\r\n        self.vision_buffer.append({\r\n            'type': 'scan',\r\n            'min_distance': min_distance,\r\n            'timestamp': msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\r\n        })\r\n\r\n    def language_callback(self, msg):\r\n        \"\"\"Handle language input.\"\"\"\r\n        self.language_buffer.append({\r\n            'text': msg.data,\r\n            'timestamp': self.get_clock().now().nanoseconds * 1e-9\r\n        })\r\n\r\n    def action_callback(self, msg):\r\n        \"\"\"Handle action input.\"\"\"\r\n        try:\r\n            action_data = json.loads(msg.data)\r\n            self.action_buffer.append({\r\n                'data': action_data,\r\n                'timestamp': self.get_clock().now().nanoseconds * 1e-9\r\n            })\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error(f'Error parsing action data: {msg.data}')\r\n\r\n    def extract_image_features(self, image_msg):\r\n        \"\"\"Extract simple features from image (simplified for example).\"\"\"\r\n        # In practice, this would use a CNN or other vision model\r\n        # For this example, we'll return dummy features\r\n        return {\r\n            'mean_intensity': 128,  # Placeholder\r\n            'edges': 100,           # Placeholder\r\n            'motion': False         # Placeholder\r\n        }\r\n\r\n    def fusion_loop(self):\r\n        \"\"\"Main fusion loop.\"\"\"\r\n        # Get current data from all modalities\r\n        vision_data = list(self.vision_buffer)\r\n        language_data = list(self.language_buffer)\r\n        action_data = list(self.action_buffer)\r\n\r\n        # Fuse the modalities\r\n        fused_state = self.fusion_model.fuse_modalities(\r\n            vision_data, language_data, action_data\r\n        )\r\n\r\n        # Make decision based on fused state\r\n        decision = self.make_decision(fused_state)\r\n\r\n        # Publish results\r\n        if fused_state:\r\n            fused_msg = String()\r\n            fused_msg.data = json.dumps(fused_state)\r\n            self.fused_state_pub.publish(fused_msg)\r\n\r\n        if decision:\r\n            decision_msg = String()\r\n            decision_msg.data = json.dumps(decision)\r\n            self.decision_pub.publish(decision_msg)\r\n\r\n    def make_decision(self, fused_state: Dict) -> Optional[Dict]:\r\n        \"\"\"Make decision based on fused state.\"\"\"\r\n        # Example decision logic\r\n        if not fused_state:\r\n            return None\r\n\r\n        # Check for obstacles\r\n        if 'min_distance' in fused_state and fused_state['min_distance'] < 0.5:\r\n            return {\r\n                'action': 'avoid_obstacle',\r\n                'reason': 'Obstacle detected in path',\r\n                'confidence': 0.9\r\n            }\r\n\r\n        # Check for language commands\r\n        if 'language_intent' in fused_state:\r\n            return {\r\n                'action': 'execute_command',\r\n                'command': fused_state['language_intent'],\r\n                'confidence': 0.8\r\n            }\r\n\r\n        # Default: continue current behavior\r\n        return {\r\n            'action': 'continue',\r\n            'confidence': 0.7\r\n        }\r\n\r\n\r\nclass MultimodalFusionModel:\r\n    \"\"\"Simplified multimodal fusion model.\"\"\"\r\n\r\n    def __init__(self):\r\n        # In practice, this would be a neural network or other fusion model\r\n        pass\r\n\r\n    def fuse_modalities(self, vision_data: List, language_data: List, action_data: List) -> Dict:\r\n        \"\"\"Fuse information from all modalities.\"\"\"\r\n        fused_state = {}\r\n\r\n        # Process vision data\r\n        if vision_data:\r\n            latest_vision = vision_data[-1]  # Most recent\r\n            if latest_vision['type'] == 'scan':\r\n                fused_state['min_distance'] = latest_vision['min_distance']\r\n            elif latest_vision['type'] == 'detection':\r\n                fused_state['detections'] = latest_vision['data']\r\n\r\n        # Process language data\r\n        if language_data:\r\n            latest_language = language_data[-1]  # Most recent\r\n            fused_state['language_input'] = latest_language['text']\r\n            # Simple intent extraction\r\n            text_lower = latest_language['text'].lower()\r\n            if 'go to' in text_lower or 'navigate' in text_lower:\r\n                fused_state['language_intent'] = 'navigation'\r\n            elif 'find' in text_lower or 'look' in text_lower:\r\n                fused_state['language_intent'] = 'perception'\r\n            else:\r\n                fused_state['language_intent'] = 'other'\r\n\r\n        # Process action data\r\n        if action_data:\r\n            latest_action = action_data[-1]  # Most recent\r\n            fused_state['action_status'] = latest_action['data']\r\n\r\n        # Add timestamp\r\n        fused_state['timestamp'] = time.time()\r\n\r\n        return fused_state\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = MultimodalFusionNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('Multimodal fusion node stopped by user')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"creating-a-vla-coordinator-node",children:"Creating a VLA Coordinator Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\n\"\"\"\r\nVLA coordinator node for managing the overall system flow.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom action_msgs.msg import GoalStatus\r\nfrom geometry_msgs.msg import Twist\r\nfrom typing import Dict, List, Optional, Any\r\nimport json\r\nimport time\r\nfrom enum import Enum\r\n\r\n\r\nclass VLAState(Enum):\r\n    IDLE = \"idle\"\r\n    PERCEIVING = \"perceiving\"\r\n    PLANNING = \"planning\"\r\n    EXECUTING = \"executing\"\r\n    WAITING = \"waiting\"\r\n    ERROR = \"error\"\r\n\r\n\r\nclass VLACoordinatorNode(Node):\r\n    \"\"\"\r\n    Node to coordinate the overall VLA system workflow.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('vla_coordinator_node')\r\n\r\n        # Initialize system state\r\n        self.current_state = VLAState.IDLE\r\n        self.pending_commands = []\r\n        self.current_plan = []\r\n        self.current_action = None\r\n        self.system_context = {}\r\n\r\n        # Create subscribers\r\n        self.voice_command_sub = self.create_subscription(\r\n            String,\r\n            '/voice_commands',\r\n            self.voice_command_callback,\r\n            10\r\n        )\r\n\r\n        self.vla_status_sub = self.create_subscription(\r\n            String,\r\n            '/vla_status',\r\n            self.vla_status_callback,\r\n            10\r\n        )\r\n\r\n        self.fused_state_sub = self.create_subscription(\r\n            String,\r\n            '/fused_state',\r\n            self.fused_state_callback,\r\n            10\r\n        )\r\n\r\n        self.decision_sub = self.create_subscription(\r\n            String,\r\n            '/fused_decision',\r\n            self.decision_callback,\r\n            10\r\n        )\r\n\r\n        # Create publishers\r\n        self.command_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n        self.system_status_pub = self.create_publisher(String, '/system_status', 10)\r\n        self.coordinator_status_pub = self.create_publisher(String, '/coordinator_status', 10)\r\n\r\n        # Timer for state management\r\n        self.state_timer = self.create_timer(0.1, self.state_management_loop)\r\n\r\n        self.get_logger().info('VLA coordinator node initialized')\r\n\r\n    def voice_command_callback(self, msg):\r\n        \"\"\"Handle voice commands.\"\"\"\r\n        command = msg.data\r\n        self.get_logger().info(f'Received voice command: {command}')\r\n\r\n        # Add to pending commands\r\n        self.pending_commands.append({\r\n            'command': command,\r\n            'timestamp': time.time(),\r\n            'priority': 1  # Default priority\r\n        })\r\n\r\n        # Update state if idle\r\n        if self.current_state == VLAState.IDLE:\r\n            self.transition_to(VLAState.PLANNING)\r\n\r\n    def vla_status_callback(self, msg):\r\n        \"\"\"Handle VLA status updates.\"\"\"\r\n        try:\r\n            status = json.loads(msg.data)\r\n            self.system_context.update(status)\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error('Error parsing VLA status')\r\n\r\n    def fused_state_callback(self, msg):\r\n        \"\"\"Handle fused state updates.\"\"\"\r\n        try:\r\n            fused_state = json.loads(msg.data)\r\n            self.system_context.update(fused_state)\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error('Error parsing fused state')\r\n\r\n    def decision_callback(self, msg):\r\n        \"\"\"Handle decision updates.\"\"\"\r\n        try:\r\n            decision = json.loads(msg.data)\r\n            self.handle_decision(decision)\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error('Error parsing decision')\r\n\r\n    def state_management_loop(self):\r\n        \"\"\"Main state management loop.\"\"\"\r\n        if self.current_state == VLAState.IDLE:\r\n            # Check for pending commands\r\n            if self.pending_commands:\r\n                self.transition_to(VLAState.PLANNING)\r\n\r\n        elif self.current_state == VLAState.PLANNING:\r\n            # Process pending commands and create plans\r\n            self.process_commands()\r\n\r\n        elif self.current_state == VLAState.EXECUTING:\r\n            # Monitor execution progress\r\n            self.monitor_execution()\r\n\r\n        elif self.current_state == VLAState.WAITING:\r\n            # Check if waiting condition is resolved\r\n            self.check_waiting_conditions()\r\n\r\n        # Publish coordinator status\r\n        self.publish_coordinator_status()\r\n\r\n    def process_commands(self):\r\n        \"\"\"Process pending commands and create plans.\"\"\"\r\n        if not self.pending_commands:\r\n            self.transition_to(VLAState.IDLE)\r\n            return\r\n\r\n        # Sort commands by priority\r\n        sorted_commands = sorted(self.pending_commands, key=lambda x: x['priority'], reverse=True)\r\n        command = sorted_commands[0]\r\n\r\n        # Generate plan for the command\r\n        # In practice, this would call the LLM planning node\r\n        self.current_plan = self.generate_plan(command['command'])\r\n\r\n        if self.current_plan:\r\n            self.pending_commands.remove(command)\r\n            self.transition_to(VLAState.EXECUTING)\r\n        else:\r\n            self.get_logger().error(f'Failed to generate plan for command: {command[\"command\"]}')\r\n            self.transition_to(VLAState.ERROR)\r\n\r\n    def generate_plan(self, command: str) -> List[Dict]:\r\n        \"\"\"Generate plan for command (simplified for example).\"\"\"\r\n        # This would normally call the LLM planning node\r\n        # For this example, we'll use a simple rule-based approach\r\n        command_lower = command.lower()\r\n\r\n        if 'kitchen' in command_lower:\r\n            return [\r\n                {'action': 'navigate_to', 'parameters': {'location': 'kitchen'}},\r\n                {'action': 'report_arrival', 'parameters': {'location': 'kitchen'}}\r\n            ]\r\n        elif 'bedroom' in command_lower:\r\n            return [\r\n                {'action': 'navigate_to', 'parameters': {'location': 'bedroom'}},\r\n                {'action': 'report_arrival', 'parameters': {'location': 'bedroom'}}\r\n            ]\r\n        elif 'stop' in command_lower or 'halt' in command_lower:\r\n            return [\r\n                {'action': 'stop_robot', 'parameters': {}}\r\n            ]\r\n        else:\r\n            return [\r\n                {'action': 'acknowledge', 'parameters': {'command': command}}\r\n            ]\r\n\r\n    def monitor_execution(self):\r\n        \"\"\"Monitor plan execution.\"\"\"\r\n        if not self.current_plan:\r\n            self.transition_to(VLAState.IDLE)\r\n            return\r\n\r\n        # Execute next action in plan\r\n        if self.current_action is None and self.current_plan:\r\n            self.current_action = self.current_plan.pop(0)\r\n            self.execute_current_action()\r\n\r\n    def execute_current_action(self):\r\n        \"\"\"Execute the current action.\"\"\"\r\n        if not self.current_action:\r\n            return\r\n\r\n        action_type = self.current_action['action']\r\n        parameters = self.current_action['parameters']\r\n\r\n        self.get_logger().info(f'Executing action: {action_type}')\r\n\r\n        # Execute based on action type\r\n        if action_type == 'navigate_to':\r\n            self.execute_navigation(parameters)\r\n        elif action_type == 'stop_robot':\r\n            self.execute_stop()\r\n        elif action_type == 'acknowledge':\r\n            self.execute_acknowledgment(parameters)\r\n\r\n        # Mark action as complete and move to next\r\n        self.current_action = None\r\n\r\n        # If plan is complete, return to idle\r\n        if not self.current_plan:\r\n            self.transition_to(VLAState.IDLE)\r\n\r\n    def execute_navigation(self, params: Dict):\r\n        \"\"\"Execute navigation action.\"\"\"\r\n        location = params.get('location', 'unknown')\r\n        self.get_logger().info(f'Navigating to {location}')\r\n        # In practice, this would send navigation goals\r\n\r\n    def execute_stop(self):\r\n        \"\"\"Execute stop action.\"\"\"\r\n        twist = Twist()\r\n        self.command_pub.publish(twist)\r\n        self.get_logger().info('Robot stopped')\r\n\r\n    def execute_acknowledgment(self, params: Dict):\r\n        \"\"\"Execute acknowledgment action.\"\"\"\r\n        command = params.get('command', 'unknown')\r\n        self.get_logger().info(f'Acknowledged command: {command}')\r\n\r\n    def handle_decision(self, decision: Dict):\r\n        \"\"\"Handle fused decision.\"\"\"\r\n        action = decision.get('action', 'unknown')\r\n        confidence = decision.get('confidence', 0.0)\r\n\r\n        if confidence > 0.7:  # High confidence decision\r\n            if action == 'avoid_obstacle':\r\n                self.avoid_obstacle()\r\n            elif action == 'execute_command':\r\n                command = decision.get('command', 'unknown')\r\n                self.execute_external_command(command)\r\n\r\n    def avoid_obstacle(self):\r\n        \"\"\"Handle obstacle avoidance decision.\"\"\"\r\n        self.get_logger().info('Avoiding obstacle')\r\n        # Implement obstacle avoidance logic\r\n        twist = Twist()\r\n        twist.linear.x = 0.0\r\n        twist.angular.z = 0.5  # Turn to avoid\r\n        self.command_pub.publish(twist)\r\n\r\n    def execute_external_command(self, command: str):\r\n        \"\"\"Execute externally generated command.\"\"\"\r\n        self.get_logger().info(f'Executing external command: {command}')\r\n        # Add to pending commands\r\n        self.pending_commands.append({\r\n            'command': command,\r\n            'timestamp': time.time(),\r\n            'priority': 2  # Higher priority for system decisions\r\n        })\r\n\r\n    def transition_to(self, new_state: VLAState):\r\n        \"\"\"Transition to new state.\"\"\"\r\n        old_state = self.current_state\r\n        self.current_state = new_state\r\n        self.get_logger().info(f'State transition: {old_state.value} -> {new_state.value}')\r\n\r\n    def check_waiting_conditions(self):\r\n        \"\"\"Check if waiting conditions are resolved.\"\"\"\r\n        # Implement logic to check if we can exit waiting state\r\n        pass\r\n\r\n    def publish_coordinator_status(self):\r\n        \"\"\"Publish coordinator status.\"\"\"\r\n        status_msg = String()\r\n        status_data = {\r\n            'current_state': self.current_state.value,\r\n            'pending_commands': len(self.pending_commands),\r\n            'current_plan_length': len(self.current_plan),\r\n            'current_action': self.current_action['action'] if self.current_action else None,\r\n            'timestamp': time.time()\r\n        }\r\n        status_msg.data = json.dumps(status_data)\r\n        self.coordinator_status_pub.publish(status_msg)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VLACoordinatorNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('VLA coordinator node stopped by user')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"creating-a-complete-vla-launch-file",children:"Creating a Complete VLA Launch File"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Create launch/vla_system_launch.py:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\r\nfrom launch.event_handlers import OnProcessStart\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import Node\r\nfrom launch.conditions import IfCondition\r\n\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    use_sim_time = DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='false',\r\n        description='Use simulation time if true'\r\n    )\r\n\r\n    enable_vision = DeclareLaunchArgument(\r\n        'enable_vision',\r\n        default_value='true',\r\n        description='Enable vision processing nodes'\r\n    )\r\n\r\n    enable_language = DeclareLaunchArgument(\r\n        'enable_language',\r\n        default_value='true',\r\n        description='Enable language processing nodes'\r\n    )\r\n\r\n    # VLA integration node\r\n    vla_integration_node = Node(\r\n        package='vla_integration',\r\n        executable='vla_integration_node',\r\n        name='vla_integration_node',\r\n        parameters=[\r\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\r\n        ],\r\n        condition=IfCondition(LaunchConfiguration('enable_language')),\r\n        output='screen'\r\n    )\r\n\r\n    # Multimodal fusion node\r\n    multimodal_fusion_node = Node(\r\n        package='vla_integration',\r\n        executable='multimodal_fusion_node',\r\n        name='multimodal_fusion_node',\r\n        parameters=[\r\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\r\n        ],\r\n        condition=IfCondition(LaunchConfiguration('enable_vision')),\r\n        output='screen'\r\n    )\r\n\r\n    # VLA coordinator node\r\n    vla_coordinator_node = Node(\r\n        package='vla_integration',\r\n        executable='vla_coordinator_node',\r\n        name='vla_coordinator_node',\r\n        parameters=[\r\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    # Launch description\r\n    ld = LaunchDescription([\r\n        use_sim_time,\r\n        enable_vision,\r\n        enable_language,\r\n        vla_integration_node,\r\n        multimodal_fusion_node,\r\n        vla_coordinator_node\r\n    ])\r\n\r\n    return ld\n"})}),"\n",(0,s.jsx)(e.h3,{id:"creating-a-system-monitor-node",children:"Creating a System Monitor Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\r\n\r\n\"\"\"\r\nSystem monitor node for VLA system health and performance.\r\n\"\"\"\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom diagnostic_msgs.msg import DiagnosticArray, DiagnosticStatus, KeyValue\r\nfrom typing import Dict, List\r\nimport time\r\nimport psutil\r\nimport threading\r\n\r\n\r\nclass SystemMonitorNode(Node):\r\n    \"\"\"\r\n    Node to monitor VLA system health and performance.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        super().__init__('system_monitor_node')\r\n\r\n        # Create subscribers for system status\r\n        self.vla_status_sub = self.create_subscription(\r\n            String,\r\n            '/vla_status',\r\n            self.vla_status_callback,\r\n            10\r\n        )\r\n\r\n        self.coordinator_status_sub = self.create_subscription(\r\n            String,\r\n            '/coordinator_status',\r\n            self.coordinator_status_callback,\r\n            10\r\n        )\r\n\r\n        # Create diagnostic publisher\r\n        self.diag_pub = self.create_publisher(DiagnosticArray, '/diagnostics', 10)\r\n\r\n        # Initialize monitoring variables\r\n        self.vla_status = {}\r\n        self.coordinator_status = {}\r\n        self.monitoring_data = {}\r\n\r\n        # Timer for periodic diagnostics\r\n        self.diag_timer = self.create_timer(1.0, self.publish_diagnostics)\r\n\r\n        self.get_logger().info('System monitor node initialized')\r\n\r\n    def vla_status_callback(self, msg):\r\n        \"\"\"Handle VLA status updates.\"\"\"\r\n        try:\r\n            self.vla_status = eval(msg.data)  # In practice, use json.loads\r\n        except:\r\n            self.get_logger().error('Error parsing VLA status')\r\n\r\n    def coordinator_status_callback(self, msg):\r\n        \"\"\"Handle coordinator status updates.\"\"\"\r\n        try:\r\n            self.coordinator_status = eval(msg.data)  # In practice, use json.loads\r\n        except:\r\n            self.get_logger().error('Error parsing coordinator status')\r\n\r\n    def publish_diagnostics(self):\r\n        \"\"\"Publish system diagnostics.\"\"\"\r\n        diag_array = DiagnosticArray()\r\n        diag_array.header.stamp = self.get_clock().now().to_msg()\r\n\r\n        # System resources\r\n        cpu_percent = psutil.cpu_percent(interval=1)\r\n        memory_percent = psutil.virtual_memory().percent\r\n        disk_percent = psutil.disk_usage('/').percent\r\n\r\n        # VLA subsystem status\r\n        vla_diag = DiagnosticStatus()\r\n        vla_diag.name = 'VLA System Status'\r\n        vla_diag.hardware_id = 'vla_integration'\r\n\r\n        # Determine overall status\r\n        if self.coordinator_status.get('current_state') == 'error':\r\n            vla_diag.level = DiagnosticStatus.ERROR\r\n            vla_diag.message = 'System in error state'\r\n        elif self.vla_status.get('execution_status') == 'failed':\r\n            vla_diag.level = DiagnosticStatus.WARN\r\n            vla_diag.message = 'Execution issues detected'\r\n        else:\r\n            vla_diag.level = DiagnosticStatus.OK\r\n            vla_diag.message = 'System operational'\r\n\r\n        # Add key-value pairs for detailed info\r\n        vla_diag.values.extend([\r\n            KeyValue(key='CPU Usage (%)', value=str(cpu_percent)),\r\n            KeyValue(key='Memory Usage (%)', value=str(memory_percent)),\r\n            KeyValue(key='Disk Usage (%)', value=str(disk_percent)),\r\n            KeyValue(key='Current State', value=self.coordinator_status.get('current_state', 'unknown')),\r\n            KeyValue(key='Pending Commands', value=str(self.coordinator_status.get('pending_commands', 0))),\r\n            KeyValue(key='Vision Available', value=str(self.vla_status.get('vision_available', False))),\r\n            KeyValue(key='Language Input Pending', value=str(self.vla_status.get('language_input_pending', False)))\r\n        ])\r\n\r\n        diag_array.status.append(vla_diag)\r\n\r\n        # Publish diagnostics\r\n        self.diag_pub.publish(diag_array)\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SystemMonitorNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info('System monitor node stopped by user')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,s.jsx)(e.h2,{id:"testing--verification",children:"Testing & Verification"}),"\n",(0,s.jsx)(e.h3,{id:"running-the-complete-vla-system",children:"Running the Complete VLA System"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Build the integration packages:"})}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"cd ~/ros2_ws\r\ncolcon build --packages-select vla_integration\r\nsource install/setup.bash\n"})}),"\n",(0,s.jsxs)(e.ol,{start:"2",children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Run the complete VLA system:"})}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Terminal 1: Launch the complete system\r\nros2 launch vla_integration vla_system_launch.py\r\n\r\n# Terminal 2: Send test commands\r\nros2 topic pub /voice_commands std_msgs/msg/String \"data: 'Go to the kitchen'\"\r\n\r\n# Terminal 3: Monitor system status\r\nros2 topic echo /system_status\r\n\r\n# Terminal 4: Monitor diagnostics\r\nros2 topic echo /diagnostics\n"})}),"\n",(0,s.jsxs)(e.ol,{start:"3",children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Test multimodal integration:"})}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Combine voice and vision inputs\r\nros2 topic pub /voice_commands std_msgs/msg/String \"data: 'Find the red ball'\"\r\n# Then check if perception system responds appropriately\n"})}),"\n",(0,s.jsx)(e.h3,{id:"useful-vla-integration-commands",children:"Useful VLA Integration Commands"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Monitor all VLA topics:"})}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Use multiple terminals to monitor\r\nros2 topic echo /vla_status\r\nros2 topic echo /fused_state\r\nros2 topic echo /fused_decision\r\nros2 topic echo /coordinator_status\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Check system diagnostics:"})}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"ros2 topic echo /diagnostics\n"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:(0,s.jsx)(e.strong,{children:"Visualize in RViz:"})}),"\n"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# For navigation and perception visualization\r\nrviz2\n"})}),"\n",(0,s.jsx)(e.h3,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Test response times for different input combinations\r\n# Test coordination between modalities\r\n# Test system robustness under various conditions\r\n# Measure resource utilization\n"})}),"\n",(0,s.jsx)(e.h2,{id:"common-issues",children:"Common Issues"}),"\n",(0,s.jsx)(e.h3,{id:"issue-timing-mismatches-between-modalities",children:"Issue: Timing mismatches between modalities"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement proper timestamp synchronization"}),"\n",(0,s.jsx)(e.li,{children:"Use message filters for time-based synchronization"}),"\n",(0,s.jsx)(e.li,{children:"Implement buffering mechanisms for temporal alignment"}),"\n",(0,s.jsx)(e.li,{children:"Use appropriate QoS profiles for different data types"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"issue-coordination-conflicts-between-modalities",children:"Issue: Coordination conflicts between modalities"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement priority-based arbitration"}),"\n",(0,s.jsx)(e.li,{children:"Use state machines for clear coordination protocols"}),"\n",(0,s.jsx)(e.li,{children:"Establish clear ownership of action execution"}),"\n",(0,s.jsx)(e.li,{children:"Implement conflict resolution strategies"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"issue-computational-overload-with-multimodal-processing",children:"Issue: Computational overload with multimodal processing"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Use efficient fusion algorithms"}),"\n",(0,s.jsx)(e.li,{children:"Implement selective processing based on relevance"}),"\n",(0,s.jsx)(e.li,{children:"Use multi-threading for parallel processing"}),"\n",(0,s.jsx)(e.li,{children:"Optimize individual modality processing pipelines"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"issue-semantic-gap-between-modalities",children:"Issue: Semantic gap between modalities"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Solution"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Develop shared semantic representations"}),"\n",(0,s.jsx)(e.li,{children:"Implement cross-modal grounding mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Use attention mechanisms to focus on relevant information"}),"\n",(0,s.jsx)(e.li,{children:"Create domain-specific ontologies for concept alignment"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"VLA integration requires careful coordination of multiple processing modalities"}),"\n",(0,s.jsx)(e.li,{children:"Temporal and spatial alignment is crucial for effective fusion"}),"\n",(0,s.jsx)(e.li,{children:"System architecture should support modular development and testing"}),"\n",(0,s.jsx)(e.li,{children:"Safety and error handling are paramount in integrated systems"}),"\n",(0,s.jsx)(e.li,{children:"Performance monitoring ensures system reliability"}),"\n",(0,s.jsx)(e.li,{children:"Clear interfaces between modules enable maintainable code"}),"\n",(0,s.jsx)(e.li,{children:"Feedback loops improve system adaptability and robustness"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(e.p,{children:"In the next chapter, you'll learn about the complete capstone project that brings together all the concepts learned in this book to create an autonomous humanoid robot system."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},7074:(n,e,r)=>{r.d(e,{R:()=>i,x:()=>o});var t=r(6540);const s={},a=t.createContext(s);function i(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);