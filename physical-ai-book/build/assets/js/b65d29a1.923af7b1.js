"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5197],{7074:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>a});var o=r(6540);const i={},s=o.createContext(i);function t(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),o.createElement(s.Provider,{value:n},e.children)}},9907:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>t,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"vla/voice-commands","title":"Voice Commands - Natural Language Interaction with Robots","description":"Learning Objectives","source":"@site/docs/vla/voice-commands.md","sourceDirName":"vla","slug":"/vla/voice-commands","permalink":"/docs/vla/voice-commands","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/voice-commands.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"LLM Planning - Cognitive Planning with Large Language Models","permalink":"/docs/vla/llm-planning"},"next":{"title":"Capstone Project Integration Guide","permalink":"/docs/capstone/capstone-project-integration-guide"}}');var i=r(4848),s=r(7074);const t={sidebar_position:1},a="Voice Commands - Natural Language Interaction with Robots",c={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Conceptual Overview",id:"conceptual-overview",level:2},{value:"Key Components of Voice Command Systems",id:"key-components-of-voice-command-systems",level:3},{value:"Voice Command Architecture",id:"voice-command-architecture",level:3},{value:"Benefits of Voice Commands",id:"benefits-of-voice-commands",level:3},{value:"Hands-On Implementation",id:"hands-on-implementation",level:2},{value:"Installing Speech Recognition Dependencies",id:"installing-speech-recognition-dependencies",level:3},{value:"Basic Speech Recognition Node",id:"basic-speech-recognition-node",level:3},{value:"Advanced Voice Command with Natural Language Processing",id:"advanced-voice-command-with-natural-language-processing",level:3},{value:"Offline Speech Recognition with Vosk",id:"offline-speech-recognition-with-vosk",level:3},{value:"Text-to-Speech Feedback Node",id:"text-to-speech-feedback-node",level:3},{value:"Creating a Voice Command Package",id:"creating-a-voice-command-package",level:3},{value:"Launch File for Voice Commands",id:"launch-file-for-voice-commands",level:3},{value:"Testing &amp; Verification",id:"testing--verification",level:2},{value:"Running Voice Command System",id:"running-voice-command-system",level:3},{value:"Useful Voice Command Commands",id:"useful-voice-command-commands",level:3},{value:"Performance Testing",id:"performance-testing",level:3},{value:"Common Issues",id:"common-issues",level:2},{value:"Issue: Microphone not detected or no audio input",id:"issue-microphone-not-detected-or-no-audio-input",level:3},{value:"Issue: High CPU usage with speech recognition",id:"issue-high-cpu-usage-with-speech-recognition",level:3},{value:"Issue: Poor recognition accuracy",id:"issue-poor-recognition-accuracy",level:3},{value:"Issue: Delay in voice feedback",id:"issue-delay-in-voice-feedback",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-commands---natural-language-interaction-with-robots",children:"Voice Commands - Natural Language Interaction with Robots"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement speech recognition systems for robot command interpretation"}),"\n",(0,i.jsx)(n.li,{children:"Design natural language processing pipelines for voice commands"}),"\n",(0,i.jsx)(n.li,{children:"Integrate voice command systems with robot control architectures"}),"\n",(0,i.jsx)(n.li,{children:"Create custom voice command vocabularies and grammars"}),"\n",(0,i.jsx)(n.li,{children:"Handle speech-to-text conversion and command parsing"}),"\n",(0,i.jsx)(n.li,{children:"Implement voice feedback and confirmation systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before starting this chapter, you should:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Have ROS 2 Humble Hawksbill installed and configured"}),"\n",(0,i.jsx)(n.li,{children:"Understand ROS 2 nodes, topics, and message types"}),"\n",(0,i.jsx)(n.li,{children:"Completed the Isaac perception and navigation chapters"}),"\n",(0,i.jsx)(n.li,{children:"Basic knowledge of Python and speech processing concepts"}),"\n",(0,i.jsx)(n.li,{children:"Understanding of natural language processing fundamentals"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"conceptual-overview",children:"Conceptual Overview"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Voice Command Systems"})," enable robots to understand and respond to spoken instructions from humans. This creates a more natural and intuitive interaction model compared to traditional button-based or app-based interfaces."]}),"\n",(0,i.jsx)(n.h3,{id:"key-components-of-voice-command-systems",children:"Key Components of Voice Command Systems"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech Recognition"}),": Converting audio to text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Interpreting meaning from text"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Mapping"}),": Connecting understood commands to robot actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Voice Feedback"}),": Providing audio confirmation to users"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Noise Filtering"}),": Processing audio in noisy environments"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"voice-command-architecture",children:"Voice Command Architecture"}),"\n",(0,i.jsx)(n.p,{children:"The typical voice command architecture follows this flow:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Audio Input \u2192 Noise Reduction \u2192 Speech Recognition \u2192 NLU \u2192 Command Mapping \u2192 Robot Action\n"})}),"\n",(0,i.jsx)(n.h3,{id:"benefits-of-voice-commands",children:"Benefits of Voice Commands"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Natural Interaction"}),": More intuitive than manual controls"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Accessibility"}),": Helps users with mobility limitations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hands-Free Operation"}),": Allows multitasking"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Remote Control"}),": Works at distances where physical controls don't"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multilingual Support"}),": Can support multiple languages"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-implementation",children:"Hands-On Implementation"}),"\n",(0,i.jsx)(n.h3,{id:"installing-speech-recognition-dependencies",children:"Installing Speech Recognition Dependencies"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install system dependencies\r\nsudo apt update\r\nsudo apt install python3-pyaudio python3-speechrecognition python3-pip portaudio19-dev\r\n\r\n# Install Python packages\r\npip3 install SpeechRecognition pyttsx3 vosk transformers torch numpy\r\npip3 install sounddevice webrtcvad pyaudio\n"})}),"\n",(0,i.jsx)(n.h3,{id:"basic-speech-recognition-node",children:"Basic Speech Recognition Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\n"""\r\nBasic speech recognition node for voice commands.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nimport speech_recognition as sr\r\nimport threading\r\nimport queue\r\nimport time\r\n\r\n\r\nclass VoiceCommandNode(Node):\r\n    """\r\n    Node to recognize voice commands and convert them to robot actions.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_node\')\r\n\r\n        # Initialize speech recognizer\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n\r\n        # Calibrate microphone for ambient noise\r\n        with self.microphone as source:\r\n            self.get_logger().info(\'Calibrating for ambient noise...\')\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n            self.get_logger().info(\'Calibration complete\')\r\n\r\n        # Publishers\r\n        self.command_pub = self.create_publisher(String, \'voice_commands\', 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\r\n\r\n        # Command mappings\r\n        self.command_mappings = {\r\n            \'move forward\': self.move_forward,\r\n            \'go forward\': self.move_forward,\r\n            \'move backward\': self.move_backward,\r\n            \'go backward\': self.move_backward,\r\n            \'turn left\': self.turn_left,\r\n            \'turn right\': self.turn_right,\r\n            \'stop\': self.stop_robot,\r\n            \'halt\': self.stop_robot,\r\n            \'go left\': self.move_left,\r\n            \'go right\': self.move_right,\r\n        }\r\n\r\n        # Start listening thread\r\n        self.listen_thread = threading.Thread(target=self.listen_continuously)\r\n        self.listen_thread.daemon = True\r\n        self.listen_thread.start()\r\n\r\n        # Queue for audio processing\r\n        self.audio_queue = queue.Queue()\r\n\r\n        self.get_logger().info(\'Voice command node initialized\')\r\n\r\n    def listen_continuously(self):\r\n        """Continuously listen for voice commands."""\r\n        with self.microphone as source:\r\n            self.get_logger().info(\'Listening for voice commands...\')\r\n\r\n        while rclpy.ok():\r\n            try:\r\n                # Listen for audio with timeout\r\n                with self.microphone as source:\r\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\r\n\r\n                # Add audio to queue for processing\r\n                self.audio_queue.put(audio)\r\n\r\n            except sr.WaitTimeoutError:\r\n                # Continue listening if no audio detected\r\n                continue\r\n            except Exception as e:\r\n                self.get_logger().error(f\'Error in audio capture: {e}\')\r\n                time.sleep(0.1)\r\n\r\n    def process_audio(self):\r\n        """Process audio from the queue."""\r\n        while not self.audio_queue.empty():\r\n            try:\r\n                audio = self.audio_queue.get_nowait()\r\n\r\n                # Recognize speech using Google\'s service\r\n                try:\r\n                    text = self.recognizer.recognize_google(audio)\r\n                    self.get_logger().info(f\'Recognized: {text}\')\r\n\r\n                    # Process the recognized text\r\n                    self.process_command(text.lower().strip())\r\n\r\n                except sr.UnknownValueError:\r\n                    self.get_logger().info(\'Could not understand audio\')\r\n                except sr.RequestError as e:\r\n                    self.get_logger().error(f\'Error with speech recognition service: {e}\')\r\n\r\n            except queue.Empty:\r\n                break\r\n\r\n    def process_command(self, text):\r\n        """Process recognized text and execute appropriate command."""\r\n        # Publish the recognized command\r\n        cmd_msg = String()\r\n        cmd_msg.data = text\r\n        self.command_pub.publish(cmd_msg)\r\n\r\n        # Check if command matches any mapping\r\n        for command_phrase, command_func in self.command_mappings.items():\r\n            if command_phrase in text:\r\n                self.get_logger().info(f\'Executing command: {command_phrase}\')\r\n                command_func()\r\n                return\r\n\r\n        # If no command matched\r\n        self.get_logger().info(f\'Unrecognized command: {text}\')\r\n\r\n    def move_forward(self):\r\n        """Move robot forward."""\r\n        twist = Twist()\r\n        twist.linear.x = 0.5  # m/s\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def move_backward(self):\r\n        """Move robot backward."""\r\n        twist = Twist()\r\n        twist.linear.x = -0.5  # m/s\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def turn_left(self):\r\n        """Turn robot left."""\r\n        twist = Twist()\r\n        twist.angular.z = 0.5  # rad/s\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def turn_right(self):\r\n        """Turn robot right."""\r\n        twist = Twist()\r\n        twist.angular.z = -0.5  # rad/s\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def move_left(self):\r\n        """Strafe robot left."""\r\n        twist = Twist()\r\n        twist.linear.y = 0.5  # m/s\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def move_right(self):\r\n        """Strafe robot right."""\r\n        twist = Twist()\r\n        twist.linear.y = -0.5  # m/s\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def stop_robot(self):\r\n        """Stop robot movement."""\r\n        twist = Twist()\r\n        # All velocities remain 0\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def spin_once(self):\r\n        """Process any queued audio."""\r\n        self.process_audio()\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceCommandNode()\r\n\r\n    try:\r\n        while rclpy.ok():\r\n            node.spin_once()\r\n            time.sleep(0.01)  # Small delay to prevent busy waiting\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Voice command node stopped by user\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"advanced-voice-command-with-natural-language-processing",children:"Advanced Voice Command with Natural Language Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\n"""\r\nAdvanced voice command node with natural language understanding.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nfrom sensor_msgs.msg import LaserScan\r\nimport speech_recognition as sr\r\nimport threading\r\nimport queue\r\nimport time\r\nimport re\r\nfrom enum import Enum\r\n\r\n\r\nclass CommandType(Enum):\r\n    MOVE = "move"\r\n    TURN = "turn"\r\n    STOP = "stop"\r\n    NAVIGATE = "navigate"\r\n    QUERY = "query"\r\n\r\n\r\nclass Command:\r\n    """Represents a parsed voice command."""\r\n    def __init__(self, cmd_type, direction=None, distance=None, speed=None, destination=None):\r\n        self.type = cmd_type\r\n        self.direction = direction\r\n        self.distance = distance\r\n        self.speed = speed\r\n        self.destination = destination\r\n\r\n\r\nclass AdvancedVoiceCommandNode(Node):\r\n    """\r\n    Advanced voice command node with natural language understanding.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'advanced_voice_command_node\')\r\n\r\n        # Initialize speech recognizer\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n\r\n        # Calibrate microphone\r\n        with self.microphone as source:\r\n            self.get_logger().info(\'Calibrating for ambient noise...\')\r\n            self.recognizer.adjust_for_ambient_noise(source)\r\n            self.get_logger().info(\'Calibration complete\')\r\n\r\n        # Publishers\r\n        self.command_pub = self.create_publisher(String, \'parsed_commands\', 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\r\n\r\n        # Subscribers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            \'/scan\',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n\r\n        # Initialize variables\r\n        self.laser_data = None\r\n        self.command_history = []\r\n\r\n        # Start listening thread\r\n        self.listen_thread = threading.Thread(target=self.listen_continuously)\r\n        self.listen_thread.daemon = True\r\n        self.listen_thread.start()\r\n\r\n        # Queue for audio processing\r\n        self.audio_queue = queue.Queue()\r\n\r\n        self.get_logger().info(\'Advanced voice command node initialized\')\r\n\r\n    def scan_callback(self, msg):\r\n        """Store laser scan data for obstacle detection."""\r\n        self.laser_data = msg\r\n\r\n    def listen_continuously(self):\r\n        """Continuously listen for voice commands."""\r\n        with self.microphone as source:\r\n            self.get_logger().info(\'Listening for voice commands...\')\r\n\r\n        while rclpy.ok():\r\n            try:\r\n                with self.microphone as source:\r\n                    audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5.0)\r\n                self.audio_queue.put(audio)\r\n            except sr.WaitTimeoutError:\r\n                continue\r\n            except Exception as e:\r\n                self.get_logger().error(f\'Error in audio capture: {e}\')\r\n                time.sleep(0.1)\r\n\r\n    def process_audio(self):\r\n        """Process audio from the queue."""\r\n        while not self.audio_queue.empty():\r\n            try:\r\n                audio = self.audio_queue.get_nowait()\r\n\r\n                try:\r\n                    text = self.recognizer.recognize_google(audio)\r\n                    self.get_logger().info(f\'Recognized: {text}\')\r\n\r\n                    # Parse and execute command\r\n                    self.parse_and_execute_command(text.lower().strip())\r\n\r\n                except sr.UnknownValueError:\r\n                    self.get_logger().info(\'Could not understand audio\')\r\n                except sr.RequestError as e:\r\n                    self.get_logger().error(f\'Error with speech recognition service: {e}\')\r\n\r\n            except queue.Empty:\r\n                break\r\n\r\n    def parse_and_execute_command(self, text):\r\n        """Parse text and execute appropriate command."""\r\n        # Publish original recognized text\r\n        original_msg = String()\r\n        original_msg.data = text\r\n        self.command_pub.publish(original_msg)\r\n\r\n        # Parse the command\r\n        command = self.parse_command(text)\r\n\r\n        if command:\r\n            self.get_logger().info(f\'Parsed command: {command.type.value}\')\r\n\r\n            # Execute based on command type\r\n            if command.type == CommandType.MOVE:\r\n                self.execute_move_command(command)\r\n            elif command.type == CommandType.TURN:\r\n                self.execute_turn_command(command)\r\n            elif command.type == CommandType.STOP:\r\n                self.execute_stop_command(command)\r\n            elif command.type == CommandType.NAVIGATE:\r\n                self.execute_navigate_command(command)\r\n            elif command.type == CommandType.QUERY:\r\n                self.execute_query_command(command)\r\n\r\n            # Store in history\r\n            self.command_history.append(command)\r\n        else:\r\n            self.get_logger().info(f\'Could not parse command: {text}\')\r\n\r\n    def parse_command(self, text):\r\n        """Parse text to extract command components."""\r\n        # Define patterns for different command types\r\n\r\n        # Move commands: "move forward 2 meters", "go left slowly"\r\n        move_patterns = [\r\n            r\'(?:move|go)\\s+(forward|backward|ahead|back|left|right|up|down)\',\r\n            r\'(?:move|go)\\s+(?:the\\s+)?robot\\s+(forward|backward|ahead|back|left|right)\'\r\n        ]\r\n\r\n        # Turn commands: "turn left", "rotate right"\r\n        turn_patterns = [\r\n            r\'(?:turn|rotate)\\s+(left|right)\',\r\n            r\'(?:spin|pivot)\\s+(left|right)\'\r\n        ]\r\n\r\n        # Stop commands: "stop", "halt", "freeze"\r\n        stop_patterns = [\r\n            r\'(?:stop|halt|freeze|pause|stand)\',\r\n            r\'(?:come\\s+to\\s+a\\s+stop|stop\\s+moving)\'\r\n        ]\r\n\r\n        # Navigate commands: "go to kitchen", "navigate to charging station"\r\n        navigate_patterns = [\r\n            r\'(?:go\\s+to|navigate\\s+to|move\\s+to)\\s+(.+?)(?:\\s+please|\\.|$)\',\r\n            r\'(?:take\\s+me\\s+to|bring\\s+me\\s+to)\\s+(.+?)(?:\\s+please|\\.|$)\'\r\n        ]\r\n\r\n        # Query commands: "where are you", "what is your status"\r\n        query_patterns = [\r\n            r\'(?:where\\s+are\\s+you|where\\s+is\\s+the\\s+robot|location|position)\',\r\n            r\'(?:status|what\\s+are\\s+you\\s+doing|what\\s+is\\s+your\\s+status)\',\r\n            r\'(?:battery|power|charge|energy)\'\r\n        ]\r\n\r\n        # Extract distance and speed modifiers\r\n        distance_match = re.search(r\'(\\d+(?:\\.\\d+)?)\\s*(?:meter|m|foot|ft)\', text)\r\n        speed_match = re.search(r\'(slowly|slow|fast|quickly|quick)\', text)\r\n\r\n        # Try to match patterns\r\n        for pattern in move_patterns:\r\n            match = re.search(pattern, text)\r\n            if match:\r\n                direction = match.group(1)\r\n                distance = float(distance_match.group(1)) if distance_match else None\r\n                speed = speed_match.group(1) if speed_match else None\r\n                return Command(CommandType.MOVE, direction=direction, distance=distance, speed=speed)\r\n\r\n        for pattern in turn_patterns:\r\n            match = re.search(pattern, text)\r\n            if match:\r\n                direction = match.group(1)\r\n                speed = speed_match.group(1) if speed_match else None\r\n                return Command(CommandType.TURN, direction=direction, speed=speed)\r\n\r\n        for pattern in stop_patterns:\r\n            if re.search(pattern, text):\r\n                return Command(CommandType.STOP)\r\n\r\n        for pattern in navigate_patterns:\r\n            match = re.search(pattern, text)\r\n            if match:\r\n                destination = match.group(1).strip()\r\n                return Command(CommandType.NAVIGATE, destination=destination)\r\n\r\n        for pattern in query_patterns:\r\n            if re.search(pattern, text):\r\n                return Command(CommandType.QUERY)\r\n\r\n        return None  # No command matched\r\n\r\n    def execute_move_command(self, command):\r\n        """Execute a move command."""\r\n        twist = Twist()\r\n\r\n        if command.direction in [\'forward\', \'ahead\']:\r\n            twist.linear.x = 0.5 if command.speed != \'slow\' else 0.2\r\n        elif command.direction in [\'backward\', \'back\']:\r\n            twist.linear.x = -0.5 if command.speed != \'slow\' else -0.2\r\n        elif command.direction == \'left\':\r\n            twist.linear.y = 0.3 if command.speed != \'slow\' else 0.15\r\n        elif command.direction == \'right\':\r\n            twist.linear.y = -0.3 if command.speed != \'slow\' else -0.15\r\n        elif command.direction == \'up\':\r\n            twist.linear.z = 0.2\r\n        elif command.direction == \'down\':\r\n            twist.linear.z = -0.2\r\n\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def execute_turn_command(self, command):\r\n        """Execute a turn command."""\r\n        twist = Twist()\r\n\r\n        if command.direction == \'left\':\r\n            twist.angular.z = 0.5 if command.speed != \'slow\' else 0.2\r\n        elif command.direction == \'right\':\r\n            twist.angular.z = -0.5 if command.speed != \'slow\' else -0.2\r\n\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def execute_stop_command(self, command):\r\n        """Execute a stop command."""\r\n        twist = Twist()\r\n        # All velocities remain 0\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def execute_navigate_command(self, command):\r\n        """Execute a navigate command."""\r\n        self.get_logger().info(f\'Navigating to: {command.destination}\')\r\n        # In a real implementation, this would interface with navigation stack\r\n        # For now, just log the command\r\n        pass\r\n\r\n    def execute_query_command(self, command):\r\n        """Execute a query command."""\r\n        # In a real implementation, this would respond with robot status\r\n        self.get_logger().info(f\'Query command received: {command.destination}\')\r\n        pass\r\n\r\n    def spin_once(self):\r\n        """Process any queued audio."""\r\n        self.process_audio()\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = AdvancedVoiceCommandNode()\r\n\r\n    try:\r\n        while rclpy.ok():\r\n            node.spin_once()\r\n            time.sleep(0.01)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Advanced voice command node stopped by user\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"offline-speech-recognition-with-vosk",children:"Offline Speech Recognition with Vosk"}),"\n",(0,i.jsx)(n.p,{children:"For privacy-sensitive applications, we can use offline speech recognition:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\n"""\r\nOffline voice command node using Vosk for speech recognition.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Twist\r\nfrom vosk import Model, KaldiRecognizer\r\nimport pyaudio\r\nimport json\r\nimport threading\r\nimport queue\r\n\r\n\r\nclass OfflineVoiceCommandNode(Node):\r\n    """\r\n    Voice command node using offline Vosk speech recognition.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'offline_voice_command_node\')\r\n\r\n        # Initialize Vosk model (download from https://alphacephei.com/vosk/models)\r\n        try:\r\n            self.model = Model(lang="en-us")  # Change language as needed\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Failed to load Vosk model: {e}\')\r\n            self.get_logger().error(\'Download model from https://alphacephei.com/vosk/models\')\r\n            raise\r\n\r\n        # Initialize audio\r\n        self.sample_rate = 16000\r\n        self.rec = KaldiRecognizer(self.model, self.sample_rate)\r\n\r\n        # Initialize PyAudio\r\n        self.p = pyaudio.PyAudio()\r\n        self.stream = self.p.open(\r\n            format=pyaudio.paInt16,\r\n            channels=1,\r\n            rate=self.sample_rate,\r\n            input=True,\r\n            frames_per_buffer=8000\r\n        )\r\n\r\n        # Publishers\r\n        self.command_pub = self.create_publisher(String, \'offline_commands\', 10)\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\r\n\r\n        # Command mappings\r\n        self.command_mappings = {\r\n            \'move forward\': self.move_forward,\r\n            \'go forward\': self.move_forward,\r\n            \'move backward\': self.move_backward,\r\n            \'go backward\': self.move_backward,\r\n            \'turn left\': self.turn_left,\r\n            \'turn right\': self.turn_right,\r\n            \'stop\': self.stop_robot,\r\n            \'halt\': self.stop_robot,\r\n        }\r\n\r\n        # Audio processing thread\r\n        self.audio_thread = threading.Thread(target=self.process_audio_stream)\r\n        self.audio_thread.daemon = True\r\n        self.audio_thread.start()\r\n\r\n        self.get_logger().info(\'Offline voice command node initialized\')\r\n\r\n    def process_audio_stream(self):\r\n        """Process audio stream continuously."""\r\n        while rclpy.ok():\r\n            try:\r\n                data = self.stream.read(4000, exception_on_overflow=False)\r\n\r\n                if len(data) == 0:\r\n                    continue\r\n\r\n                if self.rec.AcceptWaveform(data):\r\n                    result = self.rec.Result()\r\n                    result_dict = json.loads(result)\r\n\r\n                    if \'text\' in result_dict and result_dict[\'text\']:\r\n                        text = result_dict[\'text\'].lower().strip()\r\n                        if text:  # Only process non-empty text\r\n                            self.get_logger().info(f\'Recognized (offline): {text}\')\r\n                            self.process_command(text)\r\n\r\n            except Exception as e:\r\n                self.get_logger().error(f\'Error in audio processing: {e}\')\r\n\r\n    def process_command(self, text):\r\n        """Process recognized text and execute appropriate command."""\r\n        # Publish the recognized command\r\n        cmd_msg = String()\r\n        cmd_msg.data = text\r\n        self.command_pub.publish(cmd_msg)\r\n\r\n        # Check if command matches any mapping\r\n        for command_phrase, command_func in self.command_mappings.items():\r\n            if command_phrase in text:\r\n                self.get_logger().info(f\'Executing command: {command_phrase}\')\r\n                command_func()\r\n                return\r\n\r\n        # If no command matched\r\n        self.get_logger().info(f\'Unrecognized command: {text}\')\r\n\r\n    def move_forward(self):\r\n        """Move robot forward."""\r\n        twist = Twist()\r\n        twist.linear.x = 0.5\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def move_backward(self):\r\n        """Move robot backward."""\r\n        twist = Twist()\r\n        twist.linear.x = -0.5\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def turn_left(self):\r\n        """Turn robot left."""\r\n        twist = Twist()\r\n        twist.angular.z = 0.5\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def turn_right(self):\r\n        """Turn robot right."""\r\n        twist = Twist()\r\n        twist.angular.z = -0.5\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def stop_robot(self):\r\n        """Stop robot movement."""\r\n        twist = Twist()\r\n        self.cmd_vel_pub.publish(twist)\r\n\r\n    def destroy_node(self):\r\n        """Clean up resources."""\r\n        if hasattr(self, \'stream\'):\r\n            self.stream.stop_stream()\r\n            self.stream.close()\r\n        if hasattr(self, \'p\'):\r\n            self.p.terminate()\r\n        super().destroy_node()\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = OfflineVoiceCommandNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Offline voice command node stopped by user\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"text-to-speech-feedback-node",children:"Text-to-Speech Feedback Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n\r\n"""\r\nText-to-speech feedback node for voice command responses.\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport pyttsx3\r\nimport threading\r\n\r\n\r\nclass VoiceFeedbackNode(Node):\r\n    """\r\n    Node to provide voice feedback for robot actions.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(\'voice_feedback_node\')\r\n\r\n        # Initialize text-to-speech engine\r\n        self.tts_engine = pyttsx3.init()\r\n\r\n        # Configure voice properties\r\n        voices = self.tts_engine.getProperty(\'voices\')\r\n        if voices:\r\n            self.tts_engine.setProperty(\'voice\', voices[0].id)  # Use first available voice\r\n        self.tts_engine.setProperty(\'rate\', 150)  # Speed of speech\r\n        self.tts_engine.setProperty(\'volume\', 0.9)  # Volume level\r\n\r\n        # Create subscriber for feedback requests\r\n        self.feedback_sub = self.create_subscription(\r\n            String,\r\n            \'voice_feedback\',\r\n            self.feedback_callback,\r\n            10\r\n        )\r\n\r\n        # Thread lock for TTS\r\n        self.tts_lock = threading.Lock()\r\n\r\n        self.get_logger().info(\'Voice feedback node initialized\')\r\n\r\n    def feedback_callback(self, msg):\r\n        """Handle feedback requests."""\r\n        text = msg.data\r\n        self.get_logger().info(f\'Playing voice feedback: {text}\')\r\n\r\n        # Speak the text in a separate thread to avoid blocking\r\n        speak_thread = threading.Thread(target=self.speak_text, args=(text,))\r\n        speak_thread.daemon = True\r\n        speak_thread.start()\r\n\r\n    def speak_text(self, text):\r\n        """Speak text using TTS engine."""\r\n        with self.tts_lock:\r\n            try:\r\n                self.tts_engine.say(text)\r\n                self.tts_engine.runAndWait()\r\n            except Exception as e:\r\n                self.get_logger().error(f\'Error in text-to-speech: {e}\')\r\n\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceFeedbackNode()\r\n\r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info(\'Voice feedback node stopped by user\')\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"creating-a-voice-command-package",children:"Creating a Voice Command Package"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Create the package:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws/src\r\nros2 pkg create --build-type ament_python voice_commands\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Update package.xml:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\r\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\r\n<package format="3">\r\n  <name>voice_commands</name>\r\n  <version>0.0.1</version>\r\n  <description>Voice command processing for robotics</description>\r\n  <maintainer email="user@todo.todo">user</maintainer>\r\n  <license>Apache-2.0</license>\r\n\r\n  <depend>rclpy</depend>\r\n  <depend>std_msgs</depend>\r\n  <depend>geometry_msgs</depend>\r\n  <depend>sensor_msgs</depend>\r\n\r\n  <export>\r\n    <build_type>ament_python</build_type>\r\n  </export>\r\n</package>\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Create setup.py:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\r\n\r\npackage_name = 'voice_commands'\r\n\r\nsetup(\r\n    name=package_name,\r\n    version='0.0.1',\r\n    packages=find_packages(exclude=['test']),\r\n    data_files=[\r\n        ('share/ament_index/resource_index/packages',\r\n            ['resource/' + package_name]),\r\n        ('share/' + package_name, ['package.xml']),\r\n    ],\r\n    install_requires=['setuptools'],\r\n    zip_safe=True,\r\n    maintainer='user',\r\n    maintainer_email='user@todo.todo',\r\n    description='Voice command processing for robotics',\r\n    license='Apache-2.0',\r\n    tests_require=['pytest'],\r\n    entry_points={\r\n        'console_scripts': [\r\n            'voice_command_node = voice_commands.voice_command_node:main',\r\n            'advanced_voice_node = voice_commands.advanced_voice_node:main',\r\n            'offline_voice_node = voice_commands.offline_voice_node:main',\r\n            'voice_feedback_node = voice_commands.voice_feedback_node:main',\r\n        ],\r\n    },\r\n)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"launch-file-for-voice-commands",children:"Launch File for Voice Commands"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Create launch/voice_commands_launch.py:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\r\nfrom launch.actions import DeclareLaunchArgument\r\nfrom launch.substitutions import LaunchConfiguration\r\nfrom launch_ros.actions import Node\r\n\r\n\r\ndef generate_launch_description():\r\n    # Declare launch arguments\r\n    use_sim_time = DeclareLaunchArgument(\r\n        'use_sim_time',\r\n        default_value='false',\r\n        description='Use simulation time if true'\r\n    )\r\n\r\n    # Voice command node\r\n    voice_command_node = Node(\r\n        package='voice_commands',\r\n        executable='voice_command_node',\r\n        name='voice_command_node',\r\n        parameters=[\r\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    # Voice feedback node\r\n    voice_feedback_node = Node(\r\n        package='voice_commands',\r\n        executable='voice_feedback_node',\r\n        name='voice_feedback_node',\r\n        parameters=[\r\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\r\n        ],\r\n        output='screen'\r\n    )\r\n\r\n    return LaunchDescription([\r\n        use_sim_time,\r\n        voice_command_node,\r\n        voice_feedback_node\r\n    ])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"testing--verification",children:"Testing & Verification"}),"\n",(0,i.jsx)(n.h3,{id:"running-voice-command-system",children:"Running Voice Command System"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Install dependencies:"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip3 install SpeechRecognition pyttsx3 vosk transformers torch numpy\r\nsudo apt install python3-pyaudio portaudio19-dev\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Build the package:"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/ros2_ws\r\ncolcon build --packages-select voice_commands\r\nsource install/setup.bash\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"3",children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Run the voice command system:"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Run the voice command node\r\nros2 run voice_commands voice_command_node\r\n\r\n# Terminal 2: Test with manual commands\r\nros2 topic pub /voice_feedback std_msgs/msg/String \"data: 'Hello, I am ready to receive commands'\"\n"})}),"\n",(0,i.jsxs)(n.ol,{start:"4",children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Test with different voice commands:"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Speak commands like:\r\n# "Move forward"\r\n# "Turn left"\r\n# "Stop"\r\n# "Go backward"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"useful-voice-command-commands",children:"Useful Voice Command Commands"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Monitor voice commands:"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /voice_commands\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Send feedback:"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 topic pub /voice_feedback std_msgs/msg/String \"data: 'Command received'\"\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Check audio devices:"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Python script to list audio devices\r\npython3 -c \"import pyaudio; p = pyaudio.PyAudio(); print('Devices:', p.get_device_count()); [print(i, p.get_device_info_by_index(i)['name']) for i in range(p.get_device_count())]\"\n"})}),"\n",(0,i.jsx)(n.h3,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Test recognition accuracy in different environments\r\n# Test response time to commands\r\n# Test with different speakers and accents\n"})}),"\n",(0,i.jsx)(n.h2,{id:"common-issues",children:"Common Issues"}),"\n",(0,i.jsx)(n.h3,{id:"issue-microphone-not-detected-or-no-audio-input",children:"Issue: Microphone not detected or no audio input"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Check microphone permissions and connections"}),"\n",(0,i.jsxs)(n.li,{children:["Verify audio device using ",(0,i.jsx)(n.code,{children:"arecord -l"})]}),"\n",(0,i.jsxs)(n.li,{children:["Test microphone with ",(0,i.jsx)(n.code,{children:"arecord -D hw:0,0 -f cd test.wav"})]}),"\n",(0,i.jsx)(n.li,{children:"Ensure proper ALSA/PulseAudio configuration"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"issue-high-cpu-usage-with-speech-recognition",children:"Issue: High CPU usage with speech recognition"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use offline models like Vosk for continuous recognition"}),"\n",(0,i.jsx)(n.li,{children:"Implement wake word detection to activate recognition only when needed"}),"\n",(0,i.jsx)(n.li,{children:"Reduce sampling rate if quality permits"}),"\n",(0,i.jsx)(n.li,{children:"Use threading to prevent blocking"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"issue-poor-recognition-accuracy",children:"Issue: Poor recognition accuracy"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Calibrate microphone for ambient noise"}),"\n",(0,i.jsx)(n.li,{children:"Use directional microphones in noisy environments"}),"\n",(0,i.jsx)(n.li,{children:"Train custom acoustic models for specific environments"}),"\n",(0,i.jsx)(n.li,{children:"Implement confidence thresholds to filter uncertain recognitions"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"issue-delay-in-voice-feedback",children:"Issue: Delay in voice feedback"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solution"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use streaming TTS engines"}),"\n",(0,i.jsx)(n.li,{children:"Implement interruptible speech for urgent feedback"}),"\n",(0,i.jsx)(n.li,{children:"Optimize audio processing pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Use lightweight synthesis models"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Voice commands provide natural, intuitive robot interaction"}),"\n",(0,i.jsx)(n.li,{children:"Offline recognition offers privacy and reliability"}),"\n",(0,i.jsx)(n.li,{children:"Natural language processing enables complex command understanding"}),"\n",(0,i.jsx)(n.li,{children:"Audio feedback improves user experience and confirms actions"}),"\n",(0,i.jsx)(n.li,{children:"Wake word detection reduces power consumption"}),"\n",(0,i.jsx)(n.li,{children:"Noise filtering is crucial for real-world deployment"}),"\n",(0,i.jsx)(n.li,{children:"Integration with navigation and control systems enables complex tasks"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"In the next chapter, you'll learn about Large Language Model (LLM) integration for advanced planning and reasoning in robotics applications."})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);