"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9401],{127:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"isaac-platform/robot-manipulation-rl-example","title":"Basic RL Training Example for Robot Manipulation","description":"This document provides a practical example of reinforcement learning for robot manipulation tasks in Isaac Sim.","source":"@site/docs/isaac-platform/robot-manipulation-rl-example.md","sourceDirName":"isaac-platform","slug":"/isaac-platform/robot-manipulation-rl-example","permalink":"/./docs/isaac-platform/robot-manipulation-rl-example","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-platform/robot-manipulation-rl-example.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Setup Guide for Reinforcement Learning","permalink":"/./docs/isaac-platform/isaac-sim-rl-setup-guide"},"next":{"title":"LLM Integration Guide for Robotics Applications","permalink":"/./docs/isaac-platform/llm-integration-guide"}}');var i=r(4848),o=r(7074);const s={},a="Basic RL Training Example for Robot Manipulation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Environment Setup",id:"environment-setup",level:2},{value:"1. Robot and Object Configuration",id:"1-robot-and-object-configuration",level:3},{value:"Running the Example",id:"running-the-example",level:2},{value:"1. Setup and Installation",id:"1-setup-and-installation",level:3},{value:"2. Execute Training",id:"2-execute-training",level:3},{value:"3. Monitor Training",id:"3-monitor-training",level:3},{value:"Advanced RL Techniques",id:"advanced-rl-techniques",level:2},{value:"1. Curriculum Learning",id:"1-curriculum-learning",level:3},{value:"2. Hindsight Experience Replay (HER)",id:"2-hindsight-experience-replay-her",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"1. Parallel Environments",id:"1-parallel-environments",level:3},{value:"2. GPU Acceleration",id:"2-gpu-acceleration",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues:",id:"common-issues",level:3},{value:"Verification Steps:",id:"verification-steps",level:3},{value:"Next Steps",id:"next-steps",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"basic-rl-training-example-for-robot-manipulation",children:"Basic RL Training Example for Robot Manipulation"})}),"\n",(0,i.jsx)(e.p,{children:"This document provides a practical example of reinforcement learning for robot manipulation tasks in Isaac Sim."}),"\n",(0,i.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(e.p,{children:"This example demonstrates how to train a robotic arm to perform basic manipulation tasks using reinforcement learning. The example uses the Franka Emika Panda robot as a reference, but the principles apply to other manipulator robots including humanoid arms."}),"\n",(0,i.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Completed Isaac Sim RL setup (see isaac-sim-rl-setup-guide.md)"}),"\n",(0,i.jsx)(e.li,{children:"Isaac Sim with RL Games extension enabled"}),"\n",(0,i.jsx)(e.li,{children:"Python environment with RL libraries installed"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"environment-setup",children:"Environment Setup"}),"\n",(0,i.jsx)(e.h3,{id:"1-robot-and-object-configuration",children:"1. Robot and Object Configuration"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# robot_manipulation_env.py\r\nimport numpy as np\r\nimport torch\r\nimport omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.utils.prims import get_prim_at_path\r\nfrom omni.isaac.core.utils.rotations import euler_angles_to_quat\r\nfrom omni.isaac.core.articulations import ArticulationView\r\nfrom omni.isaac.core.objects import DynamicCuboid\r\n\r\n\r\nclass RobotManipulationEnv:\r\n    def __init__(self, world: World):\r\n        self.world = world\r\n        self.robot = None\r\n        self.object = None\r\n        self.target = None\r\n\r\n        # Environment parameters\r\n        self.action_scale = 0.05  # Scale for joint position changes\r\n        self.distance_threshold = 0.05  # Distance threshold for success\r\n        self.max_episode_length = 500\r\n\r\n        # Initialize the environment\r\n        self.setup_scene()\r\n\r\n    def setup_scene(self):\r\n        """Setup the simulation scene with robot, object, and target"""\r\n        # Get assets root path\r\n        assets_root_path = get_assets_root_path()\r\n\r\n        # Add Franka robot\r\n        franka_asset_path = assets_root_path + "/Isaac/Robots/Franka/franka_instanceable.usd"\r\n        add_reference_to_stage(usd_path=franka_asset_path, prim_path="/World/Franka")\r\n\r\n        # Create target object (cube to manipulate)\r\n        self.object = self.world.scene.add(\r\n            DynamicCuboid(\r\n                prim_path="/World/Object",\r\n                name="object",\r\n                position=np.array([0.5, 0.0, 0.1]),\r\n                size=0.05,\r\n                color=np.array([1.0, 0.0, 0.0])\r\n            )\r\n        )\r\n\r\n        # Create target position\r\n        self.target = self.world.scene.add(\r\n            DynamicCuboid(\r\n                prim_path="/World/Target",\r\n                name="target",\r\n                position=np.array([0.6, 0.2, 0.1]),\r\n                size=0.05,\r\n                color=np.array([0.0, 1.0, 0.0]),\r\n                mass=0  # Static target\r\n            )\r\n        )\r\n\r\n        # Get robot view\r\n        self.robot = self.world.scene.get_object("franka")\r\n\r\n    def get_observation(self):\r\n        """Get current observation from the environment"""\r\n        # Get robot joint positions and velocities\r\n        joint_positions = self.robot.get_joint_positions()\r\n        joint_velocities = self.robot.get_joint_velocities()\r\n\r\n        # Get object position\r\n        object_position = self.object.get_world_poses()[0][0]\r\n\r\n        # Get target position\r\n        target_position = self.target.get_world_poses()[0][0]\r\n\r\n        # Get end-effector position\r\n        ee_position = self.get_end_effector_position()\r\n\r\n        # Create observation vector\r\n        observation = np.concatenate([\r\n            joint_positions,           # Joint positions\r\n            joint_velocities,          # Joint velocities\r\n            object_position,           # Object position\r\n            target_position,           # Target position\r\n            ee_position,               # End-effector position\r\n            [np.linalg.norm(object_position - ee_position)],  # Distance to object\r\n            [np.linalg.norm(object_position - target_position)]  # Distance to target\r\n        ])\r\n\r\n        return observation\r\n\r\n    def get_end_effector_position(self):\r\n        """Get the position of the robot\'s end effector"""\r\n        # Get the end effector link (typically the last link)\r\n        # This may vary depending on the robot model\r\n        ee_link_path = "/World/Franka/panda_hand"\r\n        ee_prim = get_prim_at_path(ee_link_path)\r\n        if ee_prim:\r\n            # Get the world position of the end effector\r\n            from pxr import Gf\r\n            pos_attr = ee_prim.GetAttribute("xformOp:translate")\r\n            if pos_attr:\r\n                return np.array(pos_attr.Get())\r\n\r\n        # Fallback: use robot\'s root position + offset\r\n        robot_pos, _ = self.robot.get_world_poses()\r\n        return robot_pos[0] + np.array([0.5, 0.0, 0.5])\r\n\r\n    def apply_action(self, action):\r\n        """Apply action to the robot"""\r\n        # Rescale action\r\n        scaled_action = action * self.action_scale\r\n\r\n        # Get current joint positions\r\n        current_positions = self.robot.get_joint_positions()\r\n\r\n        # Calculate new joint positions\r\n        new_positions = current_positions + scaled_action\r\n\r\n        # Apply new joint positions\r\n        self.robot.set_joint_positions(new_positions)\r\n\r\n    def compute_reward(self, action):\r\n        """Compute reward based on current state"""\r\n        # Get positions\r\n        object_position = self.object.get_world_poses()[0][0]\r\n        target_position = self.target.get_world_poses()[0][0]\r\n        ee_position = self.get_end_effector_position()\r\n\r\n        # Distance from end-effector to object (for grasping)\r\n        dist_ee_to_obj = np.linalg.norm(object_position - ee_position)\r\n\r\n        # Distance from object to target (for manipulation)\r\n        dist_obj_to_target = np.linalg.norm(object_position - target_position)\r\n\r\n        # Reward components\r\n        grasp_reward = -dist_ee_to_obj  # Encourage approaching object\r\n        manipulation_reward = -dist_obj_to_target  # Encourage moving object to target\r\n        action_penalty = -np.sum(np.abs(action)) * 0.01  # Penalty for large actions\r\n\r\n        # Success bonus if object is close to target\r\n        success_bonus = 100.0 if dist_obj_to_target < self.distance_threshold else 0.0\r\n\r\n        # Total reward\r\n        total_reward = grasp_reward * 0.1 + manipulation_reward + action_penalty + success_bonus\r\n\r\n        return total_reward\r\n\r\n    def is_done(self):\r\n        """Check if episode is done"""\r\n        current_step = self.world.current_time_step_index\r\n\r\n        # Get positions\r\n        object_position = self.object.get_world_poses()[0][0]\r\n        target_position = self.target.get_world_poses()[0][0]\r\n        dist_obj_to_target = np.linalg.norm(object_position - target_position)\r\n\r\n        # Check if episode is done\r\n        done = (\r\n            current_step >= self.max_episode_length or\r\n            dist_obj_to_target < self.distance_threshold\r\n        )\r\n\r\n        return done, dist_obj_to_target < self.distance_threshold\r\n\r\n    def reset(self):\r\n        """Reset the environment"""\r\n        # Reset object position\r\n        new_obj_pos = np.array([0.5, 0.0, 0.1]) + np.random.uniform(-0.1, 0.1, 3)\r\n        self.object.set_world_poses(positions=np.array([new_obj_pos]))\r\n\r\n        # Reset robot joints to initial positions\r\n        initial_joint_positions = np.array([0.0, -1.0, 0.0, -2.2, 0.0, 1.2, 0.0])\r\n        self.robot.set_joint_positions(initial_joint_positions)\r\n\r\n        return self.get_observation()\r\n\r\n\r\n# Training script using Stable Baselines3\r\ndef train_robot_manipulation():\r\n    """Train a robot to manipulate objects using RL"""\r\n    from stable_baselines3 import PPO\r\n    from stable_baselines3.common.env_util import make_vec_env\r\n    from stable_baselines3.common.callbacks import EvalCallback\r\n    import gymnasium as gym\r\n    from gymnasium import spaces\r\n\r\n    # Custom gym environment wrapper\r\n    class IsaacGymEnv(gym.Env):\r\n        def __init__(self, world):\r\n            self.env = RobotManipulationEnv(world)\r\n\r\n            # Define action and observation spaces\r\n            num_joints = 7  # Franka has 7 joints\r\n            self.action_space = spaces.Box(\r\n                low=-1.0, high=1.0, shape=(num_joints,), dtype=np.float32\r\n            )\r\n            self.observation_space = spaces.Box(\r\n                low=-np.inf, high=np.inf,\r\n                shape=(num_joints * 2 + 3 + 3 + 3 + 2,),  # pos + vel + obj_pos + target_pos + ee_pos + distances\r\n                dtype=np.float32\r\n            )\r\n\r\n        def reset(self, seed=None, options=None):\r\n            super().reset(seed=seed)\r\n            obs = self.env.reset()\r\n            return obs.astype(np.float32), {}\r\n\r\n        def step(self, action):\r\n            self.env.apply_action(action)\r\n\r\n            # Step the simulation\r\n            self.env.world.step(render=True)\r\n\r\n            obs = self.env.get_observation()\r\n            reward = self.env.compute_reward(action)\r\n            done, success = self.env.is_done()\r\n\r\n            info = {"is_success": success}\r\n\r\n            return obs.astype(np.float32), reward, done, False, info\r\n\r\n    # Launch Isaac Sim\r\n    config = {"headless": True}\r\n    simulation_app = omni.simulation.SimulationApp(config)\r\n\r\n    # Create world\r\n    world = World(stage_units_in_meters=1.0)\r\n\r\n    # Create environment\r\n    isaac_env = IsaacGymEnv(world)\r\n\r\n    # Create vectorized environment\r\n    vec_env = make_vec_env(lambda: isaac_env, n_envs=1)\r\n\r\n    # Create PPO model\r\n    model = PPO(\r\n        "MlpPolicy",\r\n        vec_env,\r\n        verbose=1,\r\n        tensorboard_log="./tb_logs/",\r\n        learning_rate=3e-4,\r\n        n_steps=2048,\r\n        batch_size=64,\r\n        n_epochs=10,\r\n        gamma=0.99,\r\n        gae_lambda=0.95,\r\n        clip_range=0.2,\r\n        ent_coef=0.01\r\n    )\r\n\r\n    # Train the model\r\n    model.learn(total_timesteps=100000)\r\n\r\n    # Save the model\r\n    model.save("robot_manipulation_model")\r\n\r\n    # Close simulation\r\n    simulation_app.close()\r\n\r\n    print("Training completed and model saved!")\r\n\r\n\r\nif __name__ == "__main__":\r\n    train_robot_manipulation()\n'})}),"\n",(0,i.jsx)(e.h2,{id:"running-the-example",children:"Running the Example"}),"\n",(0,i.jsx)(e.h3,{id:"1-setup-and-installation",children:"1. Setup and Installation"}),"\n",(0,i.jsx)(e.p,{children:"First, ensure all dependencies are installed:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"# Activate Isaac Sim environment\r\ncd ~/isaac-sim\r\nsource setup_conda_env.sh\r\n\r\n# Install required packages\r\npip install stable-baselines3[extra]\r\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\r\npip install gymnasium[box2d]\r\npip install wandb\n"})}),"\n",(0,i.jsx)(e.h3,{id:"2-execute-training",children:"2. Execute Training"}),"\n",(0,i.jsxs)(e.p,{children:["Save the example code as ",(0,i.jsx)(e.code,{children:"robot_manipulation_rl.py"})," and run:"]}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"cd ~/isaac-sim\r\nsource setup_conda_env.sh\r\npython robot_manipulation_rl.py\n"})}),"\n",(0,i.jsx)(e.h3,{id:"3-monitor-training",children:"3. Monitor Training"}),"\n",(0,i.jsx)(e.p,{children:"Training progress can be monitored using TensorBoard:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-bash",children:"tensorboard --logdir tb_logs/\n"})}),"\n",(0,i.jsx)(e.h2,{id:"advanced-rl-techniques",children:"Advanced RL Techniques"}),"\n",(0,i.jsx)(e.h3,{id:"1-curriculum-learning",children:"1. Curriculum Learning"}),"\n",(0,i.jsx)(e.p,{children:"Implement progressive difficulty increase:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class CurriculumLearning:\r\n    def __init__(self):\r\n        self.current_level = 0\r\n        self.level_thresholds = [0.1, 0.05, 0.02]  # Distance thresholds for each level\r\n\r\n    def update_curriculum(self, success_rate):\r\n        if success_rate > 0.8 and self.current_level < len(self.level_thresholds) - 1:\r\n            self.current_level += 1\r\n            print(f"Curriculum advanced to level {self.current_level + 1}")\r\n\r\n    def get_current_threshold(self):\r\n        return self.level_thresholds[self.current_level]\n'})}),"\n",(0,i.jsx)(e.h3,{id:"2-hindsight-experience-replay-her",children:"2. Hindsight Experience Replay (HER)"}),"\n",(0,i.jsx)(e.p,{children:"For goal-conditioned tasks:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Example HER implementation concept\r\ndef hindsight_experience_replay(episode_transitions, reward_function):\r\n    \"\"\"\r\n    Convert failed episodes to successful ones by changing the goal\r\n    \"\"\"\r\n    her_transitions = []\r\n\r\n    for transition in episode_transitions:\r\n        # Original transition\r\n        her_transitions.append(transition)\r\n\r\n        # Future state as goal (for HER)\r\n        future_state = transition['next_state']\r\n        her_reward = reward_function(future_state, future_state)  # Success reward\r\n        her_transition = {\r\n            'state': transition['state'],\r\n            'action': transition['action'],\r\n            'reward': her_reward,\r\n            'next_state': transition['next_state'],\r\n            'goal': future_state\r\n        }\r\n        her_transitions.append(her_transition)\r\n\r\n    return her_transitions\n"})}),"\n",(0,i.jsx)(e.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsx)(e.h3,{id:"1-parallel-environments",children:"1. Parallel Environments"}),"\n",(0,i.jsx)(e.p,{children:"For faster training, use multiple parallel environments:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Configuration for parallel environments\r\nrl_config = {\r\n    "env": {\r\n        "numEnvs": 256,  # Number of parallel environments\r\n        "envSpacing": 5.0,\r\n        "episodeLength": 1000,\r\n        "enableDebugVis": False,\r\n    },\r\n    "sim": {\r\n        "dt": 1.0 / 60.0,\r\n        "use_gpu_pipeline": True,\r\n        "gravity": [0.0, 0.0, -9.81],\r\n    }\r\n}\n'})}),"\n",(0,i.jsx)(e.h3,{id:"2-gpu-acceleration",children:"2. GPU Acceleration"}),"\n",(0,i.jsx)(e.p,{children:"Enable GPU physics for better performance:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Enable GPU physics simulation\r\nfrom omni.isaac.core import World\r\n\r\nworld = World(\r\n    stage_units_in_meters=1.0,\r\n    rendering_dt=1.0/60.0,\r\n    sim_params={\r\n        "use_gpu_pipeline": True,\r\n        "dt": 1.0/60.0,\r\n        "substeps": 1,\r\n        "gravity": [0.0, 0.0, -9.81]\r\n    }\r\n)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(e.h3,{id:"common-issues",children:"Common Issues:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Slow Training"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Increase number of parallel environments"}),"\n",(0,i.jsx)(e.li,{children:"Optimize reward function computation"}),"\n",(0,i.jsx)(e.li,{children:"Use GPU acceleration for physics"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Unstable Training"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Reduce learning rate"}),"\n",(0,i.jsx)(e.li,{children:"Adjust reward scaling"}),"\n",(0,i.jsx)(e.li,{children:"Implement reward clipping"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Robot Instability"}),":"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Tune joint stiffness and damping"}),"\n",(0,i.jsx)(e.li,{children:"Reduce action magnitude"}),"\n",(0,i.jsx)(e.li,{children:"Adjust physics parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"verification-steps",children:"Verification Steps:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Environment initializes without errors"}),"\n",(0,i.jsx)(e.li,{children:"Robot responds to actions correctly"}),"\n",(0,i.jsx)(e.li,{children:"Reward function provides meaningful feedback"}),"\n",(0,i.jsx)(e.li,{children:"Training shows improvement over time"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(e.p,{children:"After implementing this basic manipulation example:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Extend to more complex manipulation tasks"}),"\n",(0,i.jsx)(e.li,{children:"Add multiple objects and obstacles"}),"\n",(0,i.jsx)(e.li,{children:"Implement more sophisticated reward shaping"}),"\n",(0,i.jsx)(e.li,{children:"Try different RL algorithms (SAC, TD3, etc.)"}),"\n",(0,i.jsx)(e.li,{children:"Transfer learned policies to real robots\r\n``parameter>"}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},7074:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);