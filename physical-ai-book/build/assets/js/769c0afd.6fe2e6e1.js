"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[315],{7074:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var t=r(6540);const a={},s=t.createContext(a);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),t.createElement(s.Provider,{value:n},e.children)}},7884:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"isaac-platform/llm-integration-guide","title":"LLM Integration Guide for Robotics Applications","description":"This document provides instructions for integrating Large Language Models (LLMs) with robotics applications using API-based approaches.","source":"@site/docs/isaac-platform/llm-integration-guide.md","sourceDirName":"isaac-platform","slug":"/isaac-platform/llm-integration-guide","permalink":"/docs/isaac-platform/llm-integration-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-platform/llm-integration-guide.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Basic RL Training Example for Robot Manipulation","permalink":"/docs/isaac-platform/robot-manipulation-rl-example"},"next":{"title":"Voice Command Processing System","permalink":"/docs/isaac-platform/voice-command-processing-system"}}');var a=r(4848),s=r(7074);const i={},o="LLM Integration Guide for Robotics Applications",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Architecture Patterns",id:"architecture-patterns",level:2},{value:"1. Cloud-Based API Integration",id:"1-cloud-based-api-integration",level:3},{value:"2. Hybrid Approach",id:"2-hybrid-approach",level:3},{value:"3. Edge Deployment (Advanced)",id:"3-edge-deployment-advanced",level:3},{value:"API-Based Integration Approaches",id:"api-based-integration-approaches",level:2},{value:"1. OpenAI GPT Integration",id:"1-openai-gpt-integration",level:3},{value:"Setup and Configuration",id:"setup-and-configuration",level:4},{value:"Example Usage",id:"example-usage",level:4},{value:"2. Anthropic Claude Integration",id:"2-anthropic-claude-integration",level:3},{value:"Setup and Configuration",id:"setup-and-configuration-1",level:4},{value:"3. Google Gemini Integration",id:"3-google-gemini-integration",level:3},{value:"Setup and Configuration",id:"setup-and-configuration-2",level:4},{value:"Implementation Examples",id:"implementation-examples",level:2},{value:"1. Natural Language Command Processing",id:"1-natural-language-command-processing",level:3},{value:"2. Task Planning and Execution",id:"2-task-planning-and-execution",level:3},{value:"Safety and Security Considerations",id:"safety-and-security-considerations",level:2},{value:"1. Input Validation",id:"1-input-validation",level:3},{value:"2. Access Control",id:"2-access-control",level:3},{value:"3. Fail-Safe Mechanisms",id:"3-fail-safe-mechanisms",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Caching Strategies",id:"1-caching-strategies",level:3},{value:"2. Asynchronous Processing",id:"2-asynchronous-processing",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Error Handling",id:"1-error-handling",level:3},{value:"2. Cost Management",id:"2-cost-management",level:3},{value:"3. Testing and Validation",id:"3-testing-and-validation",level:3},{value:"Resources and Further Reading",id:"resources-and-further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"llm-integration-guide-for-robotics-applications",children:"LLM Integration Guide for Robotics Applications"})}),"\n",(0,a.jsx)(n.p,{children:"This document provides instructions for integrating Large Language Models (LLMs) with robotics applications using API-based approaches."}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"Large Language Models can enhance robotics applications by providing natural language understanding, high-level task planning, and human-robot interaction capabilities. This guide covers API-based integration approaches that are suitable for robotics applications."}),"\n",(0,a.jsx)(n.h2,{id:"architecture-patterns",children:"Architecture Patterns"}),"\n",(0,a.jsx)(n.h3,{id:"1-cloud-based-api-integration",children:"1. Cloud-Based API Integration"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"LLMs accessed via cloud APIs (OpenAI, Anthropic, Google, etc.)"}),"\n",(0,a.jsx)(n.li,{children:"Suitable for applications with reliable internet connectivity"}),"\n",(0,a.jsx)(n.li,{children:"Lower computational requirements on robot side"}),"\n",(0,a.jsx)(n.li,{children:"Real-time processing with potential latency considerations"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-hybrid-approach",children:"2. Hybrid Approach"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Combination of local lightweight models and cloud LLMs"}),"\n",(0,a.jsx)(n.li,{children:"Critical functions handled locally, complex reasoning in cloud"}),"\n",(0,a.jsx)(n.li,{children:"Balances performance and capability requirements"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-edge-deployment-advanced",children:"3. Edge Deployment (Advanced)"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"LLMs deployed on edge devices for offline capability"}),"\n",(0,a.jsx)(n.li,{children:"Requires powerful edge hardware (NVIDIA Jetson, etc.)"}),"\n",(0,a.jsx)(n.li,{children:"Lower latency but higher resource requirements"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"api-based-integration-approaches",children:"API-Based Integration Approaches"}),"\n",(0,a.jsx)(n.h3,{id:"1-openai-gpt-integration",children:"1. OpenAI GPT Integration"}),"\n",(0,a.jsx)(n.h4,{id:"setup-and-configuration",children:"Setup and Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\r\nimport asyncio\r\nfrom typing import Dict, List, Optional\r\n\r\nclass OpenAIIntegrator:\r\n    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):\r\n        self.api_key = api_key\r\n        self.model = model\r\n        openai.api_key = api_key\r\n\r\n    async def generate_robot_command(self, user_request: str, robot_state: Dict) -> Dict:\r\n        """\r\n        Generate robot commands based on natural language request\r\n        """\r\n        system_prompt = f"""\r\n        You are a robotics command interpreter. Convert natural language requests into robot commands.\r\n        Current robot state: {robot_state}\r\n        Available actions: move_to, pick_up, place, speak, navigate, grasp, release\r\n        Return commands in JSON format with action, parameters, and confidence score.\r\n        """\r\n\r\n        response = await openai.ChatCompletion.acreate(\r\n            model=self.model,\r\n            messages=[\r\n                {"role": "system", "content": system_prompt},\r\n                {"role": "user", "content": user_request}\r\n            ],\r\n            temperature=0.3,\r\n            max_tokens=200\r\n        )\r\n\r\n        return self._parse_response(response.choices[0].message.content)\r\n\r\n    def _parse_response(self, response: str) -> Dict:\r\n        """\r\n        Parse the LLM response and extract structured robot commands\r\n        """\r\n        # Implementation to parse JSON response\r\n        import json\r\n        try:\r\n            return json.loads(response)\r\n        except json.JSONDecodeError:\r\n            # Fallback parsing\r\n            return {"action": "unknown", "confidence": 0.0}\n'})}),"\n",(0,a.jsx)(n.h4,{id:"example-usage",children:"Example Usage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Initialize the integrator\r\nllm_integrator = OpenAIIntegrator(api_key="your-api-key")\r\n\r\n# Example robot state\r\nrobot_state = {\r\n    "location": "kitchen",\r\n    "battery_level": 85,\r\n    "gripper_status": "open",\r\n    "current_task": "idle",\r\n    "objects_detected": ["mug", "table", "refrigerator"]\r\n}\r\n\r\n# Process natural language command\r\nuser_command = "Please bring me a coffee from the kitchen counter"\r\ncommand = await llm_integrator.generate_robot_command(user_command, robot_state)\r\nprint(f"Generated command: {command}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-anthropic-claude-integration",children:"2. Anthropic Claude Integration"}),"\n",(0,a.jsx)(n.h4,{id:"setup-and-configuration-1",children:"Setup and Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import anthropic\r\nfrom typing import Dict, List\r\n\r\nclass ClaudeIntegrator:\r\n    def __init__(self, api_key: str, model: str = "claude-3-opus-20240229"):\r\n        self.client = anthropic.Anthropic(api_key=api_key)\r\n        self.model = model\r\n\r\n    def generate_robot_behavior(self, user_request: str, environment_context: Dict) -> Dict:\r\n        """\r\n        Generate detailed robot behavior based on natural language request\r\n        """\r\n        prompt = f"""\r\n        Human: {user_request}\r\n\r\n        Robot Environment Context:\r\n        - Location: {environment_context.get(\'location\', \'unknown\')}\r\n        - Available objects: {environment_context.get(\'objects\', [])}\r\n        - Robot capabilities: {environment_context.get(\'capabilities\', [])}\r\n        - Current state: {environment_context.get(\'state\', {})}\r\n\r\n        Please provide:\r\n        1. High-level plan\r\n        2. Specific actions to execute\r\n        3. Expected outcomes\r\n        4. Potential challenges and solutions\r\n\r\n        Respond in structured JSON format.\r\n        """\r\n\r\n        response = self.client.messages.create(\r\n            model=self.model,\r\n            max_tokens=1000,\r\n            temperature=0.3,\r\n            system="You are a robotics planning assistant. Generate detailed plans for robot behavior based on natural language requests.",\r\n            messages=[\r\n                {"role": "user", "content": prompt}\r\n            ]\r\n        )\r\n\r\n        return self._extract_structured_response(response.content)\r\n\r\n    def _extract_structured_response(self, content) -> Dict:\r\n        """\r\n        Extract structured response from Claude\'s content\r\n        """\r\n        # Implementation to extract structured data\r\n        text_content = ""\r\n        for block in content:\r\n            if block.type == "text":\r\n                text_content += block.text\r\n\r\n        # Parse the text content for JSON structure\r\n        import json\r\n        import re\r\n\r\n        # Extract JSON from the response\r\n        json_match = re.search(r\'\\{.*\\}\', text_content, re.DOTALL)\r\n        if json_match:\r\n            try:\r\n                return json.loads(json_match.group())\r\n            except json.JSONDecodeError:\r\n                pass\r\n\r\n        return {"plan": text_content, "actions": [], "confidence": 0.5}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-google-gemini-integration",children:"3. Google Gemini Integration"}),"\n",(0,a.jsx)(n.h4,{id:"setup-and-configuration-2",children:"Setup and Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import google.generativeai as genai\r\nfrom typing import Dict, List\r\n\r\nclass GeminiIntegrator:\r\n    def __init__(self, api_key: str, model: str = "gemini-pro"):\r\n        genai.configure(api_key=api_key)\r\n        self.model = genai.GenerativeModel(model)\r\n\r\n    async def generate_task_plan(self, goal: str, constraints: List[str]) -> Dict:\r\n        """\r\n        Generate task plan for robot based on goal and constraints\r\n        """\r\n        prompt = f"""\r\n        Goal: {goal}\r\n        Constraints: {constraints}\r\n\r\n        Generate a step-by-step task plan for a robot to achieve the goal.\r\n        Consider:\r\n        1. Environmental constraints\r\n        2. Robot capabilities\r\n        3. Safety requirements\r\n        4. Efficiency optimization\r\n\r\n        Return in JSON format with steps, estimated time, and confidence scores.\r\n        """\r\n\r\n        response = await self.model.generate_content_async(prompt)\r\n        return self._parse_task_plan(response.text)\r\n\r\n    def _parse_task_plan(self, response: str) -> Dict:\r\n        """\r\n        Parse the task plan response\r\n        """\r\n        import json\r\n        import re\r\n\r\n        # Extract JSON from response\r\n        json_pattern = r\'\\{[^{}]*\\}\'  # Simple pattern, can be enhanced\r\n        matches = re.findall(json_pattern, response)\r\n\r\n        if matches:\r\n            try:\r\n                return json.loads(matches[-1])  # Use the last JSON block\r\n            except json.JSONDecodeError:\r\n                pass\r\n\r\n        return {"steps": [response], "estimated_time": "unknown", "confidence": 0.5}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"implementation-examples",children:"Implementation Examples"}),"\n",(0,a.jsx)(n.h3,{id:"1-natural-language-command-processing",children:"1. Natural Language Command Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# robot_llm_interface.py\r\nimport asyncio\r\nfrom typing import Dict, List, Any\r\nimport logging\r\n\r\nclass RobotLLMInterface:\r\n    """\r\n    Main interface for LLM integration with robot systems\r\n    """\r\n    def __init__(self, llm_integrator):\r\n        self.llm_integrator = llm_integrator\r\n        self.logger = logging.getLogger(__name__)\r\n\r\n    async def process_command(self, user_input: str, robot_context: Dict) -> Dict:\r\n        """\r\n        Process natural language command and generate robot actions\r\n        """\r\n        try:\r\n            # Generate command from LLM\r\n            llm_response = await self.llm_integrator.generate_robot_command(\r\n                user_input, robot_context\r\n            )\r\n\r\n            # Validate and sanitize response\r\n            validated_command = self._validate_command(llm_response, robot_context)\r\n\r\n            # Log the interaction\r\n            self.logger.info(f"Processed command: {user_input} -> {validated_command}")\r\n\r\n            return validated_command\r\n\r\n        except Exception as e:\r\n            self.logger.error(f"Error processing command: {e}")\r\n            return {\r\n                "action": "error",\r\n                "message": f"Failed to process command: {str(e)}",\r\n                "confidence": 0.0\r\n            }\r\n\r\n    def _validate_command(self, command: Dict, context: Dict) -> Dict:\r\n        """\r\n        Validate and sanitize LLM-generated commands\r\n        """\r\n        # Check if action is in allowed actions\r\n        allowed_actions = ["move_to", "pick_up", "place", "speak", "navigate", "grasp", "release"]\r\n\r\n        if command.get("action") not in allowed_actions:\r\n            return {\r\n                "action": "error",\r\n                "message": f"Invalid action: {command.get(\'action\')}",\r\n                "confidence": 0.0\r\n            }\r\n\r\n        # Validate parameters based on action\r\n        if command["action"] in ["move_to", "navigate"]:\r\n            if "location" not in command.get("parameters", {}):\r\n                return {\r\n                    "action": "error",\r\n                    "message": "Missing location parameter for navigation",\r\n                    "confidence": 0.0\r\n                }\r\n\r\n        # Add safety checks\r\n        command["safety_verified"] = True\r\n\r\n        return command\r\n\r\n    async def generate_conversation_response(self, user_input: str, conversation_history: List[Dict]) -> str:\r\n        """\r\n        Generate natural language response for human-robot interaction\r\n        """\r\n        context = f"""\r\n        Conversation history: {conversation_history}\r\n        User input: {user_input}\r\n\r\n        Respond as a helpful robot assistant. Keep responses concise and relevant to the robot\'s capabilities.\r\n        """\r\n\r\n        try:\r\n            response = await self.llm_integrator.generate_response(context)\r\n            return response\r\n        except Exception as e:\r\n            self.logger.error(f"Error generating conversation response: {e}")\r\n            return "I\'m sorry, I couldn\'t process that request."\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-task-planning-and-execution",children:"2. Task Planning and Execution"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# task_planner.py\r\nfrom typing import Dict, List, Optional\r\nimport asyncio\r\nimport time\r\n\r\nclass RobotTaskPlanner:\r\n    """\r\n    Task planning system using LLM for high-level reasoning\r\n    """\r\n    def __init__(self, llm_integrator):\r\n        self.llm_integrator = llm_integrator\r\n        self.current_task = None\r\n        self.task_history = []\r\n\r\n    async def plan_task(self, goal: str, environment_state: Dict) -> List[Dict]:\r\n        """\r\n        Generate a task plan based on goal and environment state\r\n        """\r\n        plan_prompt = f"""\r\n        Goal: {goal}\r\n        Environment state: {environment_state}\r\n\r\n        Generate a detailed task plan with:\r\n        1. Sequential steps\r\n        2. Pre-conditions for each step\r\n        3. Expected outcomes\r\n        4. Error handling procedures\r\n        5. Alternative strategies\r\n\r\n        Return as ordered list of steps in JSON format.\r\n        """\r\n\r\n        plan = await self.llm_integrator.generate_task_plan(plan_prompt)\r\n        return plan.get("steps", [])\r\n\r\n    async def execute_task_with_llm_guidance(self, goal: str, robot_interface) -> Dict:\r\n        """\r\n        Execute a task with LLM providing guidance and adaptation\r\n        """\r\n        # Generate initial plan\r\n        environment_state = await robot_interface.get_environment_state()\r\n        task_plan = await self.plan_task(goal, environment_state)\r\n\r\n        results = {\r\n            "goal": goal,\r\n            "plan": task_plan,\r\n            "executed_steps": [],\r\n            "success": False,\r\n            "reasoning_log": []\r\n        }\r\n\r\n        for i, step in enumerate(task_plan):\r\n            self.current_task = step\r\n\r\n            # Execute step\r\n            step_result = await self._execute_step(step, robot_interface)\r\n            results["executed_steps"].append(step_result)\r\n\r\n            # Log reasoning\r\n            reasoning = f"Step {i+1}: {step[\'description\']}, Result: {step_result[\'status\']}"\r\n            results["reasoning_log"].append(reasoning)\r\n\r\n            if not step_result["success"]:\r\n                # Ask LLM for alternative approach\r\n                adaptation_prompt = f"""\r\n                Task step failed: {step}\r\n                Failure reason: {step_result.get(\'error\', \'Unknown\')}\r\n                Current environment: {await robot_interface.get_environment_state()}\r\n\r\n                Suggest alternative approach or recovery strategy.\r\n                """\r\n\r\n                alternative = await self.llm_integrator.generate_response(adaptation_prompt)\r\n                results["reasoning_log"].append(f"Adaptation: {alternative}")\r\n\r\n                # Try alternative approach\r\n                if alternative:\r\n                    alt_result = await self._execute_step(alternative, robot_interface)\r\n                    results["executed_steps"].append(alt_result)\r\n\r\n        # Determine overall success\r\n        results["success"] = all(step["success"] for step in results["executed_steps"])\r\n\r\n        return results\r\n\r\n    async def _execute_step(self, step: Dict, robot_interface) -> Dict:\r\n        """\r\n        Execute a single task step\r\n        """\r\n        try:\r\n            # Execute the robot action\r\n            result = await robot_interface.execute_action(step["action"], step.get("parameters", {}))\r\n\r\n            return {\r\n                "step": step,\r\n                "success": result.get("success", False),\r\n                "result": result,\r\n                "timestamp": time.time()\r\n            }\r\n        except Exception as e:\r\n            return {\r\n                "step": step,\r\n                "success": False,\r\n                "error": str(e),\r\n                "timestamp": time.time()\r\n            }\n'})}),"\n",(0,a.jsx)(n.h2,{id:"safety-and-security-considerations",children:"Safety and Security Considerations"}),"\n",(0,a.jsx)(n.h3,{id:"1-input-validation",children:"1. Input Validation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Sanitize all user inputs before sending to LLM"}),"\n",(0,a.jsx)(n.li,{children:"Implement content filtering to prevent harmful instructions"}),"\n",(0,a.jsx)(n.li,{children:"Validate LLM outputs before execution"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-access-control",children:"2. Access Control"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use secure API keys with limited permissions"}),"\n",(0,a.jsx)(n.li,{children:"Implement rate limiting to prevent abuse"}),"\n",(0,a.jsx)(n.li,{children:"Log all API interactions for monitoring"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-fail-safe-mechanisms",children:"3. Fail-Safe Mechanisms"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement timeout mechanisms for LLM calls"}),"\n",(0,a.jsx)(n.li,{children:"Provide fallback behaviors when LLM is unavailable"}),"\n",(0,a.jsx)(n.li,{children:"Ensure robot safety during LLM processing delays"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"1-caching-strategies",children:"1. Caching Strategies"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import functools\r\nimport time\r\nfrom typing import Any\r\n\r\nclass CachedLLMIntegrator:\r\n    def __init__(self, llm_integrator, cache_ttl: int = 300):  # 5 minutes\r\n        self.llm_integrator = llm_integrator\r\n        self.cache_ttl = cache_ttl\r\n        self.cache = {}\r\n\r\n    def _get_cache_key(self, *args, **kwargs):\r\n        import hashlib\r\n        import json\r\n        cache_input = json.dumps((args, kwargs), sort_keys=True)\r\n        return hashlib.md5(cache_input.encode()).hexdigest()\r\n\r\n    async def generate_with_cache(self, prompt: str, context: Dict[str, Any] = None) -> Any:\r\n        cache_key = self._get_cache_key(prompt, context)\r\n\r\n        # Check cache\r\n        if cache_key in self.cache:\r\n            cached_result, timestamp = self.cache[cache_key]\r\n            if time.time() - timestamp < self.cache_ttl:\r\n                return cached_result\r\n\r\n        # Generate new result\r\n        result = await self.llm_integrator.generate_response(prompt)\r\n\r\n        # Store in cache\r\n        self.cache[cache_key] = (result, time.time())\r\n\r\n        return result\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-asynchronous-processing",children:"2. Asynchronous Processing"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use async/await for non-blocking LLM calls"}),"\n",(0,a.jsx)(n.li,{children:"Implement request queuing for high-volume scenarios"}),"\n",(0,a.jsx)(n.li,{children:"Batch similar requests when possible"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# llm_ros_integration.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Pose\r\nimport asyncio\r\nimport threading\r\n\r\nclass LLMROSInterface(Node):\r\n    """\r\n    ROS 2 interface for LLM integration\r\n    """\r\n    def __init__(self, llm_integrator):\r\n        super().__init__(\'llm_interface\')\r\n        self.llm_integrator = llm_integrator\r\n\r\n        # Publishers and subscribers\r\n        self.command_publisher = self.create_publisher(String, \'robot_commands\', 10)\r\n        self.nlp_subscriber = self.create_subscription(\r\n            String, \'natural_language_commands\', self.nlp_callback, 10\r\n        )\r\n        self.state_subscriber = self.create_subscription(\r\n            String, \'robot_state\', self.state_callback, 10\r\n        )\r\n\r\n        self.current_state = {}\r\n        self.command_queue = asyncio.Queue()\r\n\r\n        # Start async processing\r\n        self.processing_thread = threading.Thread(target=self._start_async_processing)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n\r\n    def nlp_callback(self, msg):\r\n        """\r\n        Handle natural language commands\r\n        """\r\n        command_future = asyncio.run_coroutine_threadsafe(\r\n            self.process_natural_language_command(msg.data),\r\n            self.loop\r\n        )\r\n        command_future.add_done_callback(self._publish_robot_command)\r\n\r\n    def state_callback(self, msg):\r\n        """\r\n        Update robot state from ROS messages\r\n        """\r\n        import json\r\n        try:\r\n            self.current_state = json.loads(msg.data)\r\n        except json.JSONDecodeError:\r\n            self.get_logger().error(f"Invalid state message: {msg.data}")\r\n\r\n    async def process_natural_language_command(self, command: str):\r\n        """\r\n        Process natural language command using LLM\r\n        """\r\n        llm_response = await self.llm_integrator.generate_robot_command(\r\n            command, self.current_state\r\n        )\r\n        return llm_response\r\n\r\n    def _publish_robot_command(self, future):\r\n        """\r\n        Publish processed robot command\r\n        """\r\n        try:\r\n            command = future.result()\r\n            msg = String()\r\n            msg.data = str(command)\r\n            self.command_publisher.publish(msg)\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing command: {e}")\r\n\r\n    def _start_async_processing(self):\r\n        """\r\n        Start the asyncio event loop in a separate thread\r\n        """\r\n        self.loop = asyncio.new_event_loop()\r\n        asyncio.set_event_loop(self.loop)\r\n        self.loop.run_forever()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"1-error-handling",children:"1. Error Handling"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement retry mechanisms for API failures"}),"\n",(0,a.jsx)(n.li,{children:"Provide graceful degradation when LLM is unavailable"}),"\n",(0,a.jsx)(n.li,{children:"Log all interactions for debugging and monitoring"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-cost-management",children:"2. Cost Management"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Monitor API usage and costs"}),"\n",(0,a.jsx)(n.li,{children:"Implement caching for repeated requests"}),"\n",(0,a.jsx)(n.li,{children:"Use appropriate model sizes for the task complexity"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"3-testing-and-validation",children:"3. Testing and Validation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Test with various natural language inputs"}),"\n",(0,a.jsx)(n.li,{children:"Validate LLM outputs before robot execution"}),"\n",(0,a.jsx)(n.li,{children:"Implement safety checks and bounds verification"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"resources-and-further-reading",children:"Resources and Further Reading"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.anthropic.com/",children:"Anthropic API Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://ai.google.dev/",children:"Google AI Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://docs.ros.org/",children:"ROS 2 Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://research.nvidia.com/publication/llm-robotics-applications",children:"Robot Operating System 2 (ROS 2) and LLM Integration Patterns"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);