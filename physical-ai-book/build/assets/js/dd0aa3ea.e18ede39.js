"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6840],{7074:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var o=r(6540);const i={},t=o.createContext(i);function s(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(t.Provider,{value:n},e.children)}},8220:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"isaac-platform/voice-command-processing-system","title":"Voice Command Processing System","description":"This document provides a comprehensive guide for implementing a voice command processing system for robotics applications.","source":"@site/docs/isaac-platform/voice-command-processing-system.md","sourceDirName":"isaac-platform","slug":"/isaac-platform/voice-command-processing-system","permalink":"/docs/isaac-platform/voice-command-processing-system","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-platform/voice-command-processing-system.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"LLM Integration Guide for Robotics Applications","permalink":"/docs/isaac-platform/llm-integration-guide"},"next":{"title":"Sim-to-Real Transfer Guide for Jetson Deployment","permalink":"/docs/isaac-platform/sim-to-real-transfer-jetson-guide"}}');var i=r(4848),t=r(7074);const s={},a="Voice Command Processing System",c={},l=[{value:"Overview",id:"overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"1. High-Level Architecture",id:"1-high-level-architecture",level:3},{value:"2. Component Breakdown",id:"2-component-breakdown",level:3},{value:"Implementation Components",id:"implementation-components",level:2},{value:"1. Audio Input and Preprocessing",id:"1-audio-input-and-preprocessing",level:3},{value:"2. Speech Recognition Module",id:"2-speech-recognition-module",level:3},{value:"3. Natural Language Processing and Intent Recognition",id:"3-natural-language-processing-and-intent-recognition",level:3},{value:"4. Voice Command Processing System",id:"4-voice-command-processing-system",level:3},{value:"5. ROS 2 Integration",id:"5-ros-2-integration",level:3},{value:"6. Example Usage and Integration",id:"6-example-usage-and-integration",level:3},{value:"Installation Requirements",id:"installation-requirements",level:2},{value:"Python Dependencies",id:"python-dependencies",level:3},{value:"For Local Whisper Support (optional)",id:"for-local-whisper-support-optional",level:3},{value:"For ROS 2 Integration",id:"for-ros-2-integration",level:3},{value:"Configuration and Tuning",id:"configuration-and-tuning",level:2},{value:"1. Audio Configuration",id:"1-audio-configuration",level:3},{value:"2. Recognition Accuracy",id:"2-recognition-accuracy",level:3},{value:"3. Intent Recognition",id:"3-intent-recognition",level:3},{value:"Security and Privacy Considerations",id:"security-and-privacy-considerations",level:2},{value:"1. Data Privacy",id:"1-data-privacy",level:3},{value:"2. Command Validation",id:"2-command-validation",level:3},{value:"3. Access Control",id:"3-access-control",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Resource Management",id:"1-resource-management",level:3},{value:"2. Accuracy Improvements",id:"2-accuracy-improvements",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues:",id:"common-issues",level:3},{value:"Verification Steps:",id:"verification-steps",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"voice-command-processing-system",children:"Voice Command Processing System"})}),"\n",(0,i.jsx)(n.p,{children:"This document provides a comprehensive guide for implementing a voice command processing system for robotics applications."}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"The voice command processing system enables natural human-robot interaction through spoken commands. This system handles speech recognition, natural language processing, and command execution for robotics applications."}),"\n",(0,i.jsx)(n.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"1-high-level-architecture",children:"1. High-Level Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Voice Input \u2192 Speech Recognition \u2192 Natural Language Processing \u2192 Command Execution \u2192 Robot Actions\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-component-breakdown",children:"2. Component Breakdown"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Input Module"}),": Captures and preprocesses audio from microphones"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech Recognition"}),": Converts speech to text using local or cloud-based ASR"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intent Recognition"}),": Parses text to identify user intent and parameters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Command Mapping"}),": Maps intents to specific robot actions"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execution Module"}),": Executes commands safely on the robot"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"implementation-components",children:"Implementation Components"}),"\n",(0,i.jsx)(n.h3,{id:"1-audio-input-and-preprocessing",children:"1. Audio Input and Preprocessing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# audio_input.py\r\nimport pyaudio\r\nimport numpy as np\r\nimport webrtcvad\r\nimport collections\r\nimport queue\r\nfrom typing import Generator, Tuple\r\nimport threading\r\nimport time\r\n\r\nclass AudioInput:\r\n    """\r\n    Audio input module with voice activity detection\r\n    """\r\n    def __init__(self, sample_rate: int = 16000, chunk_size: int = 320):\r\n        self.sample_rate = sample_rate\r\n        self.chunk_size = chunk_size\r\n        self.audio = pyaudio.PyAudio()\r\n\r\n        # Voice activity detection\r\n        self.vad = webrtcvad.Vad(2)  # Aggressiveness level 2\r\n        self.ring_buffer = collections.deque(maxlen=300)  # 300 frames\r\n        self.triggered = False\r\n        self.vad_frames = []\r\n\r\n        # Audio stream\r\n        self.stream = self.audio.open(\r\n            format=pyaudio.paInt16,\r\n            channels=1,\r\n            rate=sample_rate,\r\n            input=True,\r\n            frames_per_buffer=chunk_size\r\n        )\r\n\r\n        # Audio queue for processing\r\n        self.audio_queue = queue.Queue()\r\n        self.is_listening = False\r\n\r\n    def start_listening(self):\r\n        """Start audio capture in a separate thread"""\r\n        self.is_listening = True\r\n        self.capture_thread = threading.Thread(target=self._capture_audio)\r\n        self.capture_thread.daemon = True\r\n        self.capture_thread.start()\r\n\r\n    def stop_listening(self):\r\n        """Stop audio capture"""\r\n        self.is_listening = False\r\n        if hasattr(self, \'capture_thread\'):\r\n            self.capture_thread.join()\r\n\r\n    def _capture_audio(self):\r\n        """Capture audio in a loop"""\r\n        while self.is_listening:\r\n            audio_data = self.stream.read(self.chunk_size, exception_on_overflow=False)\r\n            self.audio_queue.put(audio_data)\r\n\r\n    def get_audio_stream(self) -> Generator[bytes, None, None]:\r\n        """Generator that yields audio chunks when voice activity is detected"""\r\n        while self.is_listening:\r\n            try:\r\n                audio_chunk = self.audio_queue.get(timeout=0.1)\r\n                yield audio_chunk\r\n            except queue.Empty:\r\n                continue\r\n\r\n    def detect_voice_activity(self, audio_chunk: bytes) -> bool:\r\n        """Detect if voice activity is present in audio chunk"""\r\n        # Convert to appropriate format for VAD\r\n        # VAD requires 16kHz, 16-bit, mono audio in 10, 20, or 30ms chunks\r\n        chunk_duration = len(audio_chunk) * 1000 // (self.sample_rate * 2)  # Duration in ms\r\n\r\n        if chunk_duration not in [10, 20, 30]:\r\n            # Adjust chunk size or skip\r\n            return False\r\n\r\n        return self.vad.is_speech(audio_chunk, self.sample_rate)\r\n\r\n    def close(self):\r\n        """Close audio resources"""\r\n        self.is_listening = False\r\n        self.stream.stop_stream()\r\n        self.stream.close()\r\n        self.audio.terminate()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-speech-recognition-module",children:"2. Speech Recognition Module"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# speech_recognition.py\r\nimport speech_recognition as sr\r\nimport asyncio\r\nimport logging\r\nfrom typing import Optional\r\nimport tempfile\r\nimport os\r\nimport wave\r\n\r\nclass SpeechRecognizer:\r\n    """\r\n    Speech recognition module with multiple backend support\r\n    """\r\n    def __init__(self, backend: str = "google"):\r\n        self.recognizer = sr.Recognizer()\r\n        self.microphone = sr.Microphone()\r\n        self.backend = backend\r\n\r\n        # Adjust for ambient noise\r\n        with self.microphone as source:\r\n            self.recognizer.adjust_for_ambient_noise(source, duration=1.0)\r\n\r\n        # Set energy threshold\r\n        self.recognizer.energy_threshold = 4000\r\n\r\n        # Supported backends\r\n        self.supported_backends = {\r\n            "google": self._recognize_google,\r\n            "wit": self._recognize_wit,\r\n            "houndify": self._recognize_houndify,\r\n            "ibm": self._recognize_ibm\r\n        }\r\n\r\n        self.logger = logging.getLogger(__name__)\r\n\r\n    def recognize_speech(self, audio_data: sr.AudioData) -> Optional[str]:\r\n        """\r\n        Recognize speech from audio data using configured backend\r\n        """\r\n        if self.backend not in self.supported_backends:\r\n            raise ValueError(f"Unsupported backend: {self.backend}")\r\n\r\n        try:\r\n            result = self.supported_backends[self.backend](audio_data)\r\n            self.logger.info(f"Recognized speech: {result}")\r\n            return result\r\n        except Exception as e:\r\n            self.logger.error(f"Speech recognition error: {e}")\r\n            return None\r\n\r\n    def _recognize_google(self, audio_data: sr.AudioData) -> str:\r\n        """Google Web Speech API recognition"""\r\n        return self.recognizer.recognize_google(audio_data)\r\n\r\n    def _recognize_wit(self, audio_data: sr.AudioData) -> str:\r\n        """Wit.ai recognition (requires API key)"""\r\n        # This would require wit.ai API key configuration\r\n        # return self.recognizer.recognize_wit(audio_data, key="YOUR_WIT_AI_KEY")\r\n        raise NotImplementedError("Wit.ai integration requires API key setup")\r\n\r\n    def _recognize_houndify(self, audio_data: sr.AudioData) -> str:\r\n        """Houndify recognition (requires client ID and key)"""\r\n        # This would require Houndify credentials\r\n        # return self.recognizer.recognize_houndify(audio_data, client_id="YOUR_CLIENT_ID", client_key="YOUR_CLIENT_KEY")\r\n        raise NotImplementedError("Houndify integration requires credentials")\r\n\r\n    def _recognize_ibm(self, audio_data: sr.AudioData) -> str:\r\n        """IBM Watson recognition (requires credentials)"""\r\n        # This would require IBM Watson credentials\r\n        # return self.recognizer.recognize_ibm(audio_data, username="YOUR_USERNAME", password="YOUR_PASSWORD")\r\n        raise NotImplementedError("IBM Watson integration requires credentials")\r\n\r\n    def listen_for_command(self, timeout: int = 5, phrase_time_limit: int = 10) -> Optional[str]:\r\n        """\r\n        Listen for a voice command using the microphone\r\n        """\r\n        try:\r\n            with self.microphone as source:\r\n                self.logger.info("Listening for voice command...")\r\n                audio = self.recognizer.listen(source, timeout=timeout, phrase_time_limit=phrase_time_limit)\r\n\r\n            return self.recognize_speech(audio)\r\n        except sr.WaitTimeoutError:\r\n            self.logger.warning("Timeout waiting for speech")\r\n            return None\r\n        except sr.UnknownValueError:\r\n            self.logger.warning("Could not understand audio")\r\n            return None\r\n        except sr.RequestError as e:\r\n            self.logger.error(f"Speech recognition service error: {e}")\r\n            return None\r\n\r\n# Example of using Whisper for local speech recognition\r\nclass LocalSpeechRecognizer:\r\n    """\r\n    Local speech recognition using OpenAI Whisper (requires installation)\r\n    """\r\n    def __init__(self, model_size: str = "base"):\r\n        try:\r\n            import whisper\r\n            self.model = whisper.load_model(model_size)\r\n        except ImportError:\r\n            raise ImportError("Please install whisper: pip install openai-whisper")\r\n\r\n    def recognize_from_audio_data(self, audio_data: sr.AudioData) -> Optional[str]:\r\n        """\r\n        Recognize speech using local Whisper model\r\n        """\r\n        import io\r\n        import numpy as np\r\n\r\n        # Convert audio data to WAV format\r\n        wav_buffer = io.BytesIO()\r\n        with wave.open(wav_buffer, \'wb\') as wav_file:\r\n            wav_file.setnchannels(1)  # Mono\r\n            wav_file.setsampwidth(2)  # 16-bit\r\n            wav_file.setframerate(16000)  # 16kHz\r\n            wav_file.writeframes(audio_data.frame_data)\r\n\r\n        # Convert to numpy array for Whisper\r\n        wav_buffer.seek(0)\r\n        audio_array = np.frombuffer(wav_buffer.read(), dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n        # Transcribe\r\n        result = self.model.transcribe(audio_array)\r\n        return result["text"]\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-natural-language-processing-and-intent-recognition",children:"3. Natural Language Processing and Intent Recognition"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# nlp_processor.py\r\nimport re\r\nimport spacy\r\nimport logging\r\nfrom typing import Dict, List, Optional, Tuple\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass CommandIntent:\r\n    """Structured representation of a recognized command"""\r\n    intent: str\r\n    parameters: Dict[str, str]\r\n    confidence: float\r\n    raw_text: str\r\n\r\nclass NLPProcessor:\r\n    """\r\n    Natural language processing module for intent recognition\r\n    """\r\n    def __init__(self):\r\n        # Load spaCy model (install with: python -m spacy download en_core_web_sm)\r\n        try:\r\n            self.nlp = spacy.load("en_core_web_sm")\r\n        except OSError:\r\n            raise OSError("Please install spaCy English model: python -m spacy download en_core_web_sm")\r\n\r\n        self.logger = logging.getLogger(__name__)\r\n\r\n        # Define intent patterns\r\n        self.intent_patterns = {\r\n            "move_to": [\r\n                r"move to (.+)",\r\n                r"go to (.+)",\r\n                r"navigate to (.+)",\r\n                r"move towards (.+)",\r\n                r"head to (.+)"\r\n            ],\r\n            "pick_up": [\r\n                r"pick up (.+)",\r\n                r"grasp (.+)",\r\n                r"take (.+)",\r\n                r"grab (.+)"\r\n            ],\r\n            "place": [\r\n                r"place (.+) (?:at|on|in) (.+)",\r\n                r"put (.+) (?:at|on|in) (.+)",\r\n                r"drop (.+) (?:at|on|in) (.+)"\r\n            ],\r\n            "stop": [\r\n                r"stop",\r\n                r"halt",\r\n                r"freeze",\r\n                r"pause"\r\n            ],\r\n            "follow": [\r\n                r"follow (.+)",\r\n                r"follow me",\r\n                r"come with me"\r\n            ],\r\n            "greet": [\r\n                r"hello",\r\n                r"hi",\r\n                r"hey",\r\n                r"greetings"\r\n            ]\r\n        }\r\n\r\n        # Location keywords\r\n        self.location_keywords = {\r\n            "kitchen", "living room", "bedroom", "bathroom", "office",\r\n            "dining room", "hallway", "garage", "garden", "entrance"\r\n        }\r\n\r\n    def process_command(self, text: str) -> Optional[CommandIntent]:\r\n        """\r\n        Process text command and extract intent and parameters\r\n        """\r\n        text = text.lower().strip()\r\n        doc = self.nlp(text)\r\n\r\n        # Try pattern matching first\r\n        intent, params, confidence = self._match_patterns(text)\r\n\r\n        if intent:\r\n            # Enhance with NLP processing\r\n            enhanced_params = self._enhance_parameters(params, doc)\r\n            return CommandIntent(\r\n                intent=intent,\r\n                parameters=enhanced_params,\r\n                confidence=confidence,\r\n                raw_text=text\r\n            )\r\n\r\n        # If no pattern matched, try semantic analysis\r\n        semantic_intent = self._analyze_semantic_intent(doc)\r\n        if semantic_intent:\r\n            return semantic_intent\r\n\r\n        self.logger.warning(f"Could not identify intent for: {text}")\r\n        return None\r\n\r\n    def _match_patterns(self, text: str) -> Tuple[Optional[str], Dict, float]:\r\n        """\r\n        Match text against predefined patterns to identify intent\r\n        """\r\n        for intent, patterns in self.intent_patterns.items():\r\n            for pattern in patterns:\r\n                match = re.search(pattern, text)\r\n                if match:\r\n                    groups = match.groups()\r\n\r\n                    if intent == "move_to":\r\n                        return intent, {"location": groups[0]}, 0.9\r\n                    elif intent == "pick_up":\r\n                        return intent, {"object": groups[0]}, 0.9\r\n                    elif intent == "place":\r\n                        return intent, {"object": groups[0], "location": groups[1]}, 0.9\r\n                    elif intent == "follow":\r\n                        return intent, {"target": groups[0] if len(groups) > 0 else "user"}, 0.8\r\n                    elif intent in ["stop", "greet"]:\r\n                        return intent, {}, 0.95\r\n\r\n        return None, {}, 0.0\r\n\r\n    def _enhance_parameters(self, params: Dict, doc) -> Dict:\r\n        """\r\n        Enhance extracted parameters using NLP analysis\r\n        """\r\n        enhanced = params.copy()\r\n\r\n        # Extract named entities\r\n        for ent in doc.ents:\r\n            if ent.label_ in ["PERSON", "GPE", "LOC", "FAC"]:\r\n                enhanced[f"named_entity_{ent.label_.lower()}"] = ent.text\r\n\r\n        # Extract adjectives and adverbs for context\r\n        for token in doc:\r\n            if token.pos_ == "ADJ":\r\n                if "adjectives" not in enhanced:\r\n                    enhanced["adjectives"] = []\r\n                enhanced["adjectives"].append(token.text)\r\n            elif token.pos_ == "ADV":\r\n                if "adverbs" not in enhanced:\r\n                    enhanced["adverbs"] = []\r\n                enhanced["adverbs"].append(token.text)\r\n\r\n        return enhanced\r\n\r\n    def _analyze_semantic_intent(self, doc) -> Optional[CommandIntent]:\r\n        """\r\n        Analyze semantic intent when pattern matching fails\r\n        """\r\n        # Look for action verbs\r\n        action_verbs = [token.lemma_ for token in doc if token.pos_ == "VERB"]\r\n\r\n        # Look for objects\r\n        objects = [token.text for token in doc if token.pos_ in ["NOUN", "PROPN"]]\r\n\r\n        # Look for locations\r\n        locations = [ent.text for ent in doc.ents if ent.label_ in ["GPE", "LOC", "FAC"]]\r\n\r\n        # Determine intent based on semantic analysis\r\n        if any(verb in ["go", "move", "navigate", "walk", "run", "head"] for verb in action_verbs):\r\n            location = locations[0] if locations else None\r\n            return CommandIntent(\r\n                intent="move_to",\r\n                parameters={"location": location or "unknown"},\r\n                confidence=0.7,\r\n                raw_text=doc.text\r\n            )\r\n\r\n        if any(verb in ["pick", "take", "grasp", "grab", "hold"] for verb in action_verbs):\r\n            obj = objects[0] if objects else None\r\n            return CommandIntent(\r\n                intent="pick_up",\r\n                parameters={"object": obj or "unknown"},\r\n                confidence=0.7,\r\n                raw_text=doc.text\r\n            )\r\n\r\n        return None\n'})}),"\n",(0,i.jsx)(n.h3,{id:"4-voice-command-processing-system",children:"4. Voice Command Processing System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# voice_command_system.py\r\nimport asyncio\r\nimport logging\r\nfrom typing import Dict, Optional, Callable\r\nimport threading\r\nimport time\r\nfrom dataclasses import dataclass\r\n\r\n@dataclass\r\nclass VoiceCommandResult:\r\n    """Result of voice command processing"""\r\n    success: bool\r\n    message: str\r\n    command: Optional[CommandIntent] = None\r\n    execution_result: Optional[Dict] = None\r\n\r\nclass VoiceCommandProcessor:\r\n    """\r\n    Main voice command processing system\r\n    """\r\n    def __init__(self, speech_recognizer, nlp_processor):\r\n        self.speech_recognizer = speech_recognizer\r\n        self.nlp_processor = nlp_processor\r\n        self.logger = logging.getLogger(__name__)\r\n\r\n        # Command execution callbacks\r\n        self.command_handlers = {}\r\n\r\n        # System state\r\n        self.is_active = False\r\n        self.command_queue = asyncio.Queue()\r\n        self.response_callbacks = []\r\n\r\n        # Wake word detection (simple implementation)\r\n        self.wake_words = ["robot", "hey robot", "hello robot", "attention"]\r\n        self.wake_word_threshold = 0.8\r\n\r\n    def register_command_handler(self, intent: str, handler: Callable):\r\n        """\r\n        Register a handler for a specific command intent\r\n        """\r\n        self.command_handlers[intent] = handler\r\n\r\n    async def process_voice_command(self, timeout: int = 10) -> VoiceCommandResult:\r\n        """\r\n        Process a single voice command from start to finish\r\n        """\r\n        try:\r\n            # Listen for command\r\n            self.logger.info("Listening for voice command...")\r\n            text = await asyncio.get_event_loop().run_in_executor(\r\n                None,\r\n                self.speech_recognizer.listen_for_command,\r\n                timeout\r\n            )\r\n\r\n            if not text:\r\n                return VoiceCommandResult(\r\n                    success=False,\r\n                    message="No speech detected or recognition failed"\r\n                )\r\n\r\n            self.logger.info(f"Recognized: {text}")\r\n\r\n            # Process with NLP\r\n            command_intent = self.nlp_processor.process_command(text)\r\n            if not command_intent:\r\n                return VoiceCommandResult(\r\n                    success=False,\r\n                    message=f"Could not understand command: {text}"\r\n                )\r\n\r\n            # Execute command if handler exists\r\n            if command_intent.intent in self.command_handlers:\r\n                handler = self.command_handlers[command_intent.intent]\r\n                execution_result = await asyncio.get_event_loop().run_in_executor(\r\n                    None,\r\n                    handler,\r\n                    command_intent\r\n                )\r\n\r\n                return VoiceCommandResult(\r\n                    success=True,\r\n                    message=f"Command \'{command_intent.intent}\' executed successfully",\r\n                    command=command_intent,\r\n                    execution_result=execution_result\r\n                )\r\n            else:\r\n                return VoiceCommandResult(\r\n                    success=False,\r\n                    message=f"No handler for command intent: {command_intent.intent}",\r\n                    command=command_intent\r\n                )\r\n\r\n        except Exception as e:\r\n            self.logger.error(f"Error processing voice command: {e}")\r\n            return VoiceCommandResult(\r\n                success=False,\r\n                message=f"Error processing voice command: {str(e)}"\r\n            )\r\n\r\n    def add_response_callback(self, callback: Callable):\r\n        """\r\n        Add a callback for voice command responses\r\n        """\r\n        self.response_callbacks.append(callback)\r\n\r\n    def speak_response(self, text: str):\r\n        """\r\n        Generate speech response (placeholder - implement with TTS)\r\n        """\r\n        import pyttsx3\r\n        try:\r\n            engine = pyttsx3.init()\r\n            # Configure speech properties\r\n            rate = engine.getProperty(\'rate\')\r\n            engine.setProperty(\'rate\', rate - 50)  # Slower speech\r\n\r\n            volume = engine.getProperty(\'volume\')\r\n            engine.setProperty(\'volume\', volume + 0.25)\r\n\r\n            engine.say(text)\r\n            engine.runAndWait()\r\n        except Exception as e:\r\n            self.logger.error(f"Text-to-speech error: {e}")\r\n\r\n    async def continuous_listening(self):\r\n        """\r\n        Continuously listen for voice commands\r\n        """\r\n        self.is_active = True\r\n        self.logger.info("Starting continuous voice command listening...")\r\n\r\n        while self.is_active:\r\n            try:\r\n                result = await self.process_voice_command(timeout=5)\r\n\r\n                if result.success:\r\n                    self.logger.info(f"Command executed: {result.message}")\r\n\r\n                    # Execute response callbacks\r\n                    for callback in self.response_callbacks:\r\n                        callback(result)\r\n\r\n                    # Provide audio feedback\r\n                    self.speak_response("Command executed successfully")\r\n                else:\r\n                    self.logger.warning(f"Command failed: {result.message}")\r\n                    self.speak_response("Sorry, I couldn\'t understand that command")\r\n\r\n                # Small delay to prevent excessive processing\r\n                await asyncio.sleep(0.5)\r\n\r\n            except KeyboardInterrupt:\r\n                self.logger.info("Voice command processing interrupted")\r\n                break\r\n            except Exception as e:\r\n                self.logger.error(f"Error in continuous listening: {e}")\r\n                await asyncio.sleep(1)  # Wait before retrying\r\n\r\n    def stop_listening(self):\r\n        """\r\n        Stop continuous listening\r\n        """\r\n        self.is_active = False\r\n        self.logger.info("Stopped voice command listening")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"5-ros-2-integration",children:"5. ROS 2 Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# ros_voice_integration.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Pose\r\nfrom sensor_msgs.msg import AudioData\r\nimport asyncio\r\nimport threading\r\n\r\nclass ROSVoiceInterface(Node):\r\n    """\r\n    ROS 2 interface for voice command processing\r\n    """\r\n    def __init__(self, voice_processor):\r\n        super().__init__(\'voice_command_interface\')\r\n        self.voice_processor = voice_processor\r\n\r\n        # Publishers\r\n        self.command_publisher = self.create_publisher(String, \'robot_commands\', 10)\r\n        self.speech_publisher = self.create_publisher(String, \'speech_output\', 10)\r\n\r\n        # Subscribers\r\n        self.voice_command_sub = self.create_subscription(\r\n            String, \'voice_commands\', self.voice_command_callback, 10\r\n        )\r\n\r\n        # Timer for continuous processing\r\n        self.processing_timer = self.create_timer(0.1, self.process_voice_queue)\r\n\r\n        # Voice command queue\r\n        self.voice_command_queue = asyncio.Queue()\r\n\r\n        # Start async processing in separate thread\r\n        self.voice_thread = threading.Thread(target=self._run_voice_processing)\r\n        self.voice_thread.daemon = True\r\n        self.voice_thread.start()\r\n\r\n    def voice_command_callback(self, msg):\r\n        """\r\n        Handle voice command messages\r\n        """\r\n        asyncio.run_coroutine_threadsafe(\r\n            self.voice_command_queue.put(msg.data),\r\n            self.voice_loop\r\n        )\r\n\r\n    async def process_voice_queue(self):\r\n        """\r\n        Process voice commands from the queue\r\n        """\r\n        try:\r\n            # Non-blocking check for voice commands\r\n            command = self.voice_command_queue.get_nowait()\r\n            result = await self.voice_processor.process_voice_command_from_text(command)\r\n\r\n            if result.success:\r\n                # Publish robot command\r\n                cmd_msg = String()\r\n                cmd_msg.data = str(result.command)\r\n                self.command_publisher.publish(cmd_msg)\r\n\r\n                # Publish success feedback\r\n                feedback_msg = String()\r\n                feedback_msg.data = "Command executed successfully"\r\n                self.speech_publisher.publish(feedback_msg)\r\n        except asyncio.QueueEmpty:\r\n            pass  # No commands to process\r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing voice command: {e}")\r\n\r\n    def _run_voice_processing(self):\r\n        """\r\n        Run the voice processing loop in a separate thread\r\n        """\r\n        self.voice_loop = asyncio.new_event_loop()\r\n        asyncio.set_event_loop(self.voice_loop)\r\n        self.voice_loop.run_forever()\r\n\r\n    def publish_robot_command(self, command: str):\r\n        """\r\n        Publish a command to the robot\r\n        """\r\n        msg = String()\r\n        msg.data = command\r\n        self.command_publisher.publish(msg)\r\n\r\n    def publish_speech_output(self, text: str):\r\n        """\r\n        Publish speech output for TTS\r\n        """\r\n        msg = String()\r\n        msg.data = text\r\n        self.speech_publisher.publish(msg)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"6-example-usage-and-integration",children:"6. Example Usage and Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# example_voice_robot.py\r\nimport asyncio\r\nimport logging\r\nfrom voice_command_system import VoiceCommandProcessor, VoiceCommandResult\r\nfrom speech_recognition import SpeechRecognizer\r\nfrom nlp_processor import NLPProcessor\r\n\r\n# Example robot command handlers\r\nclass RobotCommandHandlers:\r\n    def __init__(self, robot_interface):\r\n        self.robot_interface = robot_interface\r\n\r\n    def handle_move_to(self, command_intent):\r\n        """Handle move_to commands"""\r\n        location = command_intent.parameters.get("location", "unknown")\r\n        self.robot_interface.move_to_location(location)\r\n        return {"status": "success", "location": location}\r\n\r\n    def handle_pick_up(self, command_intent):\r\n        """Handle pick_up commands"""\r\n        obj = command_intent.parameters.get("object", "unknown")\r\n        self.robot_interface.pick_up_object(obj)\r\n        return {"status": "success", "object": obj}\r\n\r\n    def handle_place(self, command_intent):\r\n        """Handle place commands"""\r\n        obj = command_intent.parameters.get("object", "unknown")\r\n        location = command_intent.parameters.get("location", "unknown")\r\n        self.robot_interface.place_object(obj, location)\r\n        return {"status": "success", "object": obj, "location": location}\r\n\r\n    def handle_stop(self, command_intent):\r\n        """Handle stop commands"""\r\n        self.robot_interface.stop_movement()\r\n        return {"status": "stopped"}\r\n\r\ndef main():\r\n    # Setup logging\r\n    logging.basicConfig(level=logging.INFO)\r\n\r\n    # Initialize components\r\n    speech_recognizer = SpeechRecognizer(backend="google")  # or "local" for Whisper\r\n    nlp_processor = NLPProcessor()\r\n\r\n    # Create voice command processor\r\n    voice_processor = VoiceCommandProcessor(speech_recognizer, nlp_processor)\r\n\r\n    # Example robot interface (replace with actual robot interface)\r\n    class MockRobotInterface:\r\n        def move_to_location(self, location):\r\n            print(f"Moving to {location}")\r\n\r\n        def pick_up_object(self, obj):\r\n            print(f"Picking up {obj}")\r\n\r\n        def place_object(self, obj, location):\r\n            print(f"Placing {obj} at {location}")\r\n\r\n        def stop_movement(self):\r\n            print("Stopping movement")\r\n\r\n    # Create command handlers\r\n    robot_interface = MockRobotInterface()\r\n    command_handlers = RobotCommandHandlers(robot_interface)\r\n\r\n    # Register command handlers\r\n    voice_processor.register_command_handler("move_to", command_handlers.handle_move_to)\r\n    voice_processor.register_command_handler("pick_up", command_handlers.handle_pick_up)\r\n    voice_processor.register_command_handler("place", command_handlers.handle_place)\r\n    voice_processor.register_command_handler("stop", command_handlers.handle_stop)\r\n\r\n    # Add response callback\r\n    def command_response_callback(result: VoiceCommandResult):\r\n        print(f"Command result: {result.message}")\r\n        if result.execution_result:\r\n            print(f"Execution result: {result.execution_result}")\r\n\r\n    voice_processor.add_response_callback(command_response_callback)\r\n\r\n    # Run continuous listening\r\n    print("Starting voice command system...")\r\n    print("Say \'robot\' followed by a command, or speak a direct command")\r\n    print("Examples: \'move to kitchen\', \'pick up the cup\', \'stop\'")\r\n\r\n    try:\r\n        # Run the continuous listening loop\r\n        asyncio.run(voice_processor.continuous_listening())\r\n    except KeyboardInterrupt:\r\n        print("\\nStopping voice command system...")\r\n        voice_processor.stop_listening()\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"installation-requirements",children:"Installation Requirements"}),"\n",(0,i.jsx)(n.h3,{id:"python-dependencies",children:"Python Dependencies"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install SpeechRecognition pyaudio webrtcvad spacy pyttsx3 numpy\r\npython -m spacy download en_core_web_sm\n"})}),"\n",(0,i.jsx)(n.h3,{id:"for-local-whisper-support-optional",children:"For Local Whisper Support (optional)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,i.jsx)(n.h3,{id:"for-ros-2-integration",children:"For ROS 2 Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install rclpy\n"})}),"\n",(0,i.jsx)(n.h2,{id:"configuration-and-tuning",children:"Configuration and Tuning"}),"\n",(0,i.jsx)(n.h3,{id:"1-audio-configuration",children:"1. Audio Configuration"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Adjust ",(0,i.jsx)(n.code,{children:"energy_threshold"})," based on ambient noise levels"]}),"\n",(0,i.jsxs)(n.li,{children:["Modify ",(0,i.jsx)(n.code,{children:"phrase_time_limit"})," for longer/shorter commands"]}),"\n",(0,i.jsx)(n.li,{children:"Tune VAD aggressiveness level (0-3) for voice detection sensitivity"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-recognition-accuracy",children:"2. Recognition Accuracy"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Use appropriate speech recognition backend based on requirements:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Google: Good accuracy, requires internet, free tier"}),"\n",(0,i.jsx)(n.li,{children:"Local Whisper: Privacy, offline capability, resource intensive"}),"\n",(0,i.jsx)(n.li,{children:"Wit.ai/IBM: Customizable, requires API keys"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-intent-recognition",children:"3. Intent Recognition"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Extend intent patterns for domain-specific commands"}),"\n",(0,i.jsx)(n.li,{children:"Train spaCy models on robotics-specific language"}),"\n",(0,i.jsx)(n.li,{children:"Implement confidence thresholds for command execution"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"security-and-privacy-considerations",children:"Security and Privacy Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"1-data-privacy",children:"1. Data Privacy"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use local speech recognition for privacy-sensitive applications"}),"\n",(0,i.jsx)(n.li,{children:"Encrypt audio data in transit if using cloud services"}),"\n",(0,i.jsx)(n.li,{children:"Implement data retention policies for audio recordings"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-command-validation",children:"2. Command Validation"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Validate all voice commands before execution"}),"\n",(0,i.jsx)(n.li,{children:"Implement safety checks and bounds verification"}),"\n",(0,i.jsx)(n.li,{children:"Use authentication for sensitive commands"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-access-control",children:"3. Access Control"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Limit voice command vocabulary to safe operations"}),"\n",(0,i.jsx)(n.li,{children:"Implement user identification and authorization"}),"\n",(0,i.jsx)(n.li,{children:"Log all voice commands for audit purposes"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"1-resource-management",children:"1. Resource Management"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use appropriate model sizes for target hardware"}),"\n",(0,i.jsx)(n.li,{children:"Implement audio buffering and streaming"}),"\n",(0,i.jsx)(n.li,{children:"Optimize recognition timeout values"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-accuracy-improvements",children:"2. Accuracy Improvements"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Train domain-specific NLP models"}),"\n",(0,i.jsx)(n.li,{children:"Implement acoustic adaptation for environment"}),"\n",(0,i.jsx)(n.li,{children:"Use wake word detection to reduce processing load"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues",children:"Common Issues:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio Input Problems"}),": Check microphone permissions and audio drivers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recognition Accuracy"}),": Adjust energy threshold and ambient noise settings"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Network Issues"}),": Implement fallback mechanisms for offline capability"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Processing Delays"}),": Optimize model sizes and implement async processing"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"verification-steps",children:"Verification Steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Test audio input and voice activity detection"}),"\n",(0,i.jsx)(n.li,{children:"Verify speech recognition accuracy in your environment"}),"\n",(0,i.jsx)(n.li,{children:"Confirm intent recognition works with expected commands"}),"\n",(0,i.jsx)(n.li,{children:"Test command execution safety and validation"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);