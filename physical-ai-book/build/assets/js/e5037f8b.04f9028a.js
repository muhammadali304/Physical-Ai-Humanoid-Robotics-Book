"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[8363],{6266:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"capstone/capstone-project-integration-guide","title":"Capstone Project Integration Guide","description":"This document provides a comprehensive guide for integrating all the components learned throughout the Physical AI and Humanoid Robotics curriculum into a cohesive capstone project.","source":"@site/docs/capstone/capstone-project-integration-guide.md","sourceDirName":"capstone","slug":"/capstone/capstone-project-integration-guide","permalink":"/docs/capstone/capstone-project-integration-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/capstone/capstone-project-integration-guide.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Voice Commands - Natural Language Interaction with Robots","permalink":"/docs/vla/voice-commands"},"next":{"title":"Autonomous Humanoid - Capstone Project","permalink":"/docs/capstone/autonomous-humanoid"}}');var o=t(4848),i=t(7074);const s={},r="Capstone Project Integration Guide",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Capstone Project Architecture",id:"capstone-project-architecture",level:2},{value:"1. System Architecture Diagram",id:"1-system-architecture-diagram",level:3},{value:"2. Component Integration Points",id:"2-component-integration-points",level:3},{value:"A. ROS 2 Communication Layer",id:"a-ros-2-communication-layer",level:4},{value:"B. Simulation to Real World Interface",id:"b-simulation-to-real-world-interface",level:4},{value:"Implementation Guide",id:"implementation-guide",level:2},{value:"1. Project Structure Setup",id:"1-project-structure-setup",level:3},{value:"2. Core Integration Components",id:"2-core-integration-components",level:3},{value:"A. Main Capstone Node",id:"a-main-capstone-node",level:4},{value:"B. Capstone Launch File",id:"b-capstone-launch-file",level:4},{value:"3. Capstone Project Scenarios",id:"3-capstone-project-scenarios",level:3},{value:"A. Scenario 1: Object Retrieval Task",id:"a-scenario-1-object-retrieval-task",level:4},{value:"B. Scenario 2: Room Navigation and Mapping",id:"b-scenario-2-room-navigation-and-mapping",level:4},{value:"4. Testing and Validation Framework",id:"4-testing-and-validation-framework",level:3},{value:"A. Unit Tests for Components",id:"a-unit-tests-for-components",level:4},{value:"B. Integration Tests",id:"b-integration-tests",level:4},{value:"5. Performance Monitoring and Optimization",id:"5-performance-monitoring-and-optimization",level:3},{value:"A. Performance Dashboard",id:"a-performance-dashboard",level:4},{value:"6. Deployment Considerations",id:"6-deployment-considerations",level:3},{value:"A. Simulation to Real Deployment Checklist",id:"a-simulation-to-real-deployment-checklist",level:4},{value:"B. Configuration Management",id:"b-configuration-management",level:4},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"1. Technical Requirements",id:"1-technical-requirements",level:3},{value:"2. Functional Requirements",id:"2-functional-requirements",level:3},{value:"3. Assessment Rubric",id:"3-assessment-rubric",level:3},{value:"Troubleshooting and Maintenance",id:"troubleshooting-and-maintenance",level:2},{value:"Common Issues:",id:"common-issues",level:3},{value:"Maintenance Tasks:",id:"maintenance-tasks",level:3},{value:"Resources and Further Development",id:"resources-and-further-development",level:2},{value:"Extension Opportunities:",id:"extension-opportunities",level:3},{value:"Documentation References:",id:"documentation-references",level:3}];function m(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"capstone-project-integration-guide",children:"Capstone Project Integration Guide"})}),"\n",(0,o.jsx)(e.p,{children:"This document provides a comprehensive guide for integrating all the components learned throughout the Physical AI and Humanoid Robotics curriculum into a cohesive capstone project."}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"The capstone project integrates all major components covered in the curriculum:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"ROS 2 development environment"}),"\n",(0,o.jsx)(e.li,{children:"Gazebo simulation"}),"\n",(0,o.jsx)(e.li,{children:"Isaac ROS perception and navigation"}),"\n",(0,o.jsx)(e.li,{children:"Vision-Language-Action (VLA) systems"}),"\n",(0,o.jsx)(e.li,{children:"Reinforcement learning"}),"\n",(0,o.jsx)(e.li,{children:"LLM integration"}),"\n",(0,o.jsx)(e.li,{children:"Voice command processing"}),"\n",(0,o.jsx)(e.li,{children:"Edge device deployment"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"capstone-project-architecture",children:"Capstone Project Architecture"}),"\n",(0,o.jsx)(e.h3,{id:"1-system-architecture-diagram",children:"1. System Architecture Diagram"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   User Input    \u2502    \u2502   LLM Interface  \u2502    \u2502   ROS 2 Core    \u2502\n\u2502                 \u2502\u2500\u2500\u2500\u25b6\u2502                  \u2502\u2500\u2500\u2500\u25b6\u2502                 \u2502\n\u2502 \u2022 Voice Commands\u2502    \u2502 \u2022 Natural        \u2502    \u2502 \u2022 Navigation    \u2502\n\u2502 \u2022 Text Commands \u2502    \u2502   Language       \u2502    \u2502 \u2022 Perception    \u2502\n\u2502 \u2022 GUI Control   \u2502    \u2502   Processing     \u2502    \u2502 \u2022 Control       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Decision Engine \u2502\n                    \u2502                  \u2502\n                    \u2502 \u2022 Task Planning  \u2502\n                    \u2502 \u2022 Behavior       \u2502\n                    \u2502   Selection      \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u25bc               \u25bc               \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Navigation     \u2502\u2502  Perception     \u2502\u2502  Manipulation  \u2502\n    \u2502  System         \u2502\u2502  System         \u2502\u2502  System        \u2502\n    \u2502                 \u2502\u2502                 \u2502\u2502                 \u2502\n    \u2502 \u2022 Path Planning \u2502\u2502 \u2022 Object        \u2502\u2502 \u2022 Grasp         \u2502\n    \u2502 \u2022 Localization  \u2502\u2502   Detection     \u2502\u2502   Planning      \u2502\n    \u2502 \u2022 Path Execution\u2502\u2502 \u2022 SLAM          \u2502\u2502 \u2022 Motion        \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502   Control       \u2502\n                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-component-integration-points",children:"2. Component Integration Points"}),"\n",(0,o.jsx)(e.h4,{id:"a-ros-2-communication-layer",children:"A. ROS 2 Communication Layer"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Servers"}),": Navigation goals, manipulation tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Topics"}),": Sensor data, robot state, command execution"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Services"}),": System configuration, calibration, diagnostics"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Parameters"}),": Runtime configuration, system settings"]}),"\n"]}),"\n",(0,o.jsx)(e.h4,{id:"b-simulation-to-real-world-interface",children:"B. Simulation to Real World Interface"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Gazebo Simulation"}),": Development and testing environment"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Isaac Sim"}),": Advanced physics and RL training"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Hardware Abstraction Layer"}),": Unified interface for sim/real"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"implementation-guide",children:"Implementation Guide"}),"\n",(0,o.jsx)(e.h3,{id:"1-project-structure-setup",children:"1. Project Structure Setup"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Capstone project structure\ncapstone_project/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 robot_control/           # Low-level robot control\n\u2502   \u251c\u2500\u2500 perception/              # Perception system\n\u2502   \u251c\u2500\u2500 navigation/              # Navigation system\n\u2502   \u251c\u2500\u2500 manipulation/            # Manipulation system\n\u2502   \u251c\u2500\u2500 llm_interface/           # LLM integration\n\u2502   \u251c\u2500\u2500 voice_control/           # Voice command processing\n\u2502   \u2514\u2500\u2500 decision_engine/         # Main decision system\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 robot_config.yaml        # Robot-specific configuration\n\u2502   \u251c\u2500\u2500 navigation_params.yaml   # Navigation parameters\n\u2502   \u251c\u2500\u2500 perception_params.yaml   # Perception parameters\n\u2502   \u2514\u2500\u2500 capstone_params.yaml     # Capstone project parameters\n\u251c\u2500\u2500 launch/\n\u2502   \u251c\u2500\u2500 capstone_sim.launch.py   # Simulation launch\n\u2502   \u251c\u2500\u2500 capstone_real.launch.py  # Real robot launch\n\u2502   \u2514\u2500\u2500 capstone_test.launch.py  # Testing launch\n\u251c\u2500\u2500 worlds/                      # Simulation worlds\n\u251c\u2500\u2500 models/                      # Robot and object models\n\u251c\u2500\u2500 scripts/                     # Utility scripts\n\u2514\u2500\u2500 test/                        # Test cases\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-core-integration-components",children:"2. Core Integration Components"}),"\n",(0,o.jsx)(e.h4,{id:"a-main-capstone-node",children:"A. Main Capstone Node"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# src/decision_engine/capstone_manager.py\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.executors import MultiThreadedExecutor\n\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom sensor_msgs.msg import Image, LaserScan\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import Twist\n\nimport json\nimport asyncio\nfrom typing import Dict, Any, Optional\n\n\nclass CapstoneManager(Node):\n    """\n    Main capstone project manager that integrates all components\n    """\n    def __init__(self):\n        super().__init__(\'capstone_manager\')\n\n        # Initialize component managers\n        self.llm_interface = LLMInterface(self)\n        self.voice_processor = VoiceCommandProcessor(self)\n        self.navigation_manager = NavigationManager(self)\n        self.perception_manager = PerceptionManager(self)\n        self.manipulation_manager = ManipulationManager(self)\n\n        # Publishers\n        self.status_pub = self.create_publisher(String, \'capstone_status\', 10)\n        self.command_pub = self.create_publisher(String, \'capstone_commands\', 10)\n\n        # Subscribers\n        self.voice_sub = self.create_subscription(\n            String, \'voice_commands\', self.voice_command_callback, 10\n        )\n        self.text_sub = self.create_subscription(\n            String, \'text_commands\', self.text_command_callback, 10\n        )\n\n        # Action clients\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # State management\n        self.current_task = None\n        self.robot_state = {\n            \'location\': None,\n            \'battery_level\': 100,\n            \'gripper_status\': \'open\',\n            \'current_action\': \'idle\'\n        }\n\n        # Task queue\n        self.task_queue = []\n\n        # Start periodic state update\n        self.state_timer = self.create_timer(1.0, self.update_robot_state)\n\n    def voice_command_callback(self, msg):\n        """\n        Handle voice commands through LLM processing\n        """\n        self.get_logger().info(f"Processing voice command: {msg.data}")\n\n        # Process with LLM to extract intent\n        intent = self.llm_interface.process_command(msg.data, self.robot_state)\n\n        if intent:\n            self.execute_task(intent)\n\n    def text_command_callback(self, msg):\n        """\n        Handle text commands\n        """\n        self.get_logger().info(f"Processing text command: {msg.data}")\n\n        # Parse and execute\n        command = json.loads(msg.data)\n        self.execute_task(command)\n\n    def execute_task(self, task: Dict[str, Any]):\n        """\n        Execute a high-level task using appropriate subsystems\n        """\n        task_type = task.get(\'type\', \'unknown\')\n\n        if task_type == \'navigation\':\n            self.navigation_manager.navigate_to(task[\'destination\'])\n        elif task_type == \'manipulation\':\n            self.manipulation_manager.manipulate_object(task[\'object\'], task.get(\'action\', \'pick\'))\n        elif task_type == \'perception\':\n            self.perception_manager.analyze_environment()\n        elif task_type == \'combined\':\n            self.execute_combined_task(task)\n        else:\n            self.get_logger().error(f"Unknown task type: {task_type}")\n\n    def execute_combined_task(self, task: Dict[str, Any]):\n        """\n        Execute complex tasks that involve multiple subsystems\n        """\n        steps = task.get(\'steps\', [])\n\n        for step in steps:\n            step_type = step.get(\'type\')\n            if step_type == \'navigate\':\n                result = self.navigation_manager.navigate_to(step[\'location\'])\n                if not result.success:\n                    self.get_logger().error(f"Navigation step failed: {step}")\n                    return False\n            elif step_type == \'perceive\':\n                perception_result = self.perception_manager.analyze_area(step[\'location\'])\n                step[\'perception_data\'] = perception_result\n            elif step_type == \'manipulate\':\n                result = self.manipulation_manager.manipulate_object(\n                    step[\'object\'], step[\'action\'], step.get(\'location\')\n                )\n                if not result.success:\n                    self.get_logger().error(f"Manipulation step failed: {step}")\n                    return False\n\n        return True\n\n    def update_robot_state(self):\n        """\n        Periodically update robot state from various sources\n        """\n        # Update location from localization system\n        # Update battery from power system\n        # Update gripper status from manipulation system\n        # etc.\n\n        state_msg = String()\n        state_msg.data = json.dumps(self.robot_state)\n        self.status_pub.publish(state_msg)\n\n    def shutdown(self):\n        """\n        Clean shutdown of all components\n        """\n        self.navigation_manager.shutdown()\n        self.perception_manager.shutdown()\n        self.manipulation_manager.shutdown()\n        self.llm_interface.shutdown()\n\n\nclass LLMInterface:\n    """\n    Interface to LLM for natural language processing\n    """\n    def __init__(self, node):\n        self.node = node\n        self.llm_client = OpenAIIntegrator(api_key="your-api-key")  # Or your preferred LLM\n\n    def process_command(self, command: str, robot_state: Dict) -> Optional[Dict]:\n        """\n        Process natural language command and return structured task\n        """\n        try:\n            structured_command = self.llm_client.generate_robot_command(command, robot_state)\n            return structured_command\n        except Exception as e:\n            self.node.get_logger().error(f"LLM processing error: {e}")\n            return None\n\n    def shutdown(self):\n        """\n        Shutdown LLM interface\n        """\n        pass\n\n\nclass NavigationManager:\n    """\n    Navigation system manager\n    """\n    def __init__(self, node):\n        self.node = node\n        self.nav_client = ActionClient(node, NavigateToPose, \'navigate_to_pose\')\n\n    def navigate_to(self, destination: str) -> Dict:\n        """\n        Navigate to specified destination\n        """\n        # Convert destination string to PoseStamped\n        pose = self._get_pose_for_destination(destination)\n\n        if pose is None:\n            return {\'success\': False, \'error\': f\'Unknown destination: {destination}\'}\n\n        # Send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = pose\n\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal_msg)\n\n        # Wait for result (with timeout)\n        # Implementation would wait for result here\n        return {\'success\': True, \'destination\': destination}\n\n    def _get_pose_for_destination(self, destination: str) -> Optional[PoseStamped]:\n        """\n        Get pose for named destination\n        """\n        # This would typically look up destinations in a map\n        # For now, return a default pose\n        if destination.lower() == \'kitchen\':\n            pose = PoseStamped()\n            pose.header.frame_id = \'map\'\n            pose.pose.position.x = 2.0\n            pose.pose.position.y = 1.0\n            pose.pose.position.z = 0.0\n            return pose\n        return None\n\n    def shutdown(self):\n        """\n        Shutdown navigation manager\n        """\n        pass\n\n\nclass PerceptionManager:\n    """\n    Perception system manager\n    """\n    def __init__(self, node):\n        self.node = node\n        self.object_detector = None  # Initialize your object detection model\n\n    def analyze_environment(self):\n        """\n        Analyze current environment\n        """\n        # Process sensor data to detect objects, people, etc.\n        pass\n\n    def analyze_area(self, location: str):\n        """\n        Analyze specific area\n        """\n        # Navigate to area and perform detailed analysis\n        pass\n\n    def shutdown(self):\n        """\n        Shutdown perception manager\n        """\n        pass\n\n\nclass ManipulationManager:\n    """\n    Manipulation system manager\n    """\n    def __init__(self, node):\n        self.node = node\n\n    def manipulate_object(self, obj: str, action: str, location: str = None):\n        """\n        Manipulate specified object\n        """\n        # Implement object manipulation logic\n        pass\n\n    def shutdown(self):\n        """\n        Shutdown manipulation manager\n        """\n        pass\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    capstone_manager = CapstoneManager()\n\n    # Use multi-threaded executor to handle multiple callbacks\n    executor = MultiThreadedExecutor(num_threads=4)\n    executor.add_node(capstone_manager)\n\n    try:\n        executor.spin()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        capstone_manager.shutdown()\n        capstone_manager.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(e.h4,{id:"b-capstone-launch-file",children:"B. Capstone Launch File"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# launch/capstone_sim.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, TimerAction\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\n\ndef generate_launch_description():\n    # Declare launch arguments\n    world_arg = DeclareLaunchArgument(\n        'world',\n        default_value='capstone_world.sdf',\n        description='SDF world file for simulation'\n    )\n\n    robot_name_arg = DeclareLaunchArgument(\n        'robot_name',\n        default_value='humanoid_robot',\n        description='Name of the robot model'\n    )\n\n    # Get launch configurations\n    world_name = LaunchConfiguration('world')\n    robot_name = LaunchConfiguration('robot_name')\n\n    # Launch Gazebo with capstone world\n    gzserver_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('gazebo_ros'),\n                'launch',\n                'gzserver.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'world': PathJoinSubstitution([\n                FindPackageShare('capstone_project'),\n                'worlds',\n                world_name\n            ])\n        }.items()\n    )\n\n    gzclient_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('gazebo_ros'),\n                'launch',\n                'gzclient.launch.py'\n            ])\n        ])\n    )\n\n    # Spawn robot in the world\n    spawn_robot = Node(\n        package='gazebo_ros',\n        executable='spawn_entity.py',\n        arguments=[\n            '-topic', 'robot_description',\n            '-entity', robot_name,\n            '-x', '0.0', '-y', '0.0', '-z', '0.0'\n        ],\n        output='screen'\n    )\n\n    # Launch robot state publisher\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        name='robot_state_publisher',\n        parameters=[{\n            'use_sim_time': True,\n            'robot_description': PathJoinSubstitution([\n                FindPackageShare('capstone_project'),\n                'models',\n                'humanoid_robot.urdf'\n            ])\n        }]\n    )\n\n    # Launch Nav2 stack\n    nav2_launch = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('nav2_bringup'),\n                'launch',\n                'navigation_launch.py'\n            ])\n        ]),\n        launch_arguments={'use_sim_time': True}.items()\n    )\n\n    # Launch capstone manager\n    capstone_manager = Node(\n        package='capstone_project',\n        executable='capstone_manager.py',\n        name='capstone_manager',\n        parameters=[{\n            'use_sim_time': True,\n            'config_file': PathJoinSubstitution([\n                FindPackageShare('capstone_project'),\n                'config',\n                'capstone_params.yaml'\n            ])\n        }],\n        output='screen'\n    )\n\n    # Launch voice command processor\n    voice_processor = Node(\n        package='capstone_project',\n        executable='voice_processor.py',\n        name='voice_processor',\n        parameters=[{'use_sim_time': True}],\n        output='screen'\n    )\n\n    # Launch RViz for visualization\n    rviz_config = PathJoinSubstitution([\n        FindPackageShare('capstone_project'),\n        'rviz',\n        'capstone_view.rviz'\n    ])\n\n    rviz_node = Node(\n        package='rviz2',\n        executable='rviz2',\n        arguments=['-d', rviz_config],\n        parameters=[{'use_sim_time': True}]\n    )\n\n    return LaunchDescription([\n        world_arg,\n        robot_name_arg,\n        gzserver_launch,\n        gzclient_launch,\n        spawn_robot,\n        robot_state_publisher,\n        nav2_launch,\n        capstone_manager,\n        voice_processor,\n        rviz_node\n    ])\n"})}),"\n",(0,o.jsx)(e.h3,{id:"3-capstone-project-scenarios",children:"3. Capstone Project Scenarios"}),"\n",(0,o.jsx)(e.h4,{id:"a-scenario-1-object-retrieval-task",children:"A. Scenario 1: Object Retrieval Task"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# capstone_scenarios/object_retrieval.py\nclass ObjectRetrievalScenario:\n    \"\"\"\n    Scenario: Robot retrieves object based on voice command\n    \"\"\"\n    def __init__(self, capstone_manager):\n        self.capstone_manager = capstone_manager\n\n    def execute(self):\n        \"\"\"\n        Execute object retrieval scenario\n        \"\"\"\n        # 1. Wait for voice command\n        self.capstone_manager.get_logger().info(\"Waiting for object retrieval command...\")\n\n        # 2. Process command to identify object and destination\n        # This would be triggered by voice input\n        task = {\n            'type': 'combined',\n            'steps': [\n                {\n                    'type': 'perceive',\n                    'location': 'living_room',\n                    'description': 'Look for the red cup in the living room'\n                },\n                {\n                    'type': 'navigate',\n                    'location': 'living_room',\n                    'description': 'Go to living room to find the cup'\n                },\n                {\n                    'type': 'manipulate',\n                    'action': 'pick',\n                    'object': 'red cup',\n                    'description': 'Pick up the red cup'\n                },\n                {\n                    'type': 'navigate',\n                    'location': 'kitchen',\n                    'description': 'Go to kitchen'\n                },\n                {\n                    'type': 'manipulate',\n                    'action': 'place',\n                    'object': 'red cup',\n                    'location': 'kitchen_counter',\n                    'description': 'Place cup on kitchen counter'\n                }\n            ]\n        }\n\n        # 3. Execute the task\n        result = self.capstone_manager.execute_combined_task(task)\n\n        # 4. Report results\n        if result:\n            self.capstone_manager.get_logger().info(\"Object retrieval task completed successfully!\")\n        else:\n            self.capstone_manager.get_logger().error(\"Object retrieval task failed!\")\n\n        return result\n"})}),"\n",(0,o.jsx)(e.h4,{id:"b-scenario-2-room-navigation-and-mapping",children:"B. Scenario 2: Room Navigation and Mapping"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# capstone_scenarios/room_mapping.py\nclass RoomMappingScenario:\n    \"\"\"\n    Scenario: Robot maps a room and reports findings\n    \"\"\"\n    def __init__(self, capstone_manager):\n        self.capstone_manager = capstone_manager\n\n    def execute(self):\n        \"\"\"\n        Execute room mapping scenario\n        \"\"\"\n        self.capstone_manager.get_logger().info(\"Starting room mapping scenario...\")\n\n        # 1. Navigate to different waypoints in the room\n        waypoints = [\n            {'name': 'entrance', 'x': 0.0, 'y': 0.0},\n            {'name': 'center', 'x': 2.0, 'y': 1.0},\n            {'name': 'corner1', 'x': 3.0, 'y': 2.0},\n            {'name': 'corner2', 'x': 1.0, 'y': 3.0}\n        ]\n\n        # 2. Build map while navigating\n        for waypoint in waypoints:\n            self.capstone_manager.get_logger().info(f\"Navigating to {waypoint['name']}\")\n\n            # Navigate to waypoint\n            nav_task = {\n                'type': 'navigation',\n                'destination': waypoint['name']\n            }\n            self.capstone_manager.execute_task(nav_task)\n\n            # 3. Perform perception at each waypoint\n            perception_task = {\n                'type': 'perception',\n                'location': waypoint['name']\n            }\n            self.capstone_manager.execute_task(perception_task)\n\n            # Small delay between waypoints\n            import time\n            time.sleep(2)\n\n        # 4. Generate final map report\n        self.capstone_manager.get_logger().info(\"Room mapping completed!\")\n\n        return True\n"})}),"\n",(0,o.jsx)(e.h3,{id:"4-testing-and-validation-framework",children:"4. Testing and Validation Framework"}),"\n",(0,o.jsx)(e.h4,{id:"a-unit-tests-for-components",children:"A. Unit Tests for Components"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# test/test_capstone_components.py\nimport unittest\nimport rclpy\nfrom rclpy.executors import SingleThreadedExecutor\nfrom capstone_manager import CapstoneManager\n\n\nclass TestCapstoneComponents(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        rclpy.init()\n\n    @classmethod\n    def tearDownClass(cls):\n        rclpy.shutdown()\n\n    def setUp(self):\n        self.node = CapstoneManager()\n        self.executor = SingleThreadedExecutor()\n        self.executor.add_node(self.node)\n\n    def tearDown(self):\n        self.node.destroy_node()\n\n    def test_navigation_integration(self):\n        """Test navigation component integration"""\n        # Test that navigation manager can be initialized\n        self.assertIsNotNone(self.node.navigation_manager)\n\n        # Test navigation to known location\n        result = self.node.navigation_manager.navigate_to(\'kitchen\')\n        self.assertIsNotNone(result)\n\n    def test_perception_integration(self):\n        """Test perception component integration"""\n        self.assertIsNotNone(self.node.perception_manager)\n\n    def test_manipulation_integration(self):\n        """Test manipulation component integration"""\n        self.assertIsNotNone(self.node.manipulation_manager)\n\n    def test_llm_integration(self):\n        """Test LLM interface integration"""\n        self.assertIsNotNone(self.node.llm_interface)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'})}),"\n",(0,o.jsx)(e.h4,{id:"b-integration-tests",children:"B. Integration Tests"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# test/test_capstone_integration.py\nimport unittest\nimport rclpy\nfrom std_msgs.msg import String\nimport time\n\n\nclass TestCapstoneIntegration(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        rclpy.init()\n\n    @classmethod\n    def tearDownClass(cls):\n        rclpy.shutdown()\n\n    def setUp(self):\n        self.node = rclpy.create_node(\'capstone_integration_tester\')\n\n        # Create publisher for voice commands\n        self.voice_pub = self.node.create_publisher(String, \'voice_commands\', 10)\n\n        # Create subscriber for status\n        self.status_sub = self.node.create_subscription(\n            String, \'capstone_status\', self.status_callback, 10\n        )\n\n        self.status_received = False\n        self.status_data = None\n\n    def tearDown(self):\n        self.node.destroy_node()\n\n    def status_callback(self, msg):\n        self.status_received = True\n        self.status_data = msg.data\n\n    def test_voice_command_processing(self):\n        """Test end-to-end voice command processing"""\n        # Send a test command\n        test_cmd = String()\n        test_cmd.data = "Go to the kitchen"\n        self.voice_pub.publish(test_cmd)\n\n        # Wait for response\n        timeout = time.time() + 10.0  # 10 second timeout\n        while not self.status_received and time.time() < timeout:\n            rclpy.spin_once(self.node, timeout_sec=0.1)\n\n        self.assertTrue(self.status_received)\n        self.assertIsNotNone(self.status_data)\n'})}),"\n",(0,o.jsx)(e.h3,{id:"5-performance-monitoring-and-optimization",children:"5. Performance Monitoring and Optimization"}),"\n",(0,o.jsx)(e.h4,{id:"a-performance-dashboard",children:"A. Performance Dashboard"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# scripts/performance_monitor.py\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import Float32, Int32\nimport psutil\nimport time\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport threading\n\n\nclass CapstonePerformanceMonitor(Node):\n    \"\"\"\n    Monitor and visualize capstone project performance\n    \"\"\"\n    def __init__(self):\n        super().__init__('capstone_performance_monitor')\n\n        # Performance data storage\n        self.cpu_history = deque(maxlen=100)\n        self.memory_history = deque(maxlen=100)\n        self.time_history = deque(maxlen=100)\n\n        # Publishers for performance metrics\n        self.cpu_pub = self.create_publisher(Float32, 'performance/cpu_percent', 10)\n        self.memory_pub = self.create_publisher(Float32, 'performance/memory_percent', 10)\n\n        # Timer for periodic monitoring\n        self.monitor_timer = self.create_timer(1.0, self.monitor_performance)\n\n        # Visualization thread\n        self.viz_thread = threading.Thread(target=self.run_visualization)\n        self.viz_thread.daemon = True\n        self.viz_thread.start()\n\n        self.start_time = time.time()\n\n    def monitor_performance(self):\n        \"\"\"\n        Monitor system performance metrics\n        \"\"\"\n        cpu_percent = psutil.cpu_percent()\n        memory_percent = psutil.virtual_memory().percent\n\n        current_time = time.time() - self.start_time\n\n        # Store in history\n        self.cpu_history.append(cpu_percent)\n        self.memory_history.append(memory_percent)\n        self.time_history.append(current_time)\n\n        # Publish metrics\n        cpu_msg = Float32()\n        cpu_msg.data = float(cpu_percent)\n        self.cpu_pub.publish(cpu_msg)\n\n        memory_msg = Float32()\n        memory_msg.data = float(memory_percent)\n        self.memory_pub.publish(memory_msg)\n\n        self.get_logger().info(f\"CPU: {cpu_percent:.1f}%, Memory: {memory_percent:.1f}%\")\n\n    def run_visualization(self):\n        \"\"\"\n        Run real-time performance visualization\n        \"\"\"\n        plt.ion()  # Interactive mode\n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))\n\n        while rclpy.ok():\n            if len(self.time_history) > 1:\n                ax1.clear()\n                ax1.plot(list(self.time_history), list(self.cpu_history), 'b-', label='CPU %')\n                ax1.set_ylabel('CPU Usage (%)')\n                ax1.set_title('Capstone Project Performance')\n                ax1.legend()\n                ax1.grid(True)\n\n                ax2.clear()\n                ax2.plot(list(self.time_history), list(self.memory_history), 'r-', label='Memory %')\n                ax2.set_ylabel('Memory Usage (%)')\n                ax2.set_xlabel('Time (seconds)')\n                ax2.legend()\n                ax2.grid(True)\n\n                plt.tight_layout()\n                plt.pause(0.01)\n\n            time.sleep(0.1)\n\n    def get_performance_summary(self):\n        \"\"\"\n        Get performance summary statistics\n        \"\"\"\n        if not self.cpu_history:\n            return {}\n\n        return {\n            'avg_cpu': sum(self.cpu_history) / len(self.cpu_history),\n            'max_cpu': max(self.cpu_history),\n            'avg_memory': sum(self.memory_history) / len(self.memory_history),\n            'max_memory': max(self.memory_history),\n            'duration': time.time() - self.start_time\n        }\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    monitor = CapstonePerformanceMonitor()\n\n    try:\n        rclpy.spin(monitor)\n    except KeyboardInterrupt:\n        summary = monitor.get_performance_summary()\n        print(f\"\\nPerformance Summary:\")\n        print(f\"  Duration: {summary.get('duration', 0):.2f}s\")\n        print(f\"  Avg CPU: {summary.get('avg_cpu', 0):.2f}%\")\n        print(f\"  Max CPU: {summary.get('max_cpu', 0):.2f}%\")\n        print(f\"  Avg Memory: {summary.get('avg_memory', 0):.2f}%\")\n        print(f\"  Max Memory: {summary.get('max_memory', 0):.2f}%\")\n    finally:\n        monitor.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h3,{id:"6-deployment-considerations",children:"6. Deployment Considerations"}),"\n",(0,o.jsx)(e.h4,{id:"a-simulation-to-real-deployment-checklist",children:"A. Simulation to Real Deployment Checklist"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:'# config/deployment_checklist.yaml\ndeployment_checklist:\n  simulation_phase:\n    - "Verify all components work in Gazebo simulation"\n    - "Test navigation in various simulated environments"\n    - "Validate perception system with simulated sensors"\n    - "Test voice command processing with simulated input"\n    - "Run performance benchmarks in simulation"\n\n  hardware_integration_phase:\n    - "Verify hardware interfaces match simulation"\n    - "Calibrate sensors for real-world conditions"\n    - "Test safety systems and emergency stops"\n    - "Validate communication protocols"\n    - "Check power consumption profiles"\n\n  real_world_testing_phase:\n    - "Start with simple navigation tasks"\n    - "Gradually increase task complexity"\n    - "Monitor system performance and stability"\n    - "Validate safety mechanisms"\n    - "Collect real-world performance metrics"\n\n  optimization_phase:\n    - "Apply edge optimization techniques"\n    - "Fine-tune for real-time performance"\n    - "Optimize power consumption"\n    - "Implement robust error handling"\n    - "Validate system reliability"\n'})}),"\n",(0,o.jsx)(e.h4,{id:"b-configuration-management",children:"B. Configuration Management"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-yaml",children:'# config/capstone_params.yaml\ncapstone_project:\n  ros__parameters:\n    # System parameters\n    use_sim_time: true\n    system_timeout: 30.0\n    max_navigation_attempts: 3\n\n    # LLM integration\n    llm_api_key: "your-api-key-here"\n    llm_model: "gpt-3.5-turbo"\n    llm_temperature: 0.3\n\n    # Voice processing\n    voice_recognition_timeout: 10.0\n    voice_energy_threshold: 4000\n    wake_word_detection: true\n\n    # Navigation parameters\n    navigation:\n      goal_tolerance: 0.5\n      max_velocity: 0.5\n      min_velocity: 0.1\n      obstacle_threshold: 0.5\n\n    # Perception parameters\n    perception:\n      detection_confidence_threshold: 0.7\n      max_detection_range: 5.0\n      object_classes: ["cup", "bottle", "box", "chair", "table"]\n\n    # Manipulation parameters\n    manipulation:\n      grasp_success_threshold: 0.8\n      manipulation_timeout: 15.0\n      safety_margin: 0.1\n'})}),"\n",(0,o.jsx)(e.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,o.jsx)(e.h3,{id:"1-technical-requirements",children:"1. Technical Requirements"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Integration"}),": All components work together seamlessly"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Performance"}),": System meets real-time requirements"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Reliability"}),": System operates without critical failures"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safety"}),": All safety mechanisms function correctly"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"2-functional-requirements",children:"2. Functional Requirements"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation"}),": Robot can navigate to specified locations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Perception"}),": Robot can identify and locate objects"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Manipulation"}),": Robot can manipulate objects safely"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Interaction"}),": Natural language interface works effectively"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"3-assessment-rubric",children:"3. Assessment Rubric"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Component Integration (25%)"}),": How well components work together"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"System Performance (25%)"}),": Real-time performance and efficiency"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Functionality (25%)"}),": Completeness of implemented features"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robustness (15%)"}),": Error handling and system reliability"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Documentation (10%)"}),": Quality of code and system documentation"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"troubleshooting-and-maintenance",children:"Troubleshooting and Maintenance"}),"\n",(0,o.jsx)(e.h3,{id:"common-issues",children:"Common Issues:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Timing Issues"}),": Use appropriate QoS settings for real-time performance"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Memory Leaks"}),": Implement proper cleanup in all components"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Communication Failures"}),": Verify network configuration and topic connections"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Performance Bottlenecks"}),": Profile and optimize critical paths"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"maintenance-tasks",children:"Maintenance Tasks:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Regular performance monitoring"}),"\n",(0,o.jsx)(e.li,{children:"System health checks"}),"\n",(0,o.jsx)(e.li,{children:"Log analysis and error tracking"}),"\n",(0,o.jsx)(e.li,{children:"Component updates and patches"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"resources-and-further-development",children:"Resources and Further Development"}),"\n",(0,o.jsx)(e.h3,{id:"extension-opportunities",children:"Extension Opportunities:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multi-Robot Coordination"}),": Extend to multiple robots"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Learning from Demonstration"}),": Add Imitation Learning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Advanced Manipulation"}),": Complex bimanual tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Human-Robot Collaboration"}),": Safe human-robot interaction"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"documentation-references",children:"Documentation References:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://docs.ros.org/",children:"ROS 2 Documentation"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://navigation.ros.org/",children:"Navigation2 Documentation"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://nvidia-isaac-ros.github.io/",children:"Isaac ROS Documentation"})}),"\n",(0,o.jsx)(e.li,{children:(0,o.jsx)(e.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API Documentation"})}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"This capstone project demonstrates the integration of all concepts learned in the Physical AI and Humanoid Robotics curriculum, providing a comprehensive example of a real-world robotics application."})]})}function p(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(m,{...n})}):m(n)}},7074:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var a=t(6540);const o={},i=a.createContext(o);function s(n){const e=a.useContext(i);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),a.createElement(i.Provider,{value:e},n.children)}}}]);