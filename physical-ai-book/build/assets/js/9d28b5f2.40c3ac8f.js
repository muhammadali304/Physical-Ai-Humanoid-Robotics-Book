"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9401],{127:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"isaac-platform/robot-manipulation-rl-example","title":"Basic RL Training Example for Robot Manipulation","description":"This document provides a practical example of reinforcement learning for robot manipulation tasks in Isaac Sim.","source":"@site/docs/isaac-platform/robot-manipulation-rl-example.md","sourceDirName":"isaac-platform","slug":"/isaac-platform/robot-manipulation-rl-example","permalink":"/docs/isaac-platform/robot-manipulation-rl-example","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/isaac-platform/robot-manipulation-rl-example.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Setup Guide for Reinforcement Learning","permalink":"/docs/isaac-platform/isaac-sim-rl-setup-guide"},"next":{"title":"LLM Integration Guide for Robotics Applications","permalink":"/docs/isaac-platform/llm-integration-guide"}}');var o=t(4848),s=t(7074);const r={},a="Basic RL Training Example for Robot Manipulation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Environment Setup",id:"environment-setup",level:2},{value:"1. Robot and Object Configuration",id:"1-robot-and-object-configuration",level:3},{value:"Running the Example",id:"running-the-example",level:2},{value:"1. Setup and Installation",id:"1-setup-and-installation",level:3},{value:"2. Execute Training",id:"2-execute-training",level:3},{value:"3. Monitor Training",id:"3-monitor-training",level:3},{value:"Advanced RL Techniques",id:"advanced-rl-techniques",level:2},{value:"1. Curriculum Learning",id:"1-curriculum-learning",level:3},{value:"2. Hindsight Experience Replay (HER)",id:"2-hindsight-experience-replay-her",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"1. Parallel Environments",id:"1-parallel-environments",level:3},{value:"2. GPU Acceleration",id:"2-gpu-acceleration",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues:",id:"common-issues",level:3},{value:"Verification Steps:",id:"verification-steps",level:3},{value:"Next Steps",id:"next-steps",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"basic-rl-training-example-for-robot-manipulation",children:"Basic RL Training Example for Robot Manipulation"})}),"\n",(0,o.jsx)(e.p,{children:"This document provides a practical example of reinforcement learning for robot manipulation tasks in Isaac Sim."}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"This example demonstrates how to train a robotic arm to perform basic manipulation tasks using reinforcement learning. The example uses the Franka Emika Panda robot as a reference, but the principles apply to other manipulator robots including humanoid arms."}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Completed Isaac Sim RL setup (see isaac-sim-rl-setup-guide.md)"}),"\n",(0,o.jsx)(e.li,{children:"Isaac Sim with RL Games extension enabled"}),"\n",(0,o.jsx)(e.li,{children:"Python environment with RL libraries installed"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"environment-setup",children:"Environment Setup"}),"\n",(0,o.jsx)(e.h3,{id:"1-robot-and-object-configuration",children:"1. Robot and Object Configuration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# robot_manipulation_env.py\nimport numpy as np\nimport torch\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.utils.rotations import euler_angles_to_quat\nfrom omni.isaac.core.articulations import ArticulationView\nfrom omni.isaac.core.objects import DynamicCuboid\n\n\nclass RobotManipulationEnv:\n    def __init__(self, world: World):\n        self.world = world\n        self.robot = None\n        self.object = None\n        self.target = None\n\n        # Environment parameters\n        self.action_scale = 0.05  # Scale for joint position changes\n        self.distance_threshold = 0.05  # Distance threshold for success\n        self.max_episode_length = 500\n\n        # Initialize the environment\n        self.setup_scene()\n\n    def setup_scene(self):\n        """Setup the simulation scene with robot, object, and target"""\n        # Get assets root path\n        assets_root_path = get_assets_root_path()\n\n        # Add Franka robot\n        franka_asset_path = assets_root_path + "/Isaac/Robots/Franka/franka_instanceable.usd"\n        add_reference_to_stage(usd_path=franka_asset_path, prim_path="/World/Franka")\n\n        # Create target object (cube to manipulate)\n        self.object = self.world.scene.add(\n            DynamicCuboid(\n                prim_path="/World/Object",\n                name="object",\n                position=np.array([0.5, 0.0, 0.1]),\n                size=0.05,\n                color=np.array([1.0, 0.0, 0.0])\n            )\n        )\n\n        # Create target position\n        self.target = self.world.scene.add(\n            DynamicCuboid(\n                prim_path="/World/Target",\n                name="target",\n                position=np.array([0.6, 0.2, 0.1]),\n                size=0.05,\n                color=np.array([0.0, 1.0, 0.0]),\n                mass=0  # Static target\n            )\n        )\n\n        # Get robot view\n        self.robot = self.world.scene.get_object("franka")\n\n    def get_observation(self):\n        """Get current observation from the environment"""\n        # Get robot joint positions and velocities\n        joint_positions = self.robot.get_joint_positions()\n        joint_velocities = self.robot.get_joint_velocities()\n\n        # Get object position\n        object_position = self.object.get_world_poses()[0][0]\n\n        # Get target position\n        target_position = self.target.get_world_poses()[0][0]\n\n        # Get end-effector position\n        ee_position = self.get_end_effector_position()\n\n        # Create observation vector\n        observation = np.concatenate([\n            joint_positions,           # Joint positions\n            joint_velocities,          # Joint velocities\n            object_position,           # Object position\n            target_position,           # Target position\n            ee_position,               # End-effector position\n            [np.linalg.norm(object_position - ee_position)],  # Distance to object\n            [np.linalg.norm(object_position - target_position)]  # Distance to target\n        ])\n\n        return observation\n\n    def get_end_effector_position(self):\n        """Get the position of the robot\'s end effector"""\n        # Get the end effector link (typically the last link)\n        # This may vary depending on the robot model\n        ee_link_path = "/World/Franka/panda_hand"\n        ee_prim = get_prim_at_path(ee_link_path)\n        if ee_prim:\n            # Get the world position of the end effector\n            from pxr import Gf\n            pos_attr = ee_prim.GetAttribute("xformOp:translate")\n            if pos_attr:\n                return np.array(pos_attr.Get())\n\n        # Fallback: use robot\'s root position + offset\n        robot_pos, _ = self.robot.get_world_poses()\n        return robot_pos[0] + np.array([0.5, 0.0, 0.5])\n\n    def apply_action(self, action):\n        """Apply action to the robot"""\n        # Rescale action\n        scaled_action = action * self.action_scale\n\n        # Get current joint positions\n        current_positions = self.robot.get_joint_positions()\n\n        # Calculate new joint positions\n        new_positions = current_positions + scaled_action\n\n        # Apply new joint positions\n        self.robot.set_joint_positions(new_positions)\n\n    def compute_reward(self, action):\n        """Compute reward based on current state"""\n        # Get positions\n        object_position = self.object.get_world_poses()[0][0]\n        target_position = self.target.get_world_poses()[0][0]\n        ee_position = self.get_end_effector_position()\n\n        # Distance from end-effector to object (for grasping)\n        dist_ee_to_obj = np.linalg.norm(object_position - ee_position)\n\n        # Distance from object to target (for manipulation)\n        dist_obj_to_target = np.linalg.norm(object_position - target_position)\n\n        # Reward components\n        grasp_reward = -dist_ee_to_obj  # Encourage approaching object\n        manipulation_reward = -dist_obj_to_target  # Encourage moving object to target\n        action_penalty = -np.sum(np.abs(action)) * 0.01  # Penalty for large actions\n\n        # Success bonus if object is close to target\n        success_bonus = 100.0 if dist_obj_to_target < self.distance_threshold else 0.0\n\n        # Total reward\n        total_reward = grasp_reward * 0.1 + manipulation_reward + action_penalty + success_bonus\n\n        return total_reward\n\n    def is_done(self):\n        """Check if episode is done"""\n        current_step = self.world.current_time_step_index\n\n        # Get positions\n        object_position = self.object.get_world_poses()[0][0]\n        target_position = self.target.get_world_poses()[0][0]\n        dist_obj_to_target = np.linalg.norm(object_position - target_position)\n\n        # Check if episode is done\n        done = (\n            current_step >= self.max_episode_length or\n            dist_obj_to_target < self.distance_threshold\n        )\n\n        return done, dist_obj_to_target < self.distance_threshold\n\n    def reset(self):\n        """Reset the environment"""\n        # Reset object position\n        new_obj_pos = np.array([0.5, 0.0, 0.1]) + np.random.uniform(-0.1, 0.1, 3)\n        self.object.set_world_poses(positions=np.array([new_obj_pos]))\n\n        # Reset robot joints to initial positions\n        initial_joint_positions = np.array([0.0, -1.0, 0.0, -2.2, 0.0, 1.2, 0.0])\n        self.robot.set_joint_positions(initial_joint_positions)\n\n        return self.get_observation()\n\n\n# Training script using Stable Baselines3\ndef train_robot_manipulation():\n    """Train a robot to manipulate objects using RL"""\n    from stable_baselines3 import PPO\n    from stable_baselines3.common.env_util import make_vec_env\n    from stable_baselines3.common.callbacks import EvalCallback\n    import gymnasium as gym\n    from gymnasium import spaces\n\n    # Custom gym environment wrapper\n    class IsaacGymEnv(gym.Env):\n        def __init__(self, world):\n            self.env = RobotManipulationEnv(world)\n\n            # Define action and observation spaces\n            num_joints = 7  # Franka has 7 joints\n            self.action_space = spaces.Box(\n                low=-1.0, high=1.0, shape=(num_joints,), dtype=np.float32\n            )\n            self.observation_space = spaces.Box(\n                low=-np.inf, high=np.inf,\n                shape=(num_joints * 2 + 3 + 3 + 3 + 2,),  # pos + vel + obj_pos + target_pos + ee_pos + distances\n                dtype=np.float32\n            )\n\n        def reset(self, seed=None, options=None):\n            super().reset(seed=seed)\n            obs = self.env.reset()\n            return obs.astype(np.float32), {}\n\n        def step(self, action):\n            self.env.apply_action(action)\n\n            # Step the simulation\n            self.env.world.step(render=True)\n\n            obs = self.env.get_observation()\n            reward = self.env.compute_reward(action)\n            done, success = self.env.is_done()\n\n            info = {"is_success": success}\n\n            return obs.astype(np.float32), reward, done, False, info\n\n    # Launch Isaac Sim\n    config = {"headless": True}\n    simulation_app = omni.simulation.SimulationApp(config)\n\n    # Create world\n    world = World(stage_units_in_meters=1.0)\n\n    # Create environment\n    isaac_env = IsaacGymEnv(world)\n\n    # Create vectorized environment\n    vec_env = make_vec_env(lambda: isaac_env, n_envs=1)\n\n    # Create PPO model\n    model = PPO(\n        "MlpPolicy",\n        vec_env,\n        verbose=1,\n        tensorboard_log="./tb_logs/",\n        learning_rate=3e-4,\n        n_steps=2048,\n        batch_size=64,\n        n_epochs=10,\n        gamma=0.99,\n        gae_lambda=0.95,\n        clip_range=0.2,\n        ent_coef=0.01\n    )\n\n    # Train the model\n    model.learn(total_timesteps=100000)\n\n    # Save the model\n    model.save("robot_manipulation_model")\n\n    # Close simulation\n    simulation_app.close()\n\n    print("Training completed and model saved!")\n\n\nif __name__ == "__main__":\n    train_robot_manipulation()\n'})}),"\n",(0,o.jsx)(e.h2,{id:"running-the-example",children:"Running the Example"}),"\n",(0,o.jsx)(e.h3,{id:"1-setup-and-installation",children:"1. Setup and Installation"}),"\n",(0,o.jsx)(e.p,{children:"First, ensure all dependencies are installed:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Activate Isaac Sim environment\ncd ~/isaac-sim\nsource setup_conda_env.sh\n\n# Install required packages\npip install stable-baselines3[extra]\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\npip install gymnasium[box2d]\npip install wandb\n"})}),"\n",(0,o.jsx)(e.h3,{id:"2-execute-training",children:"2. Execute Training"}),"\n",(0,o.jsxs)(e.p,{children:["Save the example code as ",(0,o.jsx)(e.code,{children:"robot_manipulation_rl.py"})," and run:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"cd ~/isaac-sim\nsource setup_conda_env.sh\npython robot_manipulation_rl.py\n"})}),"\n",(0,o.jsx)(e.h3,{id:"3-monitor-training",children:"3. Monitor Training"}),"\n",(0,o.jsx)(e.p,{children:"Training progress can be monitored using TensorBoard:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"tensorboard --logdir tb_logs/\n"})}),"\n",(0,o.jsx)(e.h2,{id:"advanced-rl-techniques",children:"Advanced RL Techniques"}),"\n",(0,o.jsx)(e.h3,{id:"1-curriculum-learning",children:"1. Curriculum Learning"}),"\n",(0,o.jsx)(e.p,{children:"Implement progressive difficulty increase:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'class CurriculumLearning:\n    def __init__(self):\n        self.current_level = 0\n        self.level_thresholds = [0.1, 0.05, 0.02]  # Distance thresholds for each level\n\n    def update_curriculum(self, success_rate):\n        if success_rate > 0.8 and self.current_level < len(self.level_thresholds) - 1:\n            self.current_level += 1\n            print(f"Curriculum advanced to level {self.current_level + 1}")\n\n    def get_current_threshold(self):\n        return self.level_thresholds[self.current_level]\n'})}),"\n",(0,o.jsx)(e.h3,{id:"2-hindsight-experience-replay-her",children:"2. Hindsight Experience Replay (HER)"}),"\n",(0,o.jsx)(e.p,{children:"For goal-conditioned tasks:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Example HER implementation concept\ndef hindsight_experience_replay(episode_transitions, reward_function):\n    \"\"\"\n    Convert failed episodes to successful ones by changing the goal\n    \"\"\"\n    her_transitions = []\n\n    for transition in episode_transitions:\n        # Original transition\n        her_transitions.append(transition)\n\n        # Future state as goal (for HER)\n        future_state = transition['next_state']\n        her_reward = reward_function(future_state, future_state)  # Success reward\n        her_transition = {\n            'state': transition['state'],\n            'action': transition['action'],\n            'reward': her_reward,\n            'next_state': transition['next_state'],\n            'goal': future_state\n        }\n        her_transitions.append(her_transition)\n\n    return her_transitions\n"})}),"\n",(0,o.jsx)(e.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,o.jsx)(e.h3,{id:"1-parallel-environments",children:"1. Parallel Environments"}),"\n",(0,o.jsx)(e.p,{children:"For faster training, use multiple parallel environments:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Configuration for parallel environments\nrl_config = {\n    "env": {\n        "numEnvs": 256,  # Number of parallel environments\n        "envSpacing": 5.0,\n        "episodeLength": 1000,\n        "enableDebugVis": False,\n    },\n    "sim": {\n        "dt": 1.0 / 60.0,\n        "use_gpu_pipeline": True,\n        "gravity": [0.0, 0.0, -9.81],\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"2-gpu-acceleration",children:"2. GPU Acceleration"}),"\n",(0,o.jsx)(e.p,{children:"Enable GPU physics for better performance:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Enable GPU physics simulation\nfrom omni.isaac.core import World\n\nworld = World(\n    stage_units_in_meters=1.0,\n    rendering_dt=1.0/60.0,\n    sim_params={\n        "use_gpu_pipeline": True,\n        "dt": 1.0/60.0,\n        "substeps": 1,\n        "gravity": [0.0, 0.0, -9.81]\n    }\n)\n'})}),"\n",(0,o.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(e.h3,{id:"common-issues",children:"Common Issues:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Slow Training"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Increase number of parallel environments"}),"\n",(0,o.jsx)(e.li,{children:"Optimize reward function computation"}),"\n",(0,o.jsx)(e.li,{children:"Use GPU acceleration for physics"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Unstable Training"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Reduce learning rate"}),"\n",(0,o.jsx)(e.li,{children:"Adjust reward scaling"}),"\n",(0,o.jsx)(e.li,{children:"Implement reward clipping"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Robot Instability"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Tune joint stiffness and damping"}),"\n",(0,o.jsx)(e.li,{children:"Reduce action magnitude"}),"\n",(0,o.jsx)(e.li,{children:"Adjust physics parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"verification-steps",children:"Verification Steps:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Environment initializes without errors"}),"\n",(0,o.jsx)(e.li,{children:"Robot responds to actions correctly"}),"\n",(0,o.jsx)(e.li,{children:"Reward function provides meaningful feedback"}),"\n",(0,o.jsx)(e.li,{children:"Training shows improvement over time"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(e.p,{children:"After implementing this basic manipulation example:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Extend to more complex manipulation tasks"}),"\n",(0,o.jsx)(e.li,{children:"Add multiple objects and obstacles"}),"\n",(0,o.jsx)(e.li,{children:"Implement more sophisticated reward shaping"}),"\n",(0,o.jsx)(e.li,{children:"Try different RL algorithms (SAC, TD3, etc.)"}),"\n",(0,o.jsx)(e.li,{children:"Transfer learned policies to real robots\n``parameter>"}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},7074:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);